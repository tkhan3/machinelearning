{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shameen_final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tkhan3/machinelearning/blob/master/Shameen_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "s_z28PSgRcYx",
        "colab_type": "code",
        "outputId": "74516138-aaea-40bc-e5b8-e46c3416bdaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "#import pandas as pd\n",
        "!google-drive-ocamlfuse drive\n",
        "!apt-get -qq install -y graphviz && pip install -q pydot\n",
        "!pip3 install seaborn==0.9.0\n",
        "import pydot\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 110851 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "Error: Mountpoint drive should be an existing directory.\n",
            "Collecting seaborn==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
            "\u001b[K    100% |████████████████████████████████| 215kB 26.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (1.14.6)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (2.1.2)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (0.22.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (1.11.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (0.10.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (2.5.3)\n",
            "Installing collected packages: seaborn\n",
            "  Found existing installation: seaborn 0.7.1\n",
            "    Uninstalling seaborn-0.7.1:\n",
            "      Successfully uninstalled seaborn-0.7.1\n",
            "Successfully installed seaborn-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8xZroJNsSZWy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\n",
        "from sklearn.metrics import (accuracy_score, log_loss, classification_report)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9lUkYWt3Sdcf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drive_path = 'drive/DL_COLAB/'\n",
        "\n",
        "HR_ANALYTICS_PATH = os.path.join(drive_path,\"hr_analytics\")\n",
        "\n",
        "def load_hr_data(filename, file_path=HR_ANALYTICS_PATH):\n",
        "    csv_path = os.path.join(file_path, filename)\n",
        "    return pd.read_csv(csv_path,sep=\"|\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVIfFwSuSjYr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hr_data_full = load_hr_data(\"new_Attrition_Data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mRNtml8tSqNj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hr_data_full_trim = hr_data_full[[\"Age\",\"Attrition\",\"BusinessTravel\",\"Department\",\"DistanceFromHome\",\"Education\",\"Gender\",\"HourlyRate\",\"JobInvolvement\",\"JobLevel\",\"JobRole\",\"JobSatisfaction\",\"MaritalStatus\",\"MonthlyIncome\",\"NumCompaniesWorked\",\"PercentSalaryHike\",\"PerformanceRating\",\"StockOptionLevel\",\"TotalWorkingYears\",\"TrainingTimesLastYear\",\"WorkLifeBalance\",\"YearsAtCompany\",\"YearsInCurrentRole\",\"YearsSinceLastPromotion\",\"YearsWithCurrManager\",\"OverTime\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-dMUVvKJSme3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "for train_index, test_index in split.split(hr_data_full, hr_data_full[\"Attrition\"]):\n",
        "    hr_data_train = hr_data_full_trim.loc[train_index]\n",
        "    hr_data_test = hr_data_full_trim.loc[test_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-O8auqNSvVR",
        "colab_type": "code",
        "outputId": "08597e7a-8d1e-4735-dcba-92a550de4fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "cell_type": "code",
      "source": [
        "hr_data_train.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>DistanceFromHome</th>\n",
              "      <th>Education</th>\n",
              "      <th>HourlyRate</th>\n",
              "      <th>JobInvolvement</th>\n",
              "      <th>JobLevel</th>\n",
              "      <th>JobSatisfaction</th>\n",
              "      <th>MonthlyIncome</th>\n",
              "      <th>NumCompaniesWorked</th>\n",
              "      <th>PercentSalaryHike</th>\n",
              "      <th>PerformanceRating</th>\n",
              "      <th>StockOptionLevel</th>\n",
              "      <th>TotalWorkingYears</th>\n",
              "      <th>TrainingTimesLastYear</th>\n",
              "      <th>WorkLifeBalance</th>\n",
              "      <th>YearsAtCompany</th>\n",
              "      <th>YearsInCurrentRole</th>\n",
              "      <th>YearsSinceLastPromotion</th>\n",
              "      <th>YearsWithCurrManager</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "      <td>1323.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>36.922902</td>\n",
              "      <td>9.311413</td>\n",
              "      <td>2.914588</td>\n",
              "      <td>65.651550</td>\n",
              "      <td>2.730915</td>\n",
              "      <td>2.060469</td>\n",
              "      <td>2.722600</td>\n",
              "      <td>6472.625850</td>\n",
              "      <td>2.684051</td>\n",
              "      <td>15.215420</td>\n",
              "      <td>3.156463</td>\n",
              "      <td>0.791383</td>\n",
              "      <td>11.292517</td>\n",
              "      <td>2.773243</td>\n",
              "      <td>2.761905</td>\n",
              "      <td>6.990930</td>\n",
              "      <td>4.223734</td>\n",
              "      <td>2.184429</td>\n",
              "      <td>4.139078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.122388</td>\n",
              "      <td>8.105754</td>\n",
              "      <td>1.026630</td>\n",
              "      <td>20.356772</td>\n",
              "      <td>0.706682</td>\n",
              "      <td>1.096398</td>\n",
              "      <td>1.108076</td>\n",
              "      <td>4662.442825</td>\n",
              "      <td>2.470083</td>\n",
              "      <td>3.674503</td>\n",
              "      <td>0.363431</td>\n",
              "      <td>0.854118</td>\n",
              "      <td>7.797057</td>\n",
              "      <td>1.276452</td>\n",
              "      <td>0.709776</td>\n",
              "      <td>6.094931</td>\n",
              "      <td>3.610559</td>\n",
              "      <td>3.236867</td>\n",
              "      <td>3.595089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>18.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1009.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>30.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2901.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>36.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4936.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>43.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8252.500000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>60.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>19973.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>17.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Age  DistanceFromHome    Education   HourlyRate  \\\n",
              "count  1323.000000       1323.000000  1323.000000  1323.000000   \n",
              "mean     36.922902          9.311413     2.914588    65.651550   \n",
              "std       9.122388          8.105754     1.026630    20.356772   \n",
              "min      18.000000          1.000000     1.000000    30.000000   \n",
              "25%      30.000000          2.000000     2.000000    48.000000   \n",
              "50%      36.000000          7.000000     3.000000    66.000000   \n",
              "75%      43.000000         14.000000     4.000000    83.000000   \n",
              "max      60.000000         29.000000     5.000000   100.000000   \n",
              "\n",
              "       JobInvolvement     JobLevel  JobSatisfaction  MonthlyIncome  \\\n",
              "count     1323.000000  1323.000000      1323.000000    1323.000000   \n",
              "mean         2.730915     2.060469         2.722600    6472.625850   \n",
              "std          0.706682     1.096398         1.108076    4662.442825   \n",
              "min          1.000000     1.000000         1.000000    1009.000000   \n",
              "25%          2.000000     1.000000         2.000000    2901.500000   \n",
              "50%          3.000000     2.000000         3.000000    4936.000000   \n",
              "75%          3.000000     3.000000         4.000000    8252.500000   \n",
              "max          4.000000     5.000000         4.000000   19973.000000   \n",
              "\n",
              "       NumCompaniesWorked  PercentSalaryHike  PerformanceRating  \\\n",
              "count         1323.000000        1323.000000        1323.000000   \n",
              "mean             2.684051          15.215420           3.156463   \n",
              "std              2.470083           3.674503           0.363431   \n",
              "min              0.000000          11.000000           3.000000   \n",
              "25%              1.000000          12.000000           3.000000   \n",
              "50%              2.000000          14.000000           3.000000   \n",
              "75%              4.000000          18.000000           3.000000   \n",
              "max              9.000000          25.000000           4.000000   \n",
              "\n",
              "       StockOptionLevel  TotalWorkingYears  TrainingTimesLastYear  \\\n",
              "count       1323.000000        1323.000000            1323.000000   \n",
              "mean           0.791383          11.292517               2.773243   \n",
              "std            0.854118           7.797057               1.276452   \n",
              "min            0.000000           0.000000               0.000000   \n",
              "25%            0.000000           6.000000               2.000000   \n",
              "50%            1.000000          10.000000               3.000000   \n",
              "75%            1.000000          15.000000               3.000000   \n",
              "max            3.000000          40.000000               6.000000   \n",
              "\n",
              "       WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n",
              "count      1323.000000     1323.000000         1323.000000   \n",
              "mean          2.761905        6.990930            4.223734   \n",
              "std           0.709776        6.094931            3.610559   \n",
              "min           1.000000        0.000000            0.000000   \n",
              "25%           2.000000        3.000000            2.000000   \n",
              "50%           3.000000        5.000000            3.000000   \n",
              "75%           3.000000        9.000000            7.000000   \n",
              "max           4.000000       40.000000           18.000000   \n",
              "\n",
              "       YearsSinceLastPromotion  YearsWithCurrManager  \n",
              "count              1323.000000           1323.000000  \n",
              "mean                  2.184429              4.139078  \n",
              "std                   3.236867              3.595089  \n",
              "min                   0.000000              0.000000  \n",
              "25%                   0.000000              2.000000  \n",
              "50%                   1.000000              3.000000  \n",
              "75%                   3.000000              7.000000  \n",
              "max                  15.000000             17.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "zY8e9fcMTIkx",
        "colab_type": "code",
        "outputId": "960e58b1-1a2c-41ad-b00e-db5b57901656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1251
        }
      },
      "cell_type": "code",
      "source": [
        "hr_data_train.hist(bins=70, figsize=(20,15))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f99b7501be0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b74770f0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b7431128>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b73e92e8>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f99b7421358>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b7421390>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99bce7b390>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b7338160>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f99b72ef6a0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b7321cc0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b72e2860>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b72922b0>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f99b72468d0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b71f7ef0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b71baa90>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b716c4e0>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f99b719fa90>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b715e0f0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b7112c50>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f99b711c8d0>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIYAAANdCAYAAADhsXdiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH1ZfcJiskgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f99b7509be0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GSmXkOEAS59y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hr_data_train_label = hr_data_train[\"Attrition\"].copy()\n",
        "hr_data_test_label =  hr_data_test[\"Attrition\"].copy()\n",
        "\n",
        "hr_data_train = hr_data_train.drop(\"Attrition\",axis=1)\n",
        "hr_data_test = hr_data_test.drop(\"Attrition\",axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "czH5XMOhS_Ai",
        "colab_type": "code",
        "outputId": "d67f7cbf-2045-4b95-f03e-bed505f12d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "print (hr_data_train.shape)\n",
        "print (hr_data_test.shape)\n",
        "print (hr_data_train_label.shape)\n",
        "print (hr_data_test_label.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1323, 25)\n",
            "(147, 25)\n",
            "(1323,)\n",
            "(147,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O6u_VWOkTTLj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cat_attribute = [\"BusinessTravel\",\"Department\",\"Gender\",\"JobRole\",\"MaritalStatus\",\"OverTime\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B9dQUtFGTWMc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "numeric_atribute = list(set(hr_data_train.columns) ^ set (cat_attribute))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wb8AlpJ2TdSA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy=\"median\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Icl4L3tTThBC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Inspired from stackoverflow.com/questions/25239958\n",
        "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],index=X.columns)\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "    #    return X.fillna(self.most_frequent_)\n",
        "        return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPYI7bfOTk-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a class to select numerical or categorical columns \n",
        "# since Scikit-Learn doesn't handle DataFrames yet\n",
        "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, attribute_names):\n",
        "        self.attribute_names = attribute_names\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X[self.attribute_names].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3FKno9ikToTY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_pipeline = Pipeline([\n",
        "        (\"select_numeric\", DataFrameSelector(numeric_atribute)),\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-KTfBSFUTrHl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cat_pipeline = Pipeline([\n",
        "        (\"select_cat\", DataFrameSelector(cat_attribute)),\n",
        "#        (\"imputer\", MostFrequentImputer()),\n",
        "        (\"cat_encoder\",OneHotEncoder(sparse=False,handle_unknown='ignore' )),\n",
        "        ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YXRLvWrLTtLv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preprocess_pipeline = FeatureUnion(transformer_list=[\n",
        "        (\"num_pipeline\", num_pipeline),\n",
        "        (\"cat_pipeline\", cat_pipeline),\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TkopFUIsTwBt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train  = preprocess_pipeline.fit_transform(hr_data_train)\n",
        "x_test =  preprocess_pipeline.transform(hr_data_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BkFGfANET0Z7",
        "colab_type": "code",
        "outputId": "b6c4ab58-13ab-4f67-e999-2c0f74002226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print (x_train.shape)\n",
        "print (x_test.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1323, 72)\n",
            "(147, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bQlHskP8T4NI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "y_train = le.fit_transform(hr_data_train_label)\n",
        "y_test = le.fit_transform(hr_data_test_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nd3mSO55UDMi",
        "colab_type": "code",
        "outputId": "d050a126-ace0-49cf-952d-3890094706dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print (y_train.shape)\n",
        "print (y_test.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1323,)\n",
            "(147,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MwJ91iXDUIt8",
        "colab_type": "code",
        "outputId": "cd397d0d-d3ec-477b-cbe9-16960a63f6fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "#train SVM Based Model\n",
        "svm_clf = SVC(gamma=\"auto\",random_state=42)\n",
        "svm_clf.fit(x_train, y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
              "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
              "  tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "2VDBbunCUToK",
        "colab_type": "code",
        "outputId": "89e62c5e-2e64-4f8e-f761-5f91662bd55b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "svm_scores = cross_val_score(svm_clf, x_train, y_train, cv=10)\n",
        "y_train_pred_svm = cross_val_predict(svm_clf, x_train, y_train, cv=10)\n",
        "print (svm_scores.mean())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8450501253132833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u5c72YJm34GR",
        "colab_type": "code",
        "outputId": "9bbcbdde-a867-4cd8-fefb-ff10026ef80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_train, y_train_pred_svm)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1108,    2],\n",
              "       [ 203,   10]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "1vM6n5lv45ot",
        "colab_type": "code",
        "outputId": "3a7cca9a-cfdb-426b-f0e0-88ee621d9e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_train,y_train_pred_svm).ravel()\n",
        "print (tn)\n",
        "print (fp)\n",
        "print (fn)\n",
        "print (tp)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1108\n",
            "2\n",
            "203\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lMj-Fghf38CI",
        "colab_type": "code",
        "outputId": "9371dbc1-3a3d-4407-dd43-c5722468d7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "classification_report(y_train, y_train_pred_svm)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n           0       0.85      1.00      0.92      1110\\n           1       0.83      0.05      0.09       213\\n\\n   micro avg       0.85      0.85      0.85      1323\\n   macro avg       0.84      0.52      0.50      1323\\nweighted avg       0.84      0.85      0.78      1323\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "RQIxZXYX4bd4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_svm = svm_clf.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eJZ75B2k4pgT",
        "colab_type": "code",
        "outputId": "f82cfb71-f346-407c-cf25-a343c53fa8ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test,y_test_pred_svm)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[123,   0],\n",
              "       [ 23,   1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "pmiy-qgj4z_j",
        "colab_type": "code",
        "outputId": "ab5f5310-3a17-47d3-b739-9a545dd7e9a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "param_grid = [{\"weights\":[\"uniform\",\"distance\"],\"n_neighbors\":[2,3,4,5]}]\n",
        "knn_clf = KNeighborsClassifier()\n",
        "grid_search = GridSearchCV(knn_clf, param_grid, cv=10, verbose=3, n_jobs=-1)\n",
        "grid_search.fit(x_train, y_train)\n",
        "print (grid_search.best_params_)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    3.9s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'n_neighbors': 4, 'weights': 'uniform'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:   14.0s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "gr-6KWg387oL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "knn_clf = KNeighborsClassifier(n_neighbors=4,weights='uniform')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uiUuQfzu9GN4",
        "colab_type": "code",
        "outputId": "dcbe16cf-d19b-4509-9eaf-33cfc6a39aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "knn_clf.fit(x_train,y_train)\n",
        "knn_scores = cross_val_score(knn_clf, x_train, y_train, cv=10)\n",
        "y_train_pred_knn = cross_val_predict(knn_clf, x_train, y_train, cv=10)\n",
        "print (knn_scores.mean())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8397698792435635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Mp2DyUM9Wmi",
        "colab_type": "code",
        "outputId": "f97cc3f1-c88f-4361-b755-229b8d403cb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_train,y_train_pred_knn).ravel()\n",
        "print (tn)\n",
        "print (fp)\n",
        "print (fn)\n",
        "print (tp)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1101\n",
            "9\n",
            "203\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uLAU9Ht99gMH",
        "colab_type": "code",
        "outputId": "f93ac1a0-7a6d-4140-e059-49f050764da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "classification_report(y_train, y_train_pred_knn)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n           0       0.84      0.99      0.91      1110\\n           1       0.53      0.05      0.09       213\\n\\n   micro avg       0.84      0.84      0.84      1323\\n   macro avg       0.69      0.52      0.50      1323\\nweighted avg       0.79      0.84      0.78      1323\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "8jzD8Rkg9zf_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_knn = knn_clf.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P98DV2lz98MV",
        "colab_type": "code",
        "outputId": "91cef922-a60a-4dac-ae67-c208579955c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test,y_test_pred_knn).ravel()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([123,   0,  23,   1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "C0DoaI3q-CbY",
        "colab_type": "code",
        "outputId": "77529a27-6473-46f0-b622-e9ce3cc66452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "classification_report(y_test, y_test_pred_knn)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n           0       0.84      1.00      0.91       123\\n           1       1.00      0.04      0.08        24\\n\\n   micro avg       0.84      0.84      0.84       147\\n   macro avg       0.92      0.52      0.50       147\\nweighted avg       0.87      0.84      0.78       147\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "GmlJMfXS-GLB",
        "colab_type": "code",
        "outputId": "f0703154-bbeb-4887-8b95-fbd76f81ac68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "xgboost_clf = XGBClassifier(random_state=42)\n",
        "xgboost_clf.fit(x_train,y_train)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
              "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
              "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
              "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "       seed=None, silent=True, subsample=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "MeYG6xIn-dbp",
        "colab_type": "code",
        "outputId": "1a100f10-6111-491e-c528-05d08f2af5b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "xgboost_scores = cross_val_score(xgboost_clf, x_train, y_train, cv=10)\n",
        "y_train_pred_xgboost = cross_val_predict(xgboost_clf, x_train, y_train, cv=10)\n",
        "xgboost_scores.mean()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8601902483481432"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "EzyEvmpi-3Fe",
        "colab_type": "code",
        "outputId": "bf9d9568-9238-4df2-f490-92c272a76499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_train,y_train_pred_xgboost).ravel()\n",
        "print (tn)\n",
        "print (fp)\n",
        "print (fn)\n",
        "print (tp)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1075\n",
            "35\n",
            "150\n",
            "63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mG-tObAR_MIy",
        "colab_type": "code",
        "outputId": "77c6e09c-6064-45ee-bb04-68e0abeedf0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "classification_report(y_train, y_train_pred_xgboost)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n           0       0.88      0.97      0.92      1110\\n           1       0.64      0.30      0.41       213\\n\\n   micro avg       0.86      0.86      0.86      1323\\n   macro avg       0.76      0.63      0.66      1323\\nweighted avg       0.84      0.86      0.84      1323\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "fYiyidka_GGK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_xgboost = xgboost_clf.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3kO9wcen_Z1n",
        "colab_type": "code",
        "outputId": "4f8484c6-089b-4c23-f36e-2cfc12eedc9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test,y_test_pred_xgboost).ravel()\n",
        "print (tn)\n",
        "print (fp)\n",
        "print (fn)\n",
        "print (tp)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "119\n",
            "4\n",
            "20\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nsVhA8xK_jPZ",
        "colab_type": "code",
        "outputId": "ffc2c9fa-7564-4bbc-b9b6-e50e59fdb1c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "classification_report(y_test, y_test_pred_xgboost)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n           0       0.86      0.97      0.91       123\\n           1       0.50      0.17      0.25        24\\n\\n   micro avg       0.84      0.84      0.84       147\\n   macro avg       0.68      0.57      0.58       147\\nweighted avg       0.80      0.84      0.80       147\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "yktha_dO_1EB",
        "colab_type": "code",
        "outputId": "921392bd-d7c2-477a-d282-a377c6ecad9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "cell_type": "code",
      "source": [
        "param_grid = [{\"learning_rate\":[.001,.01,.1],\"max_depth\":[1,2,3],\"n_estimators\":[100,110,120]}]\n",
        "grid_search = GridSearchCV(xgboost_clf, param_grid, cv=10, verbose=3, n_jobs=-1)\n",
        "grid_search.fit(x_train, y_train)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    2.1s\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:   15.5s\n",
            "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:   37.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
              "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
              "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
              "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
              "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "       seed=None, silent=True, subsample=1),\n",
              "       fit_params=None, iid='warn', n_jobs=-1,\n",
              "       param_grid=[{'learning_rate': [0.001, 0.01, 0.1], 'max_depth': [1, 2, 3], 'n_estimators': [100, 110, 120]}],\n",
              "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
              "       scoring=None, verbose=3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "BsnOOwI3BL-w",
        "colab_type": "code",
        "outputId": "b18e2c27-9ea3-4305-bc85-8f925e93b2a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print (grid_search.best_params_)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 110}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7voV8Y-lBXpX",
        "colab_type": "code",
        "outputId": "c0d3d640-060e-4ddc-be9f-45e0813735d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "xgboost_clf = XGBClassifier(random_state=42,max_depth=2,n_estimators=110,learning_rate=0.1)\n",
        "xgboost_clf.fit(x_train,y_train)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
              "       max_depth=2, min_child_weight=1, missing=None, n_estimators=110,\n",
              "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
              "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "       seed=None, silent=True, subsample=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "QVtevT0XBkZT",
        "colab_type": "code",
        "outputId": "c6805a3d-7f77-43c8-b86b-eb4ae89a15c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "xgboost_scores = cross_val_score(xgboost_clf, x_train, y_train, cv=10)\n",
        "y_train_pred_xgboost = cross_val_predict(xgboost_clf, x_train, y_train, cv=10)\n",
        "xgboost_scores.mean()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8654591023012076"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "_5mLQLZ7CEu_",
        "colab_type": "code",
        "outputId": "993015fa-b8f0-4494-e451-c9786c7a104c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_train,y_train_pred_xgboost).ravel()\n",
        "print (tn)\n",
        "print (fp)\n",
        "print (fn)\n",
        "print (tp)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1089\n",
            "21\n",
            "157\n",
            "56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JKVcu2tlCb_a",
        "colab_type": "code",
        "outputId": "b8ec270f-7b15-4e97-98dc-a8d08c1ca63f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "classification_report(y_train, y_train_pred_xgboost)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n           0       0.87      0.98      0.92      1110\\n           1       0.73      0.26      0.39       213\\n\\n   micro avg       0.87      0.87      0.87      1323\\n   macro avg       0.80      0.62      0.66      1323\\nweighted avg       0.85      0.87      0.84      1323\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "L3FvrxerCdg3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_xgboost = xgboost_clf.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TQLJyWW2CkhB",
        "colab_type": "code",
        "outputId": "15c80721-b5dd-486f-e50a-f9d76891e98b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "classification_report(y_test, y_test_xgboost)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n           0       0.86      0.98      0.92       123\\n           1       0.62      0.21      0.31        24\\n\\n   micro avg       0.85      0.85      0.85       147\\n   macro avg       0.74      0.59      0.61       147\\nweighted avg       0.82      0.85      0.82       147\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "ED7Is2WVCsAQ",
        "colab_type": "code",
        "outputId": "82692063-b475-496e-b803-26b7fb07776c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test,y_test_pred_xgboost).ravel()\n",
        "print (tn)\n",
        "print (fp)\n",
        "print (fn)\n",
        "print (tp)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "119\n",
            "4\n",
            "20\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UUTRJwI1DbyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7856042-2739-4294-8755-7ee8ed1ead60"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "N5uesK8SDdBF",
        "colab_type": "code",
        "outputId": "4a2770ad-3bb8-45b4-d5f7-ee1e0c67d7aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1323, 72)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "5oWHPRZdDjDF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create model\n",
        "neural_net = Sequential()\n",
        "neural_net.add(Dense(8, input_dim=72, activation='relu'))\n",
        "#neural_net.add(Dense(12, activation='relu'))\n",
        "#neural_net.add(Dense(12, activation='relu'))\n",
        "#neural_net.add(Dense(12, activation='relu'))\n",
        "neural_net.add(Dense(12, activation='relu'))\n",
        "neural_net.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "neural_net.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GN8oriNDyTC",
        "colab_type": "code",
        "outputId": "6c9ebe00-7513-4a4d-cd01-d5fdf421e554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53385
        }
      },
      "cell_type": "code",
      "source": [
        "neural_net.fit(x_train, y_train, validation_split=0.1,epochs=1500, batch_size=10)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1190 samples, validate on 133 samples\n",
            "Epoch 1/1500\n",
            "1190/1190 [==============================] - 1s 434us/step - loss: 0.4423 - acc: 0.8454 - val_loss: 0.4824 - val_acc: 0.7820\n",
            "Epoch 2/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.3829 - acc: 0.8454 - val_loss: 0.4480 - val_acc: 0.7820\n",
            "Epoch 3/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.3495 - acc: 0.8462 - val_loss: 0.4152 - val_acc: 0.7820\n",
            "Epoch 4/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.3252 - acc: 0.8681 - val_loss: 0.4118 - val_acc: 0.8346\n",
            "Epoch 5/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.3040 - acc: 0.8857 - val_loss: 0.4342 - val_acc: 0.8421\n",
            "Epoch 6/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.2944 - acc: 0.8933 - val_loss: 0.4172 - val_acc: 0.8496\n",
            "Epoch 7/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.2728 - acc: 0.9034 - val_loss: 0.4311 - val_acc: 0.8421\n",
            "Epoch 8/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.2558 - acc: 0.9042 - val_loss: 0.4401 - val_acc: 0.8195\n",
            "Epoch 9/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.2393 - acc: 0.9109 - val_loss: 0.4347 - val_acc: 0.8271\n",
            "Epoch 10/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.2232 - acc: 0.9210 - val_loss: 0.5543 - val_acc: 0.8421\n",
            "Epoch 11/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.2076 - acc: 0.9193 - val_loss: 0.4973 - val_acc: 0.8421\n",
            "Epoch 12/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.1838 - acc: 0.9328 - val_loss: 0.5045 - val_acc: 0.8346\n",
            "Epoch 13/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.1699 - acc: 0.9378 - val_loss: 0.5131 - val_acc: 0.8195\n",
            "Epoch 14/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.1542 - acc: 0.9429 - val_loss: 0.6050 - val_acc: 0.8195\n",
            "Epoch 15/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.1461 - acc: 0.9487 - val_loss: 0.6117 - val_acc: 0.8195\n",
            "Epoch 16/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.1224 - acc: 0.9597 - val_loss: 0.6809 - val_acc: 0.8346\n",
            "Epoch 17/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.1044 - acc: 0.9647 - val_loss: 0.7320 - val_acc: 0.8120\n",
            "Epoch 18/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0865 - acc: 0.9739 - val_loss: 0.7853 - val_acc: 0.8120\n",
            "Epoch 19/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0800 - acc: 0.9782 - val_loss: 0.8587 - val_acc: 0.8271\n",
            "Epoch 20/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0785 - acc: 0.9756 - val_loss: 0.8617 - val_acc: 0.8195\n",
            "Epoch 21/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0746 - acc: 0.9773 - val_loss: 0.9802 - val_acc: 0.8346\n",
            "Epoch 22/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0656 - acc: 0.9815 - val_loss: 0.8813 - val_acc: 0.8045\n",
            "Epoch 23/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0427 - acc: 0.9866 - val_loss: 1.0256 - val_acc: 0.8195\n",
            "Epoch 24/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0348 - acc: 0.9916 - val_loss: 1.1221 - val_acc: 0.8346\n",
            "Epoch 25/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0353 - acc: 0.9891 - val_loss: 1.1519 - val_acc: 0.7895\n",
            "Epoch 26/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0364 - acc: 0.9899 - val_loss: 1.3765 - val_acc: 0.8120\n",
            "Epoch 27/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0437 - acc: 0.9840 - val_loss: 1.3017 - val_acc: 0.7744\n",
            "Epoch 28/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0240 - acc: 0.9950 - val_loss: 1.3273 - val_acc: 0.7970\n",
            "Epoch 29/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0167 - acc: 0.9958 - val_loss: 1.3937 - val_acc: 0.8045\n",
            "Epoch 30/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0128 - acc: 0.9966 - val_loss: 1.4388 - val_acc: 0.8120\n",
            "Epoch 31/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0085 - acc: 0.9983 - val_loss: 1.4582 - val_acc: 0.8120\n",
            "Epoch 32/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0076 - acc: 0.9992 - val_loss: 1.4960 - val_acc: 0.8045\n",
            "Epoch 33/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 1.5564 - val_acc: 0.8120\n",
            "Epoch 34/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 1.5919 - val_acc: 0.8045\n",
            "Epoch 35/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 1.6280 - val_acc: 0.7970\n",
            "Epoch 36/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 1.6634 - val_acc: 0.8045\n",
            "Epoch 37/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 1.6993 - val_acc: 0.8045\n",
            "Epoch 38/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 1.7319 - val_acc: 0.8045\n",
            "Epoch 39/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.7545 - val_acc: 0.8045\n",
            "Epoch 40/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.7826 - val_acc: 0.8045\n",
            "Epoch 41/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 1.7994 - val_acc: 0.8045\n",
            "Epoch 42/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.8153 - val_acc: 0.8045\n",
            "Epoch 43/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 9.3747e-04 - acc: 1.0000 - val_loss: 1.8239 - val_acc: 0.8045\n",
            "Epoch 44/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 8.3839e-04 - acc: 1.0000 - val_loss: 1.8438 - val_acc: 0.8045\n",
            "Epoch 45/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 7.2957e-04 - acc: 1.0000 - val_loss: 1.8562 - val_acc: 0.8045\n",
            "Epoch 46/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 6.6552e-04 - acc: 1.0000 - val_loss: 1.8644 - val_acc: 0.8045\n",
            "Epoch 47/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 5.7079e-04 - acc: 1.0000 - val_loss: 1.8786 - val_acc: 0.8045\n",
            "Epoch 48/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 5.1173e-04 - acc: 1.0000 - val_loss: 1.8993 - val_acc: 0.8045\n",
            "Epoch 49/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 4.6917e-04 - acc: 1.0000 - val_loss: 1.9128 - val_acc: 0.8045\n",
            "Epoch 50/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 4.1357e-04 - acc: 1.0000 - val_loss: 1.9228 - val_acc: 0.8045\n",
            "Epoch 51/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 3.8194e-04 - acc: 1.0000 - val_loss: 1.9349 - val_acc: 0.8045\n",
            "Epoch 52/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 3.3843e-04 - acc: 1.0000 - val_loss: 1.9502 - val_acc: 0.8045\n",
            "Epoch 53/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 3.1493e-04 - acc: 1.0000 - val_loss: 1.9497 - val_acc: 0.8045\n",
            "Epoch 54/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 2.7092e-04 - acc: 1.0000 - val_loss: 1.9668 - val_acc: 0.8045\n",
            "Epoch 55/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 2.4621e-04 - acc: 1.0000 - val_loss: 1.9766 - val_acc: 0.8045\n",
            "Epoch 56/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 2.2467e-04 - acc: 1.0000 - val_loss: 1.9833 - val_acc: 0.8045\n",
            "Epoch 57/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 2.0135e-04 - acc: 1.0000 - val_loss: 1.9918 - val_acc: 0.8045\n",
            "Epoch 58/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 1.8428e-04 - acc: 1.0000 - val_loss: 1.9957 - val_acc: 0.8045\n",
            "Epoch 59/1500\n",
            "1190/1190 [==============================] - 0s 150us/step - loss: 1.6519e-04 - acc: 1.0000 - val_loss: 2.0064 - val_acc: 0.8045\n",
            "Epoch 60/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 1.4851e-04 - acc: 1.0000 - val_loss: 2.0107 - val_acc: 0.8045\n",
            "Epoch 61/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.3580e-04 - acc: 1.0000 - val_loss: 2.0245 - val_acc: 0.8045\n",
            "Epoch 62/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.2479e-04 - acc: 1.0000 - val_loss: 2.0266 - val_acc: 0.8045\n",
            "Epoch 63/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.1193e-04 - acc: 1.0000 - val_loss: 2.0291 - val_acc: 0.8045\n",
            "Epoch 64/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.0416e-04 - acc: 1.0000 - val_loss: 2.0468 - val_acc: 0.8045\n",
            "Epoch 65/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 9.4114e-05 - acc: 1.0000 - val_loss: 2.0391 - val_acc: 0.8045\n",
            "Epoch 66/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 8.4843e-05 - acc: 1.0000 - val_loss: 2.0509 - val_acc: 0.8045\n",
            "Epoch 67/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 7.7980e-05 - acc: 1.0000 - val_loss: 2.0565 - val_acc: 0.8045\n",
            "Epoch 68/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 7.1868e-05 - acc: 1.0000 - val_loss: 2.0578 - val_acc: 0.8045\n",
            "Epoch 69/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 6.4541e-05 - acc: 1.0000 - val_loss: 2.0695 - val_acc: 0.8045\n",
            "Epoch 70/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 5.9615e-05 - acc: 1.0000 - val_loss: 2.0707 - val_acc: 0.8045\n",
            "Epoch 71/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 5.3686e-05 - acc: 1.0000 - val_loss: 2.0725 - val_acc: 0.8045\n",
            "Epoch 72/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 5.0801e-05 - acc: 1.0000 - val_loss: 2.0868 - val_acc: 0.8045\n",
            "Epoch 73/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 4.6521e-05 - acc: 1.0000 - val_loss: 2.0936 - val_acc: 0.8045\n",
            "Epoch 74/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 4.3345e-05 - acc: 1.0000 - val_loss: 2.1049 - val_acc: 0.8045\n",
            "Epoch 75/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 4.0163e-05 - acc: 1.0000 - val_loss: 2.1036 - val_acc: 0.8045\n",
            "Epoch 76/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 3.6411e-05 - acc: 1.0000 - val_loss: 2.0979 - val_acc: 0.8045\n",
            "Epoch 77/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 3.4085e-05 - acc: 1.0000 - val_loss: 2.1095 - val_acc: 0.8045\n",
            "Epoch 78/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 3.1000e-05 - acc: 1.0000 - val_loss: 2.1108 - val_acc: 0.8045\n",
            "Epoch 79/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 2.8588e-05 - acc: 1.0000 - val_loss: 2.1197 - val_acc: 0.8045\n",
            "Epoch 80/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 2.6929e-05 - acc: 1.0000 - val_loss: 2.1183 - val_acc: 0.8045\n",
            "Epoch 81/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 2.3948e-05 - acc: 1.0000 - val_loss: 2.1360 - val_acc: 0.8045\n",
            "Epoch 82/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 2.3282e-05 - acc: 1.0000 - val_loss: 2.1366 - val_acc: 0.8045\n",
            "Epoch 83/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 2.1681e-05 - acc: 1.0000 - val_loss: 2.1370 - val_acc: 0.8045\n",
            "Epoch 84/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.9562e-05 - acc: 1.0000 - val_loss: 2.1421 - val_acc: 0.8045\n",
            "Epoch 85/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.7934e-05 - acc: 1.0000 - val_loss: 2.1479 - val_acc: 0.8045\n",
            "Epoch 86/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.6562e-05 - acc: 1.0000 - val_loss: 2.1474 - val_acc: 0.8045\n",
            "Epoch 87/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.5467e-05 - acc: 1.0000 - val_loss: 2.1555 - val_acc: 0.8045\n",
            "Epoch 88/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.4241e-05 - acc: 1.0000 - val_loss: 2.1640 - val_acc: 0.8045\n",
            "Epoch 89/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.3353e-05 - acc: 1.0000 - val_loss: 2.1599 - val_acc: 0.8120\n",
            "Epoch 90/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.2183e-05 - acc: 1.0000 - val_loss: 2.1682 - val_acc: 0.8045\n",
            "Epoch 91/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.1469e-05 - acc: 1.0000 - val_loss: 2.1699 - val_acc: 0.8120\n",
            "Epoch 92/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.0338e-05 - acc: 1.0000 - val_loss: 2.1660 - val_acc: 0.8120\n",
            "Epoch 93/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.0113e-05 - acc: 1.0000 - val_loss: 2.1718 - val_acc: 0.8120\n",
            "Epoch 94/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 9.1344e-06 - acc: 1.0000 - val_loss: 2.1832 - val_acc: 0.8120\n",
            "Epoch 95/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 8.3449e-06 - acc: 1.0000 - val_loss: 2.1880 - val_acc: 0.8120\n",
            "Epoch 96/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 7.8606e-06 - acc: 1.0000 - val_loss: 2.1962 - val_acc: 0.8120\n",
            "Epoch 97/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 7.1961e-06 - acc: 1.0000 - val_loss: 2.1899 - val_acc: 0.8120\n",
            "Epoch 98/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 6.8828e-06 - acc: 1.0000 - val_loss: 2.2020 - val_acc: 0.8120\n",
            "Epoch 99/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 6.3736e-06 - acc: 1.0000 - val_loss: 2.2047 - val_acc: 0.8120\n",
            "Epoch 100/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 5.8753e-06 - acc: 1.0000 - val_loss: 2.2071 - val_acc: 0.8120\n",
            "Epoch 101/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 5.4002e-06 - acc: 1.0000 - val_loss: 2.2138 - val_acc: 0.8120\n",
            "Epoch 102/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 5.0768e-06 - acc: 1.0000 - val_loss: 2.2217 - val_acc: 0.8120\n",
            "Epoch 103/1500\n",
            "1190/1190 [==============================] - 0s 156us/step - loss: 4.9383e-06 - acc: 1.0000 - val_loss: 2.2207 - val_acc: 0.8045\n",
            "Epoch 104/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 4.3511e-06 - acc: 1.0000 - val_loss: 2.2180 - val_acc: 0.8120\n",
            "Epoch 105/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 4.0913e-06 - acc: 1.0000 - val_loss: 2.2251 - val_acc: 0.8045\n",
            "Epoch 106/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 3.8046e-06 - acc: 1.0000 - val_loss: 2.2267 - val_acc: 0.8045\n",
            "Epoch 107/1500\n",
            "1190/1190 [==============================] - 0s 149us/step - loss: 3.5768e-06 - acc: 1.0000 - val_loss: 2.2377 - val_acc: 0.8045\n",
            "Epoch 108/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 3.3183e-06 - acc: 1.0000 - val_loss: 2.2362 - val_acc: 0.8045\n",
            "Epoch 109/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 3.0619e-06 - acc: 1.0000 - val_loss: 2.2391 - val_acc: 0.8045\n",
            "Epoch 110/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 2.8838e-06 - acc: 1.0000 - val_loss: 2.2517 - val_acc: 0.8045\n",
            "Epoch 111/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 2.7288e-06 - acc: 1.0000 - val_loss: 2.2516 - val_acc: 0.8045\n",
            "Epoch 112/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 2.5159e-06 - acc: 1.0000 - val_loss: 2.2616 - val_acc: 0.8045\n",
            "Epoch 113/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 2.3552e-06 - acc: 1.0000 - val_loss: 2.2504 - val_acc: 0.8045\n",
            "Epoch 114/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 2.1622e-06 - acc: 1.0000 - val_loss: 2.2703 - val_acc: 0.8045\n",
            "Epoch 115/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 2.0623e-06 - acc: 1.0000 - val_loss: 2.2651 - val_acc: 0.8045\n",
            "Epoch 116/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 1.9218e-06 - acc: 1.0000 - val_loss: 2.2675 - val_acc: 0.8045\n",
            "Epoch 117/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 1.7770e-06 - acc: 1.0000 - val_loss: 2.2666 - val_acc: 0.8045\n",
            "Epoch 118/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 1.6456e-06 - acc: 1.0000 - val_loss: 2.2738 - val_acc: 0.8045\n",
            "Epoch 119/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.5440e-06 - acc: 1.0000 - val_loss: 2.2706 - val_acc: 0.8045\n",
            "Epoch 120/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.4526e-06 - acc: 1.0000 - val_loss: 2.2827 - val_acc: 0.8045\n",
            "Epoch 121/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.3454e-06 - acc: 1.0000 - val_loss: 2.2804 - val_acc: 0.8045\n",
            "Epoch 122/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.2631e-06 - acc: 1.0000 - val_loss: 2.2897 - val_acc: 0.8045\n",
            "Epoch 123/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.2082e-06 - acc: 1.0000 - val_loss: 2.2899 - val_acc: 0.8045\n",
            "Epoch 124/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.1156e-06 - acc: 1.0000 - val_loss: 2.2989 - val_acc: 0.8045\n",
            "Epoch 125/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 1.0594e-06 - acc: 1.0000 - val_loss: 2.3035 - val_acc: 0.8045\n",
            "Epoch 126/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 9.7802e-07 - acc: 1.0000 - val_loss: 2.3045 - val_acc: 0.8045\n",
            "Epoch 127/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 9.2654e-07 - acc: 1.0000 - val_loss: 2.3144 - val_acc: 0.8045\n",
            "Epoch 128/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 8.7455e-07 - acc: 1.0000 - val_loss: 2.3079 - val_acc: 0.8045\n",
            "Epoch 129/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 8.0905e-07 - acc: 1.0000 - val_loss: 2.3230 - val_acc: 0.8045\n",
            "Epoch 130/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 7.5499e-07 - acc: 1.0000 - val_loss: 2.3224 - val_acc: 0.8045\n",
            "Epoch 131/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 7.1323e-07 - acc: 1.0000 - val_loss: 2.3206 - val_acc: 0.8045\n",
            "Epoch 132/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 6.6879e-07 - acc: 1.0000 - val_loss: 2.3330 - val_acc: 0.7970\n",
            "Epoch 133/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 6.2366e-07 - acc: 1.0000 - val_loss: 2.3231 - val_acc: 0.8045\n",
            "Epoch 134/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 6.3171e-07 - acc: 1.0000 - val_loss: 2.3347 - val_acc: 0.8045\n",
            "Epoch 135/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 5.5878e-07 - acc: 1.0000 - val_loss: 2.3412 - val_acc: 0.7970\n",
            "Epoch 136/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 5.2939e-07 - acc: 1.0000 - val_loss: 2.3465 - val_acc: 0.8045\n",
            "Epoch 137/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 4.9836e-07 - acc: 1.0000 - val_loss: 2.3428 - val_acc: 0.7970\n",
            "Epoch 138/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 4.6918e-07 - acc: 1.0000 - val_loss: 2.3500 - val_acc: 0.7970\n",
            "Epoch 139/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 4.4537e-07 - acc: 1.0000 - val_loss: 2.3603 - val_acc: 0.7970\n",
            "Epoch 140/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 4.2026e-07 - acc: 1.0000 - val_loss: 2.3625 - val_acc: 0.7970\n",
            "Epoch 141/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 3.9943e-07 - acc: 1.0000 - val_loss: 2.3608 - val_acc: 0.7970\n",
            "Epoch 142/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 3.7459e-07 - acc: 1.0000 - val_loss: 2.3642 - val_acc: 0.7970\n",
            "Epoch 143/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 3.5779e-07 - acc: 1.0000 - val_loss: 2.3705 - val_acc: 0.7970\n",
            "Epoch 144/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 3.3937e-07 - acc: 1.0000 - val_loss: 2.3808 - val_acc: 0.7970\n",
            "Epoch 145/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 3.2281e-07 - acc: 1.0000 - val_loss: 2.3724 - val_acc: 0.7970\n",
            "Epoch 146/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 3.0429e-07 - acc: 1.0000 - val_loss: 2.3862 - val_acc: 0.7970\n",
            "Epoch 147/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 2.9206e-07 - acc: 1.0000 - val_loss: 2.3874 - val_acc: 0.7970\n",
            "Epoch 148/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 2.7601e-07 - acc: 1.0000 - val_loss: 2.3846 - val_acc: 0.7970\n",
            "Epoch 149/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 2.6908e-07 - acc: 1.0000 - val_loss: 2.3932 - val_acc: 0.7970\n",
            "Epoch 150/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 2.5139e-07 - acc: 1.0000 - val_loss: 2.3935 - val_acc: 0.7970\n",
            "Epoch 151/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 2.4049e-07 - acc: 1.0000 - val_loss: 2.4052 - val_acc: 0.7970\n",
            "Epoch 152/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 2.2991e-07 - acc: 1.0000 - val_loss: 2.4147 - val_acc: 0.7970\n",
            "Epoch 153/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 2.2269e-07 - acc: 1.0000 - val_loss: 2.4069 - val_acc: 0.7970\n",
            "Epoch 154/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 2.1566e-07 - acc: 1.0000 - val_loss: 2.4096 - val_acc: 0.7970\n",
            "Epoch 155/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 2.0514e-07 - acc: 1.0000 - val_loss: 2.4033 - val_acc: 0.7970\n",
            "Epoch 156/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 1.9807e-07 - acc: 1.0000 - val_loss: 2.4197 - val_acc: 0.7970\n",
            "Epoch 157/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.8844e-07 - acc: 1.0000 - val_loss: 2.4134 - val_acc: 0.7970\n",
            "Epoch 158/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.8386e-07 - acc: 1.0000 - val_loss: 2.4214 - val_acc: 0.7970\n",
            "Epoch 159/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 1.7597e-07 - acc: 1.0000 - val_loss: 2.4218 - val_acc: 0.7970\n",
            "Epoch 160/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.7148e-07 - acc: 1.0000 - val_loss: 2.4140 - val_acc: 0.7970\n",
            "Epoch 161/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 1.6764e-07 - acc: 1.0000 - val_loss: 2.4316 - val_acc: 0.7970\n",
            "Epoch 162/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.6182e-07 - acc: 1.0000 - val_loss: 2.4352 - val_acc: 0.7970\n",
            "Epoch 163/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.5590e-07 - acc: 1.0000 - val_loss: 2.4549 - val_acc: 0.7970\n",
            "Epoch 164/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.5299e-07 - acc: 1.0000 - val_loss: 2.4391 - val_acc: 0.7970\n",
            "Epoch 165/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 1.4819e-07 - acc: 1.0000 - val_loss: 2.4471 - val_acc: 0.7970\n",
            "Epoch 166/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 1.4434e-07 - acc: 1.0000 - val_loss: 2.4504 - val_acc: 0.7970\n",
            "Epoch 167/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 1.4104e-07 - acc: 1.0000 - val_loss: 2.4507 - val_acc: 0.7970\n",
            "Epoch 168/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 1.3777e-07 - acc: 1.0000 - val_loss: 2.4651 - val_acc: 0.7970\n",
            "Epoch 169/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.3592e-07 - acc: 1.0000 - val_loss: 2.4654 - val_acc: 0.7970\n",
            "Epoch 170/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 1.3327e-07 - acc: 1.0000 - val_loss: 2.4631 - val_acc: 0.7970\n",
            "Epoch 171/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.3085e-07 - acc: 1.0000 - val_loss: 2.4682 - val_acc: 0.7970\n",
            "Epoch 172/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 1.2847e-07 - acc: 1.0000 - val_loss: 2.4671 - val_acc: 0.7970\n",
            "Epoch 173/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.2582e-07 - acc: 1.0000 - val_loss: 2.4608 - val_acc: 0.7970\n",
            "Epoch 174/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.2437e-07 - acc: 1.0000 - val_loss: 2.4708 - val_acc: 0.7970\n",
            "Epoch 175/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 1.2228e-07 - acc: 1.0000 - val_loss: 2.4774 - val_acc: 0.7970\n",
            "Epoch 176/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 1.2059e-07 - acc: 1.0000 - val_loss: 2.4713 - val_acc: 0.7970\n",
            "Epoch 177/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 1.1892e-07 - acc: 1.0000 - val_loss: 2.4729 - val_acc: 0.7970\n",
            "Epoch 178/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 1.1746e-07 - acc: 1.0000 - val_loss: 2.4833 - val_acc: 0.7970\n",
            "Epoch 179/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.1658e-07 - acc: 1.0000 - val_loss: 2.4876 - val_acc: 0.7970\n",
            "Epoch 180/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 1.1553e-07 - acc: 1.0000 - val_loss: 2.4851 - val_acc: 0.7970\n",
            "Epoch 181/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 1.1452e-07 - acc: 1.0000 - val_loss: 2.4879 - val_acc: 0.7970\n",
            "Epoch 182/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.1391e-07 - acc: 1.0000 - val_loss: 2.4912 - val_acc: 0.7970\n",
            "Epoch 183/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.1237e-07 - acc: 1.0000 - val_loss: 2.4919 - val_acc: 0.7970\n",
            "Epoch 184/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.1126e-07 - acc: 1.0000 - val_loss: 2.4966 - val_acc: 0.7970\n",
            "Epoch 185/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.1070e-07 - acc: 1.0000 - val_loss: 2.4958 - val_acc: 0.7970\n",
            "Epoch 186/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 1.1030e-07 - acc: 1.0000 - val_loss: 2.4983 - val_acc: 0.7970\n",
            "Epoch 187/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 1.0949e-07 - acc: 1.0000 - val_loss: 2.4900 - val_acc: 0.7970\n",
            "Epoch 188/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 1.0900e-07 - acc: 1.0000 - val_loss: 2.5053 - val_acc: 0.7970\n",
            "Epoch 189/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.0789e-07 - acc: 1.0000 - val_loss: 2.5086 - val_acc: 0.7970\n",
            "Epoch 190/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 1.0779e-07 - acc: 1.0000 - val_loss: 2.5038 - val_acc: 0.7970\n",
            "Epoch 191/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 1.0731e-07 - acc: 1.0000 - val_loss: 2.5091 - val_acc: 0.7970\n",
            "Epoch 192/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.0665e-07 - acc: 1.0000 - val_loss: 2.4980 - val_acc: 0.7970\n",
            "Epoch 193/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.0665e-07 - acc: 1.0000 - val_loss: 2.5162 - val_acc: 0.7970\n",
            "Epoch 194/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 1.0611e-07 - acc: 1.0000 - val_loss: 2.5144 - val_acc: 0.7970\n",
            "Epoch 195/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.0583e-07 - acc: 1.0000 - val_loss: 2.5161 - val_acc: 0.7970\n",
            "Epoch 196/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.0553e-07 - acc: 1.0000 - val_loss: 2.5121 - val_acc: 0.7970\n",
            "Epoch 197/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 1.0542e-07 - acc: 1.0000 - val_loss: 2.5131 - val_acc: 0.7970\n",
            "Epoch 198/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.0472e-07 - acc: 1.0000 - val_loss: 2.5198 - val_acc: 0.7970\n",
            "Epoch 199/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.0474e-07 - acc: 1.0000 - val_loss: 2.5236 - val_acc: 0.7970\n",
            "Epoch 200/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 1.0441e-07 - acc: 1.0000 - val_loss: 2.5250 - val_acc: 0.7970\n",
            "Epoch 201/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.0446e-07 - acc: 1.0000 - val_loss: 2.5264 - val_acc: 0.7970\n",
            "Epoch 202/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 1.0434e-07 - acc: 1.0000 - val_loss: 2.5250 - val_acc: 0.7970\n",
            "Epoch 203/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 1.0415e-07 - acc: 1.0000 - val_loss: 2.5195 - val_acc: 0.7970\n",
            "Epoch 204/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 1.0397e-07 - acc: 1.0000 - val_loss: 2.5208 - val_acc: 0.7970\n",
            "Epoch 205/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.0394e-07 - acc: 1.0000 - val_loss: 2.5304 - val_acc: 0.7970\n",
            "Epoch 206/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 1.0379e-07 - acc: 1.0000 - val_loss: 2.5279 - val_acc: 0.7970\n",
            "Epoch 207/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.0403e-07 - acc: 1.0000 - val_loss: 2.5223 - val_acc: 0.7970\n",
            "Epoch 208/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.0350e-07 - acc: 1.0000 - val_loss: 2.5259 - val_acc: 0.7970\n",
            "Epoch 209/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 1.0357e-07 - acc: 1.0000 - val_loss: 2.5246 - val_acc: 0.7970\n",
            "Epoch 210/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 1.0348e-07 - acc: 1.0000 - val_loss: 2.5260 - val_acc: 0.7970\n",
            "Epoch 211/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.0327e-07 - acc: 1.0000 - val_loss: 2.5315 - val_acc: 0.7970\n",
            "Epoch 212/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.0333e-07 - acc: 1.0000 - val_loss: 2.5193 - val_acc: 0.7970\n",
            "Epoch 213/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.0321e-07 - acc: 1.0000 - val_loss: 2.5279 - val_acc: 0.7970\n",
            "Epoch 214/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 1.0320e-07 - acc: 1.0000 - val_loss: 2.5380 - val_acc: 0.7970\n",
            "Epoch 215/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 1.0336e-07 - acc: 1.0000 - val_loss: 2.5271 - val_acc: 0.7970\n",
            "Epoch 216/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.0354e-07 - acc: 1.0000 - val_loss: 2.5178 - val_acc: 0.7970\n",
            "Epoch 217/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.0332e-07 - acc: 1.0000 - val_loss: 2.5167 - val_acc: 0.7970\n",
            "Epoch 218/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.0321e-07 - acc: 1.0000 - val_loss: 2.5155 - val_acc: 0.7970\n",
            "Epoch 219/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 1.0314e-07 - acc: 1.0000 - val_loss: 2.5226 - val_acc: 0.7970\n",
            "Epoch 220/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 1.0310e-07 - acc: 1.0000 - val_loss: 2.5185 - val_acc: 0.7970\n",
            "Epoch 221/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.0320e-07 - acc: 1.0000 - val_loss: 2.5327 - val_acc: 0.8045\n",
            "Epoch 222/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 1.0380e-07 - acc: 1.0000 - val_loss: 2.5246 - val_acc: 0.7970\n",
            "Epoch 223/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 1.0314e-07 - acc: 1.0000 - val_loss: 2.5148 - val_acc: 0.7970\n",
            "Epoch 224/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.0308e-07 - acc: 1.0000 - val_loss: 2.5115 - val_acc: 0.7970\n",
            "Epoch 225/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 1.0306e-07 - acc: 1.0000 - val_loss: 2.5167 - val_acc: 0.7970\n",
            "Epoch 226/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 1.0303e-07 - acc: 1.0000 - val_loss: 2.5059 - val_acc: 0.7970\n",
            "Epoch 227/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.0317e-07 - acc: 1.0000 - val_loss: 2.5251 - val_acc: 0.7970\n",
            "Epoch 228/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 1.0323e-07 - acc: 1.0000 - val_loss: 2.5189 - val_acc: 0.7970\n",
            "Epoch 229/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 1.0307e-07 - acc: 1.0000 - val_loss: 2.5240 - val_acc: 0.7970\n",
            "Epoch 230/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 1.0300e-07 - acc: 1.0000 - val_loss: 2.5201 - val_acc: 0.7970\n",
            "Epoch 231/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 1.0321e-07 - acc: 1.0000 - val_loss: 2.5194 - val_acc: 0.8045\n",
            "Epoch 232/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.2921 - acc: 0.9748 - val_loss: 2.2986 - val_acc: 0.8195\n",
            "Epoch 233/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.8693 - acc: 0.9168 - val_loss: 1.9077 - val_acc: 0.7744\n",
            "Epoch 234/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.4328 - acc: 0.9378 - val_loss: 1.7561 - val_acc: 0.8045\n",
            "Epoch 235/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.2291 - acc: 0.9622 - val_loss: 1.6917 - val_acc: 0.8271\n",
            "Epoch 236/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.1047 - acc: 0.9824 - val_loss: 1.6949 - val_acc: 0.8346\n",
            "Epoch 237/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0698 - acc: 0.9916 - val_loss: 1.6841 - val_acc: 0.8195\n",
            "Epoch 238/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0610 - acc: 0.9933 - val_loss: 1.6599 - val_acc: 0.8496\n",
            "Epoch 239/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0535 - acc: 0.9924 - val_loss: 1.7492 - val_acc: 0.8346\n",
            "Epoch 240/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0442 - acc: 0.9933 - val_loss: 1.7389 - val_acc: 0.8195\n",
            "Epoch 241/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0280 - acc: 0.9950 - val_loss: 1.8150 - val_acc: 0.8346\n",
            "Epoch 242/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0167 - acc: 0.9992 - val_loss: 1.8318 - val_acc: 0.8271\n",
            "Epoch 243/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0153 - acc: 0.9992 - val_loss: 1.8468 - val_acc: 0.8271\n",
            "Epoch 244/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0150 - acc: 0.9992 - val_loss: 1.8450 - val_acc: 0.8271\n",
            "Epoch 245/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0147 - acc: 0.9992 - val_loss: 1.8613 - val_acc: 0.8271\n",
            "Epoch 246/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0146 - acc: 0.9992 - val_loss: 1.8669 - val_acc: 0.8271\n",
            "Epoch 247/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0144 - acc: 0.9992 - val_loss: 1.8881 - val_acc: 0.8271\n",
            "Epoch 248/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0143 - acc: 0.9992 - val_loss: 1.8978 - val_acc: 0.8271\n",
            "Epoch 249/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0142 - acc: 0.9992 - val_loss: 1.9009 - val_acc: 0.8271\n",
            "Epoch 250/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0142 - acc: 0.9992 - val_loss: 1.9104 - val_acc: 0.8271\n",
            "Epoch 251/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0141 - acc: 0.9992 - val_loss: 1.9228 - val_acc: 0.8271\n",
            "Epoch 252/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0141 - acc: 0.9992 - val_loss: 1.9280 - val_acc: 0.8271\n",
            "Epoch 253/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0140 - acc: 0.9992 - val_loss: 1.9360 - val_acc: 0.8271\n",
            "Epoch 254/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0140 - acc: 0.9992 - val_loss: 1.9467 - val_acc: 0.8271\n",
            "Epoch 255/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0139 - acc: 0.9992 - val_loss: 1.9507 - val_acc: 0.8271\n",
            "Epoch 256/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0139 - acc: 0.9992 - val_loss: 1.9562 - val_acc: 0.8271\n",
            "Epoch 257/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0139 - acc: 0.9992 - val_loss: 1.9630 - val_acc: 0.8271\n",
            "Epoch 258/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0139 - acc: 0.9992 - val_loss: 1.9700 - val_acc: 0.8271\n",
            "Epoch 259/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0138 - acc: 0.9992 - val_loss: 1.9747 - val_acc: 0.8271\n",
            "Epoch 260/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0138 - acc: 0.9992 - val_loss: 1.9810 - val_acc: 0.8271\n",
            "Epoch 261/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0138 - acc: 0.9992 - val_loss: 1.9887 - val_acc: 0.8271\n",
            "Epoch 262/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0138 - acc: 0.9992 - val_loss: 1.9987 - val_acc: 0.8271\n",
            "Epoch 263/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0138 - acc: 0.9992 - val_loss: 2.0026 - val_acc: 0.8271\n",
            "Epoch 264/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0137 - acc: 0.9992 - val_loss: 2.0074 - val_acc: 0.8271\n",
            "Epoch 265/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0137 - acc: 0.9992 - val_loss: 2.0117 - val_acc: 0.8271\n",
            "Epoch 266/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0137 - acc: 0.9992 - val_loss: 2.0189 - val_acc: 0.8271\n",
            "Epoch 267/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0137 - acc: 0.9992 - val_loss: 2.0238 - val_acc: 0.8271\n",
            "Epoch 268/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0137 - acc: 0.9992 - val_loss: 2.0319 - val_acc: 0.8271\n",
            "Epoch 269/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0137 - acc: 0.9992 - val_loss: 2.0394 - val_acc: 0.8271\n",
            "Epoch 270/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0137 - acc: 0.9992 - val_loss: 2.0444 - val_acc: 0.8271\n",
            "Epoch 271/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0137 - acc: 0.9992 - val_loss: 2.0500 - val_acc: 0.8271\n",
            "Epoch 272/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0137 - acc: 0.9992 - val_loss: 2.0544 - val_acc: 0.8271\n",
            "Epoch 273/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0137 - acc: 0.9992 - val_loss: 2.0586 - val_acc: 0.8271\n",
            "Epoch 274/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.0651 - val_acc: 0.8271\n",
            "Epoch 275/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.0692 - val_acc: 0.8271\n",
            "Epoch 276/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.0719 - val_acc: 0.8271\n",
            "Epoch 277/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.0777 - val_acc: 0.8271\n",
            "Epoch 278/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.0794 - val_acc: 0.8271\n",
            "Epoch 279/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.0832 - val_acc: 0.8271\n",
            "Epoch 280/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.0917 - val_acc: 0.8271\n",
            "Epoch 281/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.0947 - val_acc: 0.8271\n",
            "Epoch 282/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.0967 - val_acc: 0.8271\n",
            "Epoch 283/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.0998 - val_acc: 0.8271\n",
            "Epoch 284/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1040 - val_acc: 0.8271\n",
            "Epoch 285/1500\n",
            "1190/1190 [==============================] - 0s 151us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1063 - val_acc: 0.8271\n",
            "Epoch 286/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1106 - val_acc: 0.8271\n",
            "Epoch 287/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1131 - val_acc: 0.8271\n",
            "Epoch 288/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1156 - val_acc: 0.8271\n",
            "Epoch 289/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1176 - val_acc: 0.8271\n",
            "Epoch 290/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1191 - val_acc: 0.8271\n",
            "Epoch 291/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1230 - val_acc: 0.8271\n",
            "Epoch 292/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1225 - val_acc: 0.8271\n",
            "Epoch 293/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1257 - val_acc: 0.8271\n",
            "Epoch 294/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1292 - val_acc: 0.8271\n",
            "Epoch 295/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1304 - val_acc: 0.8271\n",
            "Epoch 296/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1321 - val_acc: 0.8271\n",
            "Epoch 297/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1350 - val_acc: 0.8271\n",
            "Epoch 298/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1359 - val_acc: 0.8271\n",
            "Epoch 299/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1398 - val_acc: 0.8271\n",
            "Epoch 300/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1402 - val_acc: 0.8271\n",
            "Epoch 301/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1430 - val_acc: 0.8271\n",
            "Epoch 302/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1457 - val_acc: 0.8271\n",
            "Epoch 303/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1474 - val_acc: 0.8271\n",
            "Epoch 304/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1497 - val_acc: 0.8271\n",
            "Epoch 305/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1527 - val_acc: 0.8271\n",
            "Epoch 306/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1504 - val_acc: 0.8271\n",
            "Epoch 307/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1529 - val_acc: 0.8271\n",
            "Epoch 308/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1561 - val_acc: 0.8271\n",
            "Epoch 309/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1566 - val_acc: 0.8271\n",
            "Epoch 310/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1595 - val_acc: 0.8271\n",
            "Epoch 311/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1612 - val_acc: 0.8271\n",
            "Epoch 312/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1632 - val_acc: 0.8271\n",
            "Epoch 313/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1626 - val_acc: 0.8271\n",
            "Epoch 314/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1653 - val_acc: 0.8271\n",
            "Epoch 315/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1666 - val_acc: 0.8271\n",
            "Epoch 316/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1683 - val_acc: 0.8271\n",
            "Epoch 317/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1699 - val_acc: 0.8346\n",
            "Epoch 318/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1729 - val_acc: 0.8346\n",
            "Epoch 319/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0136 - acc: 0.9992 - val_loss: 2.1728 - val_acc: 0.8346\n",
            "Epoch 320/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1728 - val_acc: 0.8346\n",
            "Epoch 321/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1739 - val_acc: 0.8346\n",
            "Epoch 322/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1762 - val_acc: 0.8346\n",
            "Epoch 323/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1792 - val_acc: 0.8346\n",
            "Epoch 324/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1787 - val_acc: 0.8346\n",
            "Epoch 325/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1800 - val_acc: 0.8346\n",
            "Epoch 326/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1865 - val_acc: 0.8346\n",
            "Epoch 327/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1858 - val_acc: 0.8421\n",
            "Epoch 328/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1859 - val_acc: 0.8421\n",
            "Epoch 329/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1855 - val_acc: 0.8421\n",
            "Epoch 330/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1883 - val_acc: 0.8421\n",
            "Epoch 331/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1907 - val_acc: 0.8421\n",
            "Epoch 332/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1956 - val_acc: 0.8421\n",
            "Epoch 333/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1958 - val_acc: 0.8421\n",
            "Epoch 334/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1974 - val_acc: 0.8421\n",
            "Epoch 335/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1993 - val_acc: 0.8421\n",
            "Epoch 336/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.1989 - val_acc: 0.8421\n",
            "Epoch 337/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2014 - val_acc: 0.8421\n",
            "Epoch 338/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2030 - val_acc: 0.8421\n",
            "Epoch 339/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2036 - val_acc: 0.8421\n",
            "Epoch 340/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2062 - val_acc: 0.8421\n",
            "Epoch 341/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2117 - val_acc: 0.8421\n",
            "Epoch 342/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2130 - val_acc: 0.8421\n",
            "Epoch 343/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2157 - val_acc: 0.8421\n",
            "Epoch 344/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2138 - val_acc: 0.8421\n",
            "Epoch 345/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2157 - val_acc: 0.8421\n",
            "Epoch 346/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2164 - val_acc: 0.8421\n",
            "Epoch 347/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2165 - val_acc: 0.8421\n",
            "Epoch 348/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2185 - val_acc: 0.8421\n",
            "Epoch 349/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2197 - val_acc: 0.8421\n",
            "Epoch 350/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2184 - val_acc: 0.8346\n",
            "Epoch 351/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2212 - val_acc: 0.8346\n",
            "Epoch 352/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2202 - val_acc: 0.8346\n",
            "Epoch 353/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2204 - val_acc: 0.8346\n",
            "Epoch 354/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2217 - val_acc: 0.8346\n",
            "Epoch 355/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2230 - val_acc: 0.8346\n",
            "Epoch 356/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2246 - val_acc: 0.8346\n",
            "Epoch 357/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2247 - val_acc: 0.8346\n",
            "Epoch 358/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2255 - val_acc: 0.8346\n",
            "Epoch 359/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2255 - val_acc: 0.8346\n",
            "Epoch 360/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2273 - val_acc: 0.8346\n",
            "Epoch 361/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2259 - val_acc: 0.8346\n",
            "Epoch 362/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2280 - val_acc: 0.8346\n",
            "Epoch 363/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2289 - val_acc: 0.8271\n",
            "Epoch 364/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2304 - val_acc: 0.8346\n",
            "Epoch 365/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2328 - val_acc: 0.8346\n",
            "Epoch 366/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2319 - val_acc: 0.8346\n",
            "Epoch 367/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2326 - val_acc: 0.8271\n",
            "Epoch 368/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2372 - val_acc: 0.8271\n",
            "Epoch 369/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2353 - val_acc: 0.8271\n",
            "Epoch 370/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2390 - val_acc: 0.8271\n",
            "Epoch 371/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2386 - val_acc: 0.8271\n",
            "Epoch 372/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2397 - val_acc: 0.8271\n",
            "Epoch 373/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2417 - val_acc: 0.8271\n",
            "Epoch 374/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2453 - val_acc: 0.8271\n",
            "Epoch 375/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2439 - val_acc: 0.8271\n",
            "Epoch 376/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2456 - val_acc: 0.8271\n",
            "Epoch 377/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2461 - val_acc: 0.8271\n",
            "Epoch 378/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2505 - val_acc: 0.8271\n",
            "Epoch 379/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2504 - val_acc: 0.8271\n",
            "Epoch 380/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2511 - val_acc: 0.8271\n",
            "Epoch 381/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2528 - val_acc: 0.8271\n",
            "Epoch 382/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2592 - val_acc: 0.8271\n",
            "Epoch 383/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2592 - val_acc: 0.8271\n",
            "Epoch 384/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2614 - val_acc: 0.8271\n",
            "Epoch 385/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2595 - val_acc: 0.8271\n",
            "Epoch 386/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2595 - val_acc: 0.8271\n",
            "Epoch 387/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2605 - val_acc: 0.8271\n",
            "Epoch 388/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2612 - val_acc: 0.8271\n",
            "Epoch 389/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2633 - val_acc: 0.8271\n",
            "Epoch 390/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2629 - val_acc: 0.8271\n",
            "Epoch 391/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2626 - val_acc: 0.8271\n",
            "Epoch 392/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2645 - val_acc: 0.8271\n",
            "Epoch 393/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2637 - val_acc: 0.8271\n",
            "Epoch 394/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2641 - val_acc: 0.8271\n",
            "Epoch 395/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2646 - val_acc: 0.8271\n",
            "Epoch 396/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2652 - val_acc: 0.8271\n",
            "Epoch 397/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2651 - val_acc: 0.8271\n",
            "Epoch 398/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2673 - val_acc: 0.8271\n",
            "Epoch 399/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2674 - val_acc: 0.8271\n",
            "Epoch 400/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2685 - val_acc: 0.8271\n",
            "Epoch 401/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2709 - val_acc: 0.8271\n",
            "Epoch 402/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2697 - val_acc: 0.8271\n",
            "Epoch 403/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2703 - val_acc: 0.8195\n",
            "Epoch 404/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2709 - val_acc: 0.8195\n",
            "Epoch 405/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2717 - val_acc: 0.8195\n",
            "Epoch 406/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2722 - val_acc: 0.8195\n",
            "Epoch 407/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2720 - val_acc: 0.8195\n",
            "Epoch 408/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2738 - val_acc: 0.8195\n",
            "Epoch 409/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2739 - val_acc: 0.8195\n",
            "Epoch 410/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2740 - val_acc: 0.8195\n",
            "Epoch 411/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2740 - val_acc: 0.8195\n",
            "Epoch 412/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2766 - val_acc: 0.8195\n",
            "Epoch 413/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2762 - val_acc: 0.8195\n",
            "Epoch 414/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2753 - val_acc: 0.8195\n",
            "Epoch 415/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2745 - val_acc: 0.8195\n",
            "Epoch 416/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2762 - val_acc: 0.8195\n",
            "Epoch 417/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2753 - val_acc: 0.8195\n",
            "Epoch 418/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2757 - val_acc: 0.8195\n",
            "Epoch 419/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2785 - val_acc: 0.8195\n",
            "Epoch 420/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2795 - val_acc: 0.8195\n",
            "Epoch 421/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2761 - val_acc: 0.8195\n",
            "Epoch 422/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2773 - val_acc: 0.8195\n",
            "Epoch 423/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2780 - val_acc: 0.8195\n",
            "Epoch 424/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2775 - val_acc: 0.8195\n",
            "Epoch 425/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2798 - val_acc: 0.8195\n",
            "Epoch 426/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2779 - val_acc: 0.8195\n",
            "Epoch 427/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2805 - val_acc: 0.8271\n",
            "Epoch 428/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2781 - val_acc: 0.8195\n",
            "Epoch 429/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2767 - val_acc: 0.8271\n",
            "Epoch 430/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2762 - val_acc: 0.8271\n",
            "Epoch 431/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2738 - val_acc: 0.8271\n",
            "Epoch 432/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2767 - val_acc: 0.8271\n",
            "Epoch 433/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2780 - val_acc: 0.8271\n",
            "Epoch 434/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2744 - val_acc: 0.8271\n",
            "Epoch 435/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2768 - val_acc: 0.8271\n",
            "Epoch 436/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2743 - val_acc: 0.8271\n",
            "Epoch 437/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2786 - val_acc: 0.8271\n",
            "Epoch 438/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2753 - val_acc: 0.8271\n",
            "Epoch 439/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2768 - val_acc: 0.8271\n",
            "Epoch 440/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2754 - val_acc: 0.8271\n",
            "Epoch 441/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2752 - val_acc: 0.8271\n",
            "Epoch 442/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2725 - val_acc: 0.8271\n",
            "Epoch 443/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2695 - val_acc: 0.8271\n",
            "Epoch 444/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2669 - val_acc: 0.8271\n",
            "Epoch 445/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2708 - val_acc: 0.8271\n",
            "Epoch 446/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2709 - val_acc: 0.8271\n",
            "Epoch 447/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2698 - val_acc: 0.8271\n",
            "Epoch 448/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2655 - val_acc: 0.8271\n",
            "Epoch 449/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2828 - val_acc: 0.8195\n",
            "Epoch 450/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2775 - val_acc: 0.8195\n",
            "Epoch 451/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2788 - val_acc: 0.8195\n",
            "Epoch 452/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2794 - val_acc: 0.8271\n",
            "Epoch 453/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2774 - val_acc: 0.8195\n",
            "Epoch 454/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2787 - val_acc: 0.8195\n",
            "Epoch 455/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2958 - val_acc: 0.8195\n",
            "Epoch 456/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2954 - val_acc: 0.8195\n",
            "Epoch 457/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0135 - acc: 0.9992 - val_loss: 2.2789 - val_acc: 0.8271\n",
            "Epoch 458/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 1.1588 - acc: 0.8966 - val_loss: 2.6184 - val_acc: 0.8045\n",
            "Epoch 459/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.7081 - acc: 0.9294 - val_loss: 2.5170 - val_acc: 0.8120\n",
            "Epoch 460/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.2541 - acc: 0.9647 - val_loss: 2.4913 - val_acc: 0.8120\n",
            "Epoch 461/1500\n",
            "1190/1190 [==============================] - 0s 157us/step - loss: 0.1667 - acc: 0.9773 - val_loss: 2.3151 - val_acc: 0.7970\n",
            "Epoch 462/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0889 - acc: 0.9874 - val_loss: 2.2514 - val_acc: 0.7970\n",
            "Epoch 463/1500\n",
            "1190/1190 [==============================] - 0s 149us/step - loss: 0.0720 - acc: 0.9933 - val_loss: 2.2775 - val_acc: 0.7970\n",
            "Epoch 464/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0685 - acc: 0.9958 - val_loss: 2.2968 - val_acc: 0.7970\n",
            "Epoch 465/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0680 - acc: 0.9958 - val_loss: 2.3021 - val_acc: 0.7970\n",
            "Epoch 466/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0679 - acc: 0.9958 - val_loss: 2.3074 - val_acc: 0.7970\n",
            "Epoch 467/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0678 - acc: 0.9958 - val_loss: 2.3126 - val_acc: 0.7970\n",
            "Epoch 468/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0678 - acc: 0.9958 - val_loss: 2.3151 - val_acc: 0.7970\n",
            "Epoch 469/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0677 - acc: 0.9958 - val_loss: 2.3180 - val_acc: 0.7970\n",
            "Epoch 470/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0677 - acc: 0.9958 - val_loss: 2.3199 - val_acc: 0.7970\n",
            "Epoch 471/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0677 - acc: 0.9958 - val_loss: 2.3234 - val_acc: 0.7970\n",
            "Epoch 472/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0677 - acc: 0.9958 - val_loss: 2.3268 - val_acc: 0.7970\n",
            "Epoch 473/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0676 - acc: 0.9958 - val_loss: 2.3275 - val_acc: 0.7970\n",
            "Epoch 474/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0676 - acc: 0.9958 - val_loss: 2.3321 - val_acc: 0.7970\n",
            "Epoch 475/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0676 - acc: 0.9958 - val_loss: 2.3347 - val_acc: 0.7970\n",
            "Epoch 476/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0676 - acc: 0.9958 - val_loss: 2.3354 - val_acc: 0.7970\n",
            "Epoch 477/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0676 - acc: 0.9958 - val_loss: 2.3366 - val_acc: 0.7970\n",
            "Epoch 478/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0676 - acc: 0.9958 - val_loss: 2.3431 - val_acc: 0.7970\n",
            "Epoch 479/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0676 - acc: 0.9958 - val_loss: 2.3471 - val_acc: 0.7970\n",
            "Epoch 480/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0676 - acc: 0.9958 - val_loss: 2.3492 - val_acc: 0.7970\n",
            "Epoch 481/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0676 - acc: 0.9958 - val_loss: 2.3530 - val_acc: 0.7970\n",
            "Epoch 482/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3563 - val_acc: 0.7970\n",
            "Epoch 483/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3557 - val_acc: 0.7970\n",
            "Epoch 484/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3594 - val_acc: 0.7970\n",
            "Epoch 485/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3607 - val_acc: 0.7970\n",
            "Epoch 486/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3661 - val_acc: 0.7970\n",
            "Epoch 487/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3689 - val_acc: 0.7970\n",
            "Epoch 488/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3730 - val_acc: 0.7970\n",
            "Epoch 489/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3772 - val_acc: 0.7970\n",
            "Epoch 490/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3787 - val_acc: 0.7970\n",
            "Epoch 491/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3827 - val_acc: 0.7970\n",
            "Epoch 492/1500\n",
            "1190/1190 [==============================] - 0s 154us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3855 - val_acc: 0.7970\n",
            "Epoch 493/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3865 - val_acc: 0.7970\n",
            "Epoch 494/1500\n",
            "1190/1190 [==============================] - 0s 149us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3922 - val_acc: 0.7970\n",
            "Epoch 495/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3893 - val_acc: 0.7970\n",
            "Epoch 496/1500\n",
            "1190/1190 [==============================] - 0s 149us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3973 - val_acc: 0.7970\n",
            "Epoch 497/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.3944 - val_acc: 0.7970\n",
            "Epoch 498/1500\n",
            "1190/1190 [==============================] - 0s 150us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4006 - val_acc: 0.7970\n",
            "Epoch 499/1500\n",
            "1190/1190 [==============================] - 0s 149us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4052 - val_acc: 0.7970\n",
            "Epoch 500/1500\n",
            "1190/1190 [==============================] - 0s 150us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4089 - val_acc: 0.7970\n",
            "Epoch 501/1500\n",
            "1190/1190 [==============================] - 0s 152us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4125 - val_acc: 0.7970\n",
            "Epoch 502/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4124 - val_acc: 0.8045\n",
            "Epoch 503/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4134 - val_acc: 0.7970\n",
            "Epoch 504/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4184 - val_acc: 0.8045\n",
            "Epoch 505/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4218 - val_acc: 0.8045\n",
            "Epoch 506/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4248 - val_acc: 0.8045\n",
            "Epoch 507/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4278 - val_acc: 0.8045\n",
            "Epoch 508/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4297 - val_acc: 0.8045\n",
            "Epoch 509/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4339 - val_acc: 0.8045\n",
            "Epoch 510/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4383 - val_acc: 0.8045\n",
            "Epoch 511/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0675 - acc: 0.9958 - val_loss: 2.4419 - val_acc: 0.8045\n",
            "Epoch 512/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4436 - val_acc: 0.8045\n",
            "Epoch 513/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4463 - val_acc: 0.8045\n",
            "Epoch 514/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4503 - val_acc: 0.8045\n",
            "Epoch 515/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4509 - val_acc: 0.8045\n",
            "Epoch 516/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4487 - val_acc: 0.8045\n",
            "Epoch 517/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4484 - val_acc: 0.8045\n",
            "Epoch 518/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4507 - val_acc: 0.8045\n",
            "Epoch 519/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4521 - val_acc: 0.8045\n",
            "Epoch 520/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4546 - val_acc: 0.8045\n",
            "Epoch 521/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4577 - val_acc: 0.8045\n",
            "Epoch 522/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4603 - val_acc: 0.8045\n",
            "Epoch 523/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4601 - val_acc: 0.8045\n",
            "Epoch 524/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4616 - val_acc: 0.8045\n",
            "Epoch 525/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4627 - val_acc: 0.8045\n",
            "Epoch 526/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4629 - val_acc: 0.8045\n",
            "Epoch 527/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4637 - val_acc: 0.8045\n",
            "Epoch 528/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4645 - val_acc: 0.8045\n",
            "Epoch 529/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4650 - val_acc: 0.8045\n",
            "Epoch 530/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4658 - val_acc: 0.8045\n",
            "Epoch 531/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4672 - val_acc: 0.8045\n",
            "Epoch 532/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4670 - val_acc: 0.8045\n",
            "Epoch 533/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4678 - val_acc: 0.8045\n",
            "Epoch 534/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4690 - val_acc: 0.8045\n",
            "Epoch 535/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4684 - val_acc: 0.8045\n",
            "Epoch 536/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4748 - val_acc: 0.8045\n",
            "Epoch 537/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4769 - val_acc: 0.8045\n",
            "Epoch 538/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4758 - val_acc: 0.8045\n",
            "Epoch 539/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4767 - val_acc: 0.8045\n",
            "Epoch 540/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4735 - val_acc: 0.8045\n",
            "Epoch 541/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4778 - val_acc: 0.8045\n",
            "Epoch 542/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4787 - val_acc: 0.8045\n",
            "Epoch 543/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4785 - val_acc: 0.8045\n",
            "Epoch 544/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4768 - val_acc: 0.8045\n",
            "Epoch 545/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4768 - val_acc: 0.8045\n",
            "Epoch 546/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4783 - val_acc: 0.8045\n",
            "Epoch 547/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4800 - val_acc: 0.8045\n",
            "Epoch 548/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4795 - val_acc: 0.8045\n",
            "Epoch 549/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4812 - val_acc: 0.8045\n",
            "Epoch 550/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4828 - val_acc: 0.8045\n",
            "Epoch 551/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4857 - val_acc: 0.8045\n",
            "Epoch 552/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4846 - val_acc: 0.8045\n",
            "Epoch 553/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4866 - val_acc: 0.8045\n",
            "Epoch 554/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4871 - val_acc: 0.8045\n",
            "Epoch 555/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4880 - val_acc: 0.8045\n",
            "Epoch 556/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4898 - val_acc: 0.8045\n",
            "Epoch 557/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4905 - val_acc: 0.8045\n",
            "Epoch 558/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4920 - val_acc: 0.8045\n",
            "Epoch 559/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4932 - val_acc: 0.8045\n",
            "Epoch 560/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4943 - val_acc: 0.8045\n",
            "Epoch 561/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4967 - val_acc: 0.8045\n",
            "Epoch 562/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4993 - val_acc: 0.8045\n",
            "Epoch 563/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4975 - val_acc: 0.8045\n",
            "Epoch 564/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.4996 - val_acc: 0.8045\n",
            "Epoch 565/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5006 - val_acc: 0.8045\n",
            "Epoch 566/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5040 - val_acc: 0.8045\n",
            "Epoch 567/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5049 - val_acc: 0.8045\n",
            "Epoch 568/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5026 - val_acc: 0.8045\n",
            "Epoch 569/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5077 - val_acc: 0.8045\n",
            "Epoch 570/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5098 - val_acc: 0.8045\n",
            "Epoch 571/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5106 - val_acc: 0.8045\n",
            "Epoch 572/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5105 - val_acc: 0.8045\n",
            "Epoch 573/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5141 - val_acc: 0.8045\n",
            "Epoch 574/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5158 - val_acc: 0.8045\n",
            "Epoch 575/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5174 - val_acc: 0.8045\n",
            "Epoch 576/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5143 - val_acc: 0.8045\n",
            "Epoch 577/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5158 - val_acc: 0.8045\n",
            "Epoch 578/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5212 - val_acc: 0.8045\n",
            "Epoch 579/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5200 - val_acc: 0.8045\n",
            "Epoch 580/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5199 - val_acc: 0.8045\n",
            "Epoch 581/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5219 - val_acc: 0.8045\n",
            "Epoch 582/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5244 - val_acc: 0.8045\n",
            "Epoch 583/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5255 - val_acc: 0.8045\n",
            "Epoch 584/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5257 - val_acc: 0.8045\n",
            "Epoch 585/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5286 - val_acc: 0.8045\n",
            "Epoch 586/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5307 - val_acc: 0.8045\n",
            "Epoch 587/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5342 - val_acc: 0.8045\n",
            "Epoch 588/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5348 - val_acc: 0.8045\n",
            "Epoch 589/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5356 - val_acc: 0.8045\n",
            "Epoch 590/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5361 - val_acc: 0.8045\n",
            "Epoch 591/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5402 - val_acc: 0.8045\n",
            "Epoch 592/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5430 - val_acc: 0.8045\n",
            "Epoch 593/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5410 - val_acc: 0.8045\n",
            "Epoch 594/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5419 - val_acc: 0.8045\n",
            "Epoch 595/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5404 - val_acc: 0.8045\n",
            "Epoch 596/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5456 - val_acc: 0.8045\n",
            "Epoch 597/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5479 - val_acc: 0.8045\n",
            "Epoch 598/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5498 - val_acc: 0.8045\n",
            "Epoch 599/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5517 - val_acc: 0.8045\n",
            "Epoch 600/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5525 - val_acc: 0.8045\n",
            "Epoch 601/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5531 - val_acc: 0.8045\n",
            "Epoch 602/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5556 - val_acc: 0.8045\n",
            "Epoch 603/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5541 - val_acc: 0.8045\n",
            "Epoch 604/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5555 - val_acc: 0.8045\n",
            "Epoch 605/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5606 - val_acc: 0.8045\n",
            "Epoch 606/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5628 - val_acc: 0.8045\n",
            "Epoch 607/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5621 - val_acc: 0.8045\n",
            "Epoch 608/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5626 - val_acc: 0.8045\n",
            "Epoch 609/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5623 - val_acc: 0.8045\n",
            "Epoch 610/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5643 - val_acc: 0.8045\n",
            "Epoch 611/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5666 - val_acc: 0.8045\n",
            "Epoch 612/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5655 - val_acc: 0.8045\n",
            "Epoch 613/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5654 - val_acc: 0.8045\n",
            "Epoch 614/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5674 - val_acc: 0.8045\n",
            "Epoch 615/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5663 - val_acc: 0.8045\n",
            "Epoch 616/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5680 - val_acc: 0.8045\n",
            "Epoch 617/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5684 - val_acc: 0.8045\n",
            "Epoch 618/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5679 - val_acc: 0.8045\n",
            "Epoch 619/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5690 - val_acc: 0.8045\n",
            "Epoch 620/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5705 - val_acc: 0.8045\n",
            "Epoch 621/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5705 - val_acc: 0.8045\n",
            "Epoch 622/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5720 - val_acc: 0.8045\n",
            "Epoch 623/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5730 - val_acc: 0.8045\n",
            "Epoch 624/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5759 - val_acc: 0.8045\n",
            "Epoch 625/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5768 - val_acc: 0.8045\n",
            "Epoch 626/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5770 - val_acc: 0.8045\n",
            "Epoch 627/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5745 - val_acc: 0.8045\n",
            "Epoch 628/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5780 - val_acc: 0.8045\n",
            "Epoch 629/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5751 - val_acc: 0.8045\n",
            "Epoch 630/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5783 - val_acc: 0.8045\n",
            "Epoch 631/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5783 - val_acc: 0.8045\n",
            "Epoch 632/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5788 - val_acc: 0.8045\n",
            "Epoch 633/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5787 - val_acc: 0.8045\n",
            "Epoch 634/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5782 - val_acc: 0.8045\n",
            "Epoch 635/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5789 - val_acc: 0.8045\n",
            "Epoch 636/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5799 - val_acc: 0.8045\n",
            "Epoch 637/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5795 - val_acc: 0.8045\n",
            "Epoch 638/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5801 - val_acc: 0.8045\n",
            "Epoch 639/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5795 - val_acc: 0.8045\n",
            "Epoch 640/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5806 - val_acc: 0.8045\n",
            "Epoch 641/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5819 - val_acc: 0.8045\n",
            "Epoch 642/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5824 - val_acc: 0.8045\n",
            "Epoch 643/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5878 - val_acc: 0.8045\n",
            "Epoch 644/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5903 - val_acc: 0.8045\n",
            "Epoch 645/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5902 - val_acc: 0.8045\n",
            "Epoch 646/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5900 - val_acc: 0.8045\n",
            "Epoch 647/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5913 - val_acc: 0.8045\n",
            "Epoch 648/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5926 - val_acc: 0.8045\n",
            "Epoch 649/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5921 - val_acc: 0.8045\n",
            "Epoch 650/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5932 - val_acc: 0.8045\n",
            "Epoch 651/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5929 - val_acc: 0.8045\n",
            "Epoch 652/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5932 - val_acc: 0.8045\n",
            "Epoch 653/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5930 - val_acc: 0.8045\n",
            "Epoch 654/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5945 - val_acc: 0.8045\n",
            "Epoch 655/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5954 - val_acc: 0.8045\n",
            "Epoch 656/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5969 - val_acc: 0.8045\n",
            "Epoch 657/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5946 - val_acc: 0.8045\n",
            "Epoch 658/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5994 - val_acc: 0.8045\n",
            "Epoch 659/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5999 - val_acc: 0.8045\n",
            "Epoch 660/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5951 - val_acc: 0.8045\n",
            "Epoch 661/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5954 - val_acc: 0.8045\n",
            "Epoch 662/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5945 - val_acc: 0.8045\n",
            "Epoch 663/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5965 - val_acc: 0.8045\n",
            "Epoch 664/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5954 - val_acc: 0.8045\n",
            "Epoch 665/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5916 - val_acc: 0.8045\n",
            "Epoch 666/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5927 - val_acc: 0.8045\n",
            "Epoch 667/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5951 - val_acc: 0.8045\n",
            "Epoch 668/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5955 - val_acc: 0.8045\n",
            "Epoch 669/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5963 - val_acc: 0.8045\n",
            "Epoch 670/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5952 - val_acc: 0.8045\n",
            "Epoch 671/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5943 - val_acc: 0.8045\n",
            "Epoch 672/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5973 - val_acc: 0.8045\n",
            "Epoch 673/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5985 - val_acc: 0.8045\n",
            "Epoch 674/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6009 - val_acc: 0.8045\n",
            "Epoch 675/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5946 - val_acc: 0.8045\n",
            "Epoch 676/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5942 - val_acc: 0.8045\n",
            "Epoch 677/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5949 - val_acc: 0.8045\n",
            "Epoch 678/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.5969 - val_acc: 0.8045\n",
            "Epoch 679/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 680/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 681/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 682/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 683/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 684/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 685/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 686/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 687/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 688/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 689/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 690/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 691/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 692/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 693/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 694/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 695/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 696/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 697/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 698/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 699/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 700/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 701/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 702/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 703/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 704/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 705/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 706/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 707/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 708/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 709/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 710/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 711/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 712/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 713/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 714/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 715/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 716/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 717/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 718/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 719/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 720/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 721/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 722/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 723/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 724/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 725/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 726/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 727/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 728/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 729/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 730/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 731/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 732/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 733/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 734/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 735/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 736/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 737/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 738/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 739/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 740/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 741/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 742/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 743/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 744/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 745/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 746/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 747/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 748/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 749/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 750/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 751/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 752/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 753/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 754/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 755/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 756/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 757/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 758/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 759/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 760/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 761/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 762/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 763/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 764/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 765/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 766/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 767/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 768/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 769/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 770/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 771/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 772/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 773/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 774/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 775/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 776/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 777/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 778/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 779/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 780/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 781/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 782/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 783/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 784/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 785/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 786/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 787/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 788/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 789/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 790/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 791/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 792/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 793/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 794/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 795/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 796/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 797/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 798/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 799/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 800/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 801/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 802/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 803/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 804/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 805/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 806/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 807/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 808/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 809/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 810/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 811/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 812/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 813/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 814/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 815/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 816/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 817/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 818/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 819/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 820/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 821/1500\n",
            "1190/1190 [==============================] - 0s 152us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 822/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 823/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 824/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 825/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 826/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 827/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 828/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 829/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 830/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 831/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 832/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 833/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 834/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 835/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 836/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 837/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 838/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 839/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 840/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 841/1500\n",
            "1190/1190 [==============================] - 0s 150us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 842/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 843/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 844/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 845/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 846/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 847/1500\n",
            "1190/1190 [==============================] - 0s 151us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 848/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 849/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 850/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 851/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 852/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 853/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 854/1500\n",
            "1190/1190 [==============================] - 0s 151us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 855/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 856/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 857/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 858/1500\n",
            "1190/1190 [==============================] - 0s 149us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 859/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 860/1500\n",
            "1190/1190 [==============================] - 0s 151us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 861/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 862/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 863/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 864/1500\n",
            "1190/1190 [==============================] - 0s 152us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 865/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 866/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 867/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 868/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 869/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 870/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 871/1500\n",
            "1190/1190 [==============================] - 0s 150us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 872/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 873/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 874/1500\n",
            "1190/1190 [==============================] - 0s 151us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 875/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 876/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 877/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 878/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 879/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 880/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 881/1500\n",
            "1190/1190 [==============================] - 0s 149us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 882/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 883/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 884/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 885/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 886/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 887/1500\n",
            "1190/1190 [==============================] - 0s 149us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 888/1500\n",
            "1190/1190 [==============================] - 0s 149us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 889/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 890/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 891/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 892/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 893/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 894/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 895/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 896/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 897/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 898/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 899/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 900/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 901/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 902/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 903/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 904/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 905/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 906/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 907/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 908/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 909/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 910/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 911/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 912/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 913/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 914/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 915/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 916/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 917/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 918/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 919/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 920/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 921/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 922/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 923/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 924/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 925/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 926/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 927/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 928/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 929/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 930/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 931/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 932/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 933/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 934/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 935/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 936/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 937/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 938/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 939/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 940/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 941/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 942/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 943/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 944/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 945/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 946/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 947/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 948/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 949/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 950/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 951/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 952/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 953/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 954/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 955/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 956/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 957/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 958/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 959/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 960/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 961/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 962/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 963/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 964/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 965/1500\n",
            "1190/1190 [==============================] - 0s 150us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 966/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 967/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 968/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 969/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 970/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 971/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 972/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 973/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 974/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 975/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 976/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 977/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 978/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 979/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 980/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 981/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 982/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 983/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 984/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 985/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 986/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 987/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 988/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 989/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 990/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 991/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 992/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 993/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 994/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 995/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 996/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 997/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 998/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 999/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1000/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1001/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1002/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1003/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1004/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1005/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1006/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1007/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1008/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1009/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1010/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1011/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1012/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1013/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1014/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1015/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1016/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1017/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1018/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1019/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1020/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1021/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1022/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1023/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1024/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1025/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1026/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1027/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1028/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1029/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1030/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1031/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1032/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1033/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1034/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1035/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1036/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1037/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1038/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1039/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1040/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1041/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1042/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1043/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1044/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1045/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1046/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1047/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1048/1500\n",
            "1190/1190 [==============================] - 0s 132us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1049/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1050/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1051/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1052/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1053/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1054/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1055/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1056/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1057/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1058/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1059/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1060/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1061/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1062/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1063/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1064/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1065/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1066/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1067/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1068/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1069/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1070/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1071/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1072/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1073/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1074/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1075/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1076/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1077/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1078/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1079/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1080/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1081/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1082/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1083/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1084/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1085/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1086/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1087/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1088/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1089/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1090/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1091/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1092/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1093/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1094/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1095/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1096/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1097/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1098/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1099/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1100/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1101/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1102/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1103/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1104/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1105/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1106/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1107/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1108/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1109/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1110/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1111/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1112/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1113/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1114/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1115/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1116/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1117/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1118/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1119/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1120/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1121/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1122/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1123/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1124/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1125/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1126/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1127/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1128/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1129/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1130/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1131/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1132/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1133/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1134/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1135/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1136/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1137/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1138/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1139/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1140/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1141/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1142/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1143/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1144/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1145/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1146/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1147/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1148/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1149/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1150/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1151/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1152/1500\n",
            "1190/1190 [==============================] - 0s 132us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1153/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1154/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1155/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1156/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1157/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1158/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1159/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1160/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1161/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1162/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1163/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1164/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1165/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1166/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1167/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1168/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1169/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1170/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1171/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1172/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1173/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1174/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1175/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1176/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1177/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1178/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1179/1500\n",
            "1190/1190 [==============================] - 0s 163us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1180/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1181/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1182/1500\n",
            "1190/1190 [==============================] - 0s 153us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1183/1500\n",
            "1190/1190 [==============================] - 0s 149us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1184/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1185/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1186/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1187/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1188/1500\n",
            "1190/1190 [==============================] - 0s 151us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1189/1500\n",
            "1190/1190 [==============================] - 0s 152us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1190/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1191/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1192/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1193/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1194/1500\n",
            "1190/1190 [==============================] - 0s 150us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1195/1500\n",
            "1190/1190 [==============================] - 0s 150us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1196/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1197/1500\n",
            "1190/1190 [==============================] - 0s 150us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1198/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1199/1500\n",
            "1190/1190 [==============================] - 0s 146us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1200/1500\n",
            "1190/1190 [==============================] - 0s 151us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1201/1500\n",
            "1190/1190 [==============================] - 0s 151us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1202/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1203/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1204/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1205/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1206/1500\n",
            "1190/1190 [==============================] - 0s 152us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1207/1500\n",
            "1190/1190 [==============================] - 0s 148us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1208/1500\n",
            "1190/1190 [==============================] - 0s 147us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1209/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1210/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1211/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1212/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1213/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1214/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1215/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1216/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1217/1500\n",
            "1190/1190 [==============================] - 0s 152us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1218/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1219/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1220/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1221/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1222/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1223/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1224/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1225/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1226/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1227/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1228/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1229/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1230/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1231/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1232/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1233/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1234/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1235/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1236/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1237/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1238/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1239/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1240/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1241/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1242/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1243/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1244/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1245/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1246/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1247/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1248/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1249/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1250/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1251/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1252/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1253/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1254/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1255/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1256/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1257/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1258/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1259/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1260/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1261/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1262/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1263/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1264/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1265/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1266/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1267/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1268/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1269/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1270/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1271/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1272/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1273/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1274/1500\n",
            "1190/1190 [==============================] - 0s 132us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1275/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1276/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1277/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1278/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1279/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1280/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1281/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1282/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1283/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1284/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1285/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1286/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1287/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1288/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1289/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1290/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1291/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1292/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1293/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1294/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1295/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1296/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1297/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1298/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1299/1500\n",
            "1190/1190 [==============================] - 0s 132us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1300/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1301/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1302/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1303/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1304/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1305/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1306/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1307/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1308/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1309/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1310/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1311/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1312/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1313/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1314/1500\n",
            "1190/1190 [==============================] - 0s 132us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1315/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1316/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1317/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1318/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1319/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1320/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1321/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1322/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1323/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1324/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1325/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1326/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1327/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1328/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1329/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1330/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1331/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1332/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1333/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1334/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1335/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1336/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1337/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1338/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1339/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1340/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1341/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1342/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1343/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1344/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1345/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1346/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1347/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1348/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1349/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1350/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1351/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1352/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1353/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1354/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1355/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1356/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1357/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1358/1500\n",
            "1190/1190 [==============================] - 0s 145us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1359/1500\n",
            "1190/1190 [==============================] - 0s 138us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1360/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1361/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1362/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1363/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1364/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1365/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1366/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1367/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1368/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1369/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1370/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1371/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1372/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1373/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1374/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1375/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1376/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1377/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1378/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1379/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1380/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1381/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1382/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1383/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1384/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1385/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1386/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1387/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1388/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1389/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1390/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1391/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1392/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1393/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1394/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1395/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1396/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1397/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1398/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1399/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1400/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1401/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1402/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1403/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1404/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1405/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1406/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1407/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1408/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1409/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1410/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1411/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1412/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1413/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1414/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1415/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1416/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1417/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1418/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1419/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1420/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1421/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1422/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1423/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1424/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1425/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1426/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1427/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1428/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1429/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1430/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1431/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1432/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1433/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1434/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1435/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1436/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1437/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1438/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1439/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1440/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1441/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1442/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1443/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1444/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1445/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1446/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1447/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1448/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1449/1500\n",
            "1190/1190 [==============================] - 0s 133us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1450/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1451/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1452/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1453/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1454/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1455/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1456/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1457/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1458/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1459/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1460/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1461/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1462/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1463/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1464/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1465/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1466/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1467/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1468/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1469/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1470/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1471/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1472/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1473/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1474/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1475/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1476/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1477/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1478/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1479/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1480/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1481/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1482/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1483/1500\n",
            "1190/1190 [==============================] - 0s 136us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1484/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1485/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1486/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1487/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1488/1500\n",
            "1190/1190 [==============================] - 0s 139us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1489/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1490/1500\n",
            "1190/1190 [==============================] - 0s 144us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1491/1500\n",
            "1190/1190 [==============================] - 0s 137us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1492/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1493/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1494/1500\n",
            "1190/1190 [==============================] - 0s 143us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1495/1500\n",
            "1190/1190 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1496/1500\n",
            "1190/1190 [==============================] - 0s 134us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1497/1500\n",
            "1190/1190 [==============================] - 0s 141us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1498/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1499/1500\n",
            "1190/1190 [==============================] - 0s 135us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n",
            "Epoch 1500/1500\n",
            "1190/1190 [==============================] - 0s 142us/step - loss: 0.0674 - acc: 0.9958 - val_loss: 2.6004 - val_acc: 0.8045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f999fa200f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "metadata": {
        "id": "dDebIAnSD9sh",
        "colab_type": "code",
        "outputId": "00e4c8e8-8986-47c7-d3aa-da2e04b2c90b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "scores = neural_net.evaluate(x_train,y_train)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1323/1323 [==============================] - 0s 25us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bb8dNSkWF_b4",
        "colab_type": "code",
        "outputId": "47199527-159b-4d15-c78f-d505732e958c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"\\n%s: %.2f%%\" % (neural_net.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "acc: 97.66%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zxZrmp6DlFwy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train_pred_neural_net = neural_net.predict_classes(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QX9L9jLzGryC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_neural_net = neural_net.predict_classes(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jZvUGahklMk7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "561dfe40-7493-418b-bc19-735077ed720e"
      },
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_train,y_train_pred_neural_net)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1099,   11],\n",
              "       [  20,  193]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "metadata": {
        "id": "M0bWTMkzGz-R",
        "colab_type": "code",
        "outputId": "015046c4-bbd0-4783-aec2-8a29e153053e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test,y_test_pred_neural_net)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[116,   7],\n",
              "       [ 16,   8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "metadata": {
        "id": "nYxaBbxXKJRk",
        "colab_type": "code",
        "outputId": "8ac363c9-5c54-469a-f43b-288445edc2ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test,y_test_pred_neural_net).ravel()\n",
        "print (tn)\n",
        "print (fp)\n",
        "print (fn)\n",
        "print (tp)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "116\n",
            "7\n",
            "16\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j-kA3FNeXwDi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HH84zkjiaEFC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oversampler=SMOTE(random_state=0)\n",
        "oversampler=SMOTE(random_state=0)\n",
        "smote_train, smote_target = oversampler.fit_sample(x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZCwTZbvebS2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f856ba0e-a9b5-4ce2-8a2f-940534ef5b0e"
      },
      "cell_type": "code",
      "source": [
        "print (x_train.shape)\n",
        "print (y_train.shape)\n",
        "print (smote_train.shape)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1323, 72)\n",
            "(1323,)\n",
            "(2220, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AvpCL1xJbbzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53385
        },
        "outputId": "240fee3b-4705-421e-88c2-a2aeacd62250"
      },
      "cell_type": "code",
      "source": [
        "neural_net.fit(smote_train, smote_target, validation_split=0.1,epochs=1500, batch_size=10)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1998 samples, validate on 222 samples\n",
            "Epoch 1/1500\n",
            "1998/1998 [==============================] - 1s 277us/step - loss: 0.6255 - acc: 0.6496 - val_loss: 0.6349 - val_acc: 0.7117\n",
            "Epoch 2/1500\n",
            "1998/1998 [==============================] - 0s 135us/step - loss: 0.5067 - acc: 0.7713 - val_loss: 0.5255 - val_acc: 0.7973\n",
            "Epoch 3/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.4471 - acc: 0.8128 - val_loss: 0.4808 - val_acc: 0.8108\n",
            "Epoch 4/1500\n",
            "1998/1998 [==============================] - 0s 132us/step - loss: 0.4173 - acc: 0.8208 - val_loss: 0.5190 - val_acc: 0.7613\n",
            "Epoch 5/1500\n",
            "1998/1998 [==============================] - 0s 134us/step - loss: 0.3999 - acc: 0.8268 - val_loss: 0.4056 - val_acc: 0.8288\n",
            "Epoch 6/1500\n",
            "1998/1998 [==============================] - 0s 131us/step - loss: 0.3882 - acc: 0.8363 - val_loss: 0.3798 - val_acc: 0.8243\n",
            "Epoch 7/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.3772 - acc: 0.8453 - val_loss: 0.4161 - val_acc: 0.7928\n",
            "Epoch 8/1500\n",
            "1998/1998 [==============================] - 0s 133us/step - loss: 0.3662 - acc: 0.8524 - val_loss: 0.3597 - val_acc: 0.8423\n",
            "Epoch 9/1500\n",
            "1998/1998 [==============================] - 0s 131us/step - loss: 0.3576 - acc: 0.8569 - val_loss: 0.3629 - val_acc: 0.8378\n",
            "Epoch 10/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.3477 - acc: 0.8639 - val_loss: 0.3554 - val_acc: 0.8604\n",
            "Epoch 11/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.3392 - acc: 0.8694 - val_loss: 0.3577 - val_acc: 0.8514\n",
            "Epoch 12/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.3316 - acc: 0.8719 - val_loss: 0.3166 - val_acc: 0.8739\n",
            "Epoch 13/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.3222 - acc: 0.8764 - val_loss: 0.2822 - val_acc: 0.8919\n",
            "Epoch 14/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.3160 - acc: 0.8809 - val_loss: 0.3025 - val_acc: 0.8964\n",
            "Epoch 15/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.3094 - acc: 0.8794 - val_loss: 0.3305 - val_acc: 0.8739\n",
            "Epoch 16/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.3050 - acc: 0.8834 - val_loss: 0.3174 - val_acc: 0.8964\n",
            "Epoch 17/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.3012 - acc: 0.8854 - val_loss: 0.2871 - val_acc: 0.9054\n",
            "Epoch 18/1500\n",
            "1998/1998 [==============================] - 0s 134us/step - loss: 0.2957 - acc: 0.8854 - val_loss: 0.2710 - val_acc: 0.9189\n",
            "Epoch 19/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2928 - acc: 0.8899 - val_loss: 0.2869 - val_acc: 0.8964\n",
            "Epoch 20/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2891 - acc: 0.8914 - val_loss: 0.3402 - val_acc: 0.8694\n",
            "Epoch 21/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2838 - acc: 0.8944 - val_loss: 0.3004 - val_acc: 0.9009\n",
            "Epoch 22/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.2810 - acc: 0.8954 - val_loss: 0.2813 - val_acc: 0.9099\n",
            "Epoch 23/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.2778 - acc: 0.9024 - val_loss: 0.2812 - val_acc: 0.9144\n",
            "Epoch 24/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2751 - acc: 0.9019 - val_loss: 0.3121 - val_acc: 0.8874\n",
            "Epoch 25/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.2720 - acc: 0.9014 - val_loss: 0.2359 - val_acc: 0.9414\n",
            "Epoch 26/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.2683 - acc: 0.9079 - val_loss: 0.2960 - val_acc: 0.8919\n",
            "Epoch 27/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2668 - acc: 0.9039 - val_loss: 0.2670 - val_acc: 0.9144\n",
            "Epoch 28/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.2648 - acc: 0.9044 - val_loss: 0.2996 - val_acc: 0.8829\n",
            "Epoch 29/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2623 - acc: 0.9079 - val_loss: 0.2675 - val_acc: 0.9189\n",
            "Epoch 30/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.2603 - acc: 0.9084 - val_loss: 0.2705 - val_acc: 0.9009\n",
            "Epoch 31/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.2555 - acc: 0.9124 - val_loss: 0.2212 - val_acc: 0.9414\n",
            "Epoch 32/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.2549 - acc: 0.9099 - val_loss: 0.2540 - val_acc: 0.9144\n",
            "Epoch 33/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.2544 - acc: 0.9079 - val_loss: 0.2310 - val_acc: 0.9324\n",
            "Epoch 34/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.2503 - acc: 0.9099 - val_loss: 0.2331 - val_acc: 0.9234\n",
            "Epoch 35/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.2495 - acc: 0.9129 - val_loss: 0.2719 - val_acc: 0.8964\n",
            "Epoch 36/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2459 - acc: 0.9124 - val_loss: 0.2039 - val_acc: 0.9459\n",
            "Epoch 37/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2473 - acc: 0.9149 - val_loss: 0.2036 - val_acc: 0.9459\n",
            "Epoch 38/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.2422 - acc: 0.9144 - val_loss: 0.2783 - val_acc: 0.8964\n",
            "Epoch 39/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.2407 - acc: 0.9179 - val_loss: 0.2055 - val_acc: 0.9505\n",
            "Epoch 40/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.2381 - acc: 0.9174 - val_loss: 0.2833 - val_acc: 0.9009\n",
            "Epoch 41/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2372 - acc: 0.9179 - val_loss: 0.2146 - val_acc: 0.9505\n",
            "Epoch 42/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.2334 - acc: 0.9199 - val_loss: 0.2466 - val_acc: 0.9279\n",
            "Epoch 43/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.2328 - acc: 0.9199 - val_loss: 0.2497 - val_acc: 0.9054\n",
            "Epoch 44/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.2296 - acc: 0.9219 - val_loss: 0.1881 - val_acc: 0.9505\n",
            "Epoch 45/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.2270 - acc: 0.9209 - val_loss: 0.1660 - val_acc: 0.9685\n",
            "Epoch 46/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.2270 - acc: 0.9194 - val_loss: 0.2253 - val_acc: 0.9279\n",
            "Epoch 47/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2261 - acc: 0.9239 - val_loss: 0.2767 - val_acc: 0.8964\n",
            "Epoch 48/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.2252 - acc: 0.9259 - val_loss: 0.2270 - val_acc: 0.9144\n",
            "Epoch 49/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.2216 - acc: 0.9254 - val_loss: 0.2467 - val_acc: 0.9009\n",
            "Epoch 50/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2195 - acc: 0.9269 - val_loss: 0.2307 - val_acc: 0.9144\n",
            "Epoch 51/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.2194 - acc: 0.9274 - val_loss: 0.2406 - val_acc: 0.9054\n",
            "Epoch 52/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.2151 - acc: 0.9264 - val_loss: 0.1968 - val_acc: 0.9550\n",
            "Epoch 53/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.2159 - acc: 0.9259 - val_loss: 0.2526 - val_acc: 0.9099\n",
            "Epoch 54/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.2124 - acc: 0.9279 - val_loss: 0.2044 - val_acc: 0.9324\n",
            "Epoch 55/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2134 - acc: 0.9269 - val_loss: 0.2258 - val_acc: 0.9234\n",
            "Epoch 56/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.2111 - acc: 0.9284 - val_loss: 0.1936 - val_acc: 0.9414\n",
            "Epoch 57/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.2102 - acc: 0.9279 - val_loss: 0.1698 - val_acc: 0.9550\n",
            "Epoch 58/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.2057 - acc: 0.9299 - val_loss: 0.2587 - val_acc: 0.9009\n",
            "Epoch 59/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.2059 - acc: 0.9309 - val_loss: 0.1972 - val_acc: 0.9324\n",
            "Epoch 60/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.2062 - acc: 0.9324 - val_loss: 0.2552 - val_acc: 0.8964\n",
            "Epoch 61/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.2061 - acc: 0.9314 - val_loss: 0.2264 - val_acc: 0.9189\n",
            "Epoch 62/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2017 - acc: 0.9359 - val_loss: 0.1642 - val_acc: 0.9550\n",
            "Epoch 63/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.2001 - acc: 0.9314 - val_loss: 0.2229 - val_acc: 0.9279\n",
            "Epoch 64/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.2018 - acc: 0.9319 - val_loss: 0.2011 - val_acc: 0.9459\n",
            "Epoch 65/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1971 - acc: 0.9369 - val_loss: 0.2054 - val_acc: 0.9279\n",
            "Epoch 66/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1963 - acc: 0.9389 - val_loss: 0.1827 - val_acc: 0.9505\n",
            "Epoch 67/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1963 - acc: 0.9349 - val_loss: 0.1618 - val_acc: 0.9685\n",
            "Epoch 68/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.1967 - acc: 0.9334 - val_loss: 0.1753 - val_acc: 0.9595\n",
            "Epoch 69/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1944 - acc: 0.9339 - val_loss: 0.1961 - val_acc: 0.9324\n",
            "Epoch 70/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1919 - acc: 0.9339 - val_loss: 0.2282 - val_acc: 0.9144\n",
            "Epoch 71/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1921 - acc: 0.9369 - val_loss: 0.1920 - val_acc: 0.9459\n",
            "Epoch 72/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1933 - acc: 0.9364 - val_loss: 0.1728 - val_acc: 0.9550\n",
            "Epoch 73/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1894 - acc: 0.9354 - val_loss: 0.2015 - val_acc: 0.9234\n",
            "Epoch 74/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1893 - acc: 0.9384 - val_loss: 0.2106 - val_acc: 0.9234\n",
            "Epoch 75/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1884 - acc: 0.9379 - val_loss: 0.1921 - val_acc: 0.9369\n",
            "Epoch 76/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1856 - acc: 0.9384 - val_loss: 0.2525 - val_acc: 0.8919\n",
            "Epoch 77/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1864 - acc: 0.9384 - val_loss: 0.1736 - val_acc: 0.9459\n",
            "Epoch 78/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1852 - acc: 0.9434 - val_loss: 0.2000 - val_acc: 0.9324\n",
            "Epoch 79/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1846 - acc: 0.9399 - val_loss: 0.1912 - val_acc: 0.9324\n",
            "Epoch 80/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1846 - acc: 0.9359 - val_loss: 0.1513 - val_acc: 0.9595\n",
            "Epoch 81/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1842 - acc: 0.9389 - val_loss: 0.1829 - val_acc: 0.9324\n",
            "Epoch 82/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.1827 - acc: 0.9404 - val_loss: 0.1804 - val_acc: 0.9369\n",
            "Epoch 83/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1809 - acc: 0.9404 - val_loss: 0.2169 - val_acc: 0.9099\n",
            "Epoch 84/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1799 - acc: 0.9374 - val_loss: 0.1996 - val_acc: 0.9369\n",
            "Epoch 85/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1792 - acc: 0.9414 - val_loss: 0.1680 - val_acc: 0.9505\n",
            "Epoch 86/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1783 - acc: 0.9404 - val_loss: 0.1556 - val_acc: 0.9505\n",
            "Epoch 87/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1783 - acc: 0.9399 - val_loss: 0.1629 - val_acc: 0.9550\n",
            "Epoch 88/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1759 - acc: 0.9409 - val_loss: 0.2658 - val_acc: 0.8739\n",
            "Epoch 89/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1760 - acc: 0.9404 - val_loss: 0.2021 - val_acc: 0.9279\n",
            "Epoch 90/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1750 - acc: 0.9439 - val_loss: 0.1917 - val_acc: 0.9324\n",
            "Epoch 91/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1744 - acc: 0.9384 - val_loss: 0.1697 - val_acc: 0.9459\n",
            "Epoch 92/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1751 - acc: 0.9414 - val_loss: 0.1855 - val_acc: 0.9369\n",
            "Epoch 93/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.1731 - acc: 0.9444 - val_loss: 0.1816 - val_acc: 0.9369\n",
            "Epoch 94/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1704 - acc: 0.9439 - val_loss: 0.2202 - val_acc: 0.9009\n",
            "Epoch 95/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1699 - acc: 0.9424 - val_loss: 0.1759 - val_acc: 0.9414\n",
            "Epoch 96/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.1679 - acc: 0.9479 - val_loss: 0.1889 - val_acc: 0.9324\n",
            "Epoch 97/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.1675 - acc: 0.9444 - val_loss: 0.1890 - val_acc: 0.9459\n",
            "Epoch 98/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1678 - acc: 0.9449 - val_loss: 0.1990 - val_acc: 0.9279\n",
            "Epoch 99/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1672 - acc: 0.9419 - val_loss: 0.1633 - val_acc: 0.9369\n",
            "Epoch 100/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1666 - acc: 0.9439 - val_loss: 0.1518 - val_acc: 0.9550\n",
            "Epoch 101/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.1645 - acc: 0.9449 - val_loss: 0.1715 - val_acc: 0.9505\n",
            "Epoch 102/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.1622 - acc: 0.9469 - val_loss: 0.2475 - val_acc: 0.9054\n",
            "Epoch 103/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1643 - acc: 0.9439 - val_loss: 0.2040 - val_acc: 0.9324\n",
            "Epoch 104/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1624 - acc: 0.9419 - val_loss: 0.1500 - val_acc: 0.9550\n",
            "Epoch 105/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1609 - acc: 0.9429 - val_loss: 0.1905 - val_acc: 0.9234\n",
            "Epoch 106/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1603 - acc: 0.9474 - val_loss: 0.1356 - val_acc: 0.9640\n",
            "Epoch 107/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1591 - acc: 0.9464 - val_loss: 0.1691 - val_acc: 0.9505\n",
            "Epoch 108/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1621 - acc: 0.9429 - val_loss: 0.1593 - val_acc: 0.9505\n",
            "Epoch 109/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1585 - acc: 0.9464 - val_loss: 0.1846 - val_acc: 0.9234\n",
            "Epoch 110/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.1605 - acc: 0.9424 - val_loss: 0.1738 - val_acc: 0.9369\n",
            "Epoch 111/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1555 - acc: 0.9419 - val_loss: 0.1482 - val_acc: 0.9505\n",
            "Epoch 112/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1553 - acc: 0.9454 - val_loss: 0.2045 - val_acc: 0.9369\n",
            "Epoch 113/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1569 - acc: 0.9449 - val_loss: 0.1636 - val_acc: 0.9414\n",
            "Epoch 114/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1538 - acc: 0.9469 - val_loss: 0.2241 - val_acc: 0.9189\n",
            "Epoch 115/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.1544 - acc: 0.9464 - val_loss: 0.1824 - val_acc: 0.9279\n",
            "Epoch 116/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.1524 - acc: 0.9459 - val_loss: 0.1569 - val_acc: 0.9505\n",
            "Epoch 117/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1544 - acc: 0.9434 - val_loss: 0.1741 - val_acc: 0.9369\n",
            "Epoch 118/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1512 - acc: 0.9464 - val_loss: 0.1500 - val_acc: 0.9459\n",
            "Epoch 119/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1508 - acc: 0.9469 - val_loss: 0.1991 - val_acc: 0.9279\n",
            "Epoch 120/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1488 - acc: 0.9525 - val_loss: 0.1647 - val_acc: 0.9414\n",
            "Epoch 121/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1496 - acc: 0.9469 - val_loss: 0.1198 - val_acc: 0.9595\n",
            "Epoch 122/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1504 - acc: 0.9469 - val_loss: 0.1709 - val_acc: 0.9414\n",
            "Epoch 123/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1500 - acc: 0.9505 - val_loss: 0.2059 - val_acc: 0.9324\n",
            "Epoch 124/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.1489 - acc: 0.9474 - val_loss: 0.1577 - val_acc: 0.9459\n",
            "Epoch 125/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1459 - acc: 0.9484 - val_loss: 0.2119 - val_acc: 0.9234\n",
            "Epoch 126/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1463 - acc: 0.9479 - val_loss: 0.1757 - val_acc: 0.9414\n",
            "Epoch 127/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1468 - acc: 0.9474 - val_loss: 0.1392 - val_acc: 0.9550\n",
            "Epoch 128/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1459 - acc: 0.9505 - val_loss: 0.2699 - val_acc: 0.8874\n",
            "Epoch 129/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1482 - acc: 0.9469 - val_loss: 0.1713 - val_acc: 0.9324\n",
            "Epoch 130/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1454 - acc: 0.9505 - val_loss: 0.1754 - val_acc: 0.9369\n",
            "Epoch 131/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1440 - acc: 0.9489 - val_loss: 0.1834 - val_acc: 0.9279\n",
            "Epoch 132/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1426 - acc: 0.9499 - val_loss: 0.1718 - val_acc: 0.9414\n",
            "Epoch 133/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1416 - acc: 0.9520 - val_loss: 0.1487 - val_acc: 0.9550\n",
            "Epoch 134/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1403 - acc: 0.9525 - val_loss: 0.1811 - val_acc: 0.9414\n",
            "Epoch 135/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1425 - acc: 0.9489 - val_loss: 0.1576 - val_acc: 0.9414\n",
            "Epoch 136/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1408 - acc: 0.9494 - val_loss: 0.1585 - val_acc: 0.9459\n",
            "Epoch 137/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1395 - acc: 0.9550 - val_loss: 0.1322 - val_acc: 0.9550\n",
            "Epoch 138/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1415 - acc: 0.9499 - val_loss: 0.1261 - val_acc: 0.9595\n",
            "Epoch 139/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1413 - acc: 0.9510 - val_loss: 0.1626 - val_acc: 0.9505\n",
            "Epoch 140/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.1400 - acc: 0.9540 - val_loss: 0.1704 - val_acc: 0.9414\n",
            "Epoch 141/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1387 - acc: 0.9535 - val_loss: 0.1697 - val_acc: 0.9414\n",
            "Epoch 142/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1392 - acc: 0.9525 - val_loss: 0.1655 - val_acc: 0.9369\n",
            "Epoch 143/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1378 - acc: 0.9525 - val_loss: 0.1791 - val_acc: 0.9279\n",
            "Epoch 144/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1371 - acc: 0.9525 - val_loss: 0.1850 - val_acc: 0.9369\n",
            "Epoch 145/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1384 - acc: 0.9545 - val_loss: 0.1569 - val_acc: 0.9505\n",
            "Epoch 146/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1332 - acc: 0.9590 - val_loss: 0.1190 - val_acc: 0.9640\n",
            "Epoch 147/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1366 - acc: 0.9550 - val_loss: 0.1286 - val_acc: 0.9595\n",
            "Epoch 148/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1370 - acc: 0.9550 - val_loss: 0.1998 - val_acc: 0.9234\n",
            "Epoch 149/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1377 - acc: 0.9545 - val_loss: 0.1579 - val_acc: 0.9505\n",
            "Epoch 150/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1340 - acc: 0.9520 - val_loss: 0.1471 - val_acc: 0.9459\n",
            "Epoch 151/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1337 - acc: 0.9550 - val_loss: 0.1535 - val_acc: 0.9459\n",
            "Epoch 152/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1360 - acc: 0.9555 - val_loss: 0.1706 - val_acc: 0.9324\n",
            "Epoch 153/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.1332 - acc: 0.9560 - val_loss: 0.2208 - val_acc: 0.9234\n",
            "Epoch 154/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1320 - acc: 0.9545 - val_loss: 0.2050 - val_acc: 0.9324\n",
            "Epoch 155/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1324 - acc: 0.9540 - val_loss: 0.1327 - val_acc: 0.9505\n",
            "Epoch 156/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1325 - acc: 0.9575 - val_loss: 0.1377 - val_acc: 0.9550\n",
            "Epoch 157/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1299 - acc: 0.9575 - val_loss: 0.1963 - val_acc: 0.9369\n",
            "Epoch 158/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1302 - acc: 0.9575 - val_loss: 0.1923 - val_acc: 0.9459\n",
            "Epoch 159/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1304 - acc: 0.9595 - val_loss: 0.1981 - val_acc: 0.9279\n",
            "Epoch 160/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1314 - acc: 0.9550 - val_loss: 0.1593 - val_acc: 0.9414\n",
            "Epoch 161/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1288 - acc: 0.9535 - val_loss: 0.1790 - val_acc: 0.9414\n",
            "Epoch 162/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1278 - acc: 0.9560 - val_loss: 0.1703 - val_acc: 0.9369\n",
            "Epoch 163/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1280 - acc: 0.9570 - val_loss: 0.1864 - val_acc: 0.9550\n",
            "Epoch 164/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1250 - acc: 0.9575 - val_loss: 0.1915 - val_acc: 0.9459\n",
            "Epoch 165/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.1263 - acc: 0.9590 - val_loss: 0.1209 - val_acc: 0.9550\n",
            "Epoch 166/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1262 - acc: 0.9590 - val_loss: 0.1629 - val_acc: 0.9505\n",
            "Epoch 167/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1255 - acc: 0.9575 - val_loss: 0.1982 - val_acc: 0.9414\n",
            "Epoch 168/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1250 - acc: 0.9570 - val_loss: 0.1648 - val_acc: 0.9505\n",
            "Epoch 169/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.1244 - acc: 0.9635 - val_loss: 0.1468 - val_acc: 0.9550\n",
            "Epoch 170/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1230 - acc: 0.9610 - val_loss: 0.1750 - val_acc: 0.9459\n",
            "Epoch 171/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1246 - acc: 0.9580 - val_loss: 0.1588 - val_acc: 0.9459\n",
            "Epoch 172/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1210 - acc: 0.9595 - val_loss: 0.1158 - val_acc: 0.9595\n",
            "Epoch 173/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1241 - acc: 0.9590 - val_loss: 0.2079 - val_acc: 0.9234\n",
            "Epoch 174/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.1238 - acc: 0.9550 - val_loss: 0.1799 - val_acc: 0.9324\n",
            "Epoch 175/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.1212 - acc: 0.9595 - val_loss: 0.1430 - val_acc: 0.9550\n",
            "Epoch 176/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1233 - acc: 0.9610 - val_loss: 0.1789 - val_acc: 0.9459\n",
            "Epoch 177/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1219 - acc: 0.9615 - val_loss: 0.1881 - val_acc: 0.9414\n",
            "Epoch 178/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1214 - acc: 0.9585 - val_loss: 0.1628 - val_acc: 0.9459\n",
            "Epoch 179/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.1211 - acc: 0.9585 - val_loss: 0.1753 - val_acc: 0.9459\n",
            "Epoch 180/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1216 - acc: 0.9620 - val_loss: 0.1579 - val_acc: 0.9459\n",
            "Epoch 181/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1181 - acc: 0.9565 - val_loss: 0.1330 - val_acc: 0.9550\n",
            "Epoch 182/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.1164 - acc: 0.9615 - val_loss: 0.1526 - val_acc: 0.9459\n",
            "Epoch 183/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1193 - acc: 0.9600 - val_loss: 0.1778 - val_acc: 0.9459\n",
            "Epoch 184/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1192 - acc: 0.9600 - val_loss: 0.1861 - val_acc: 0.9414\n",
            "Epoch 185/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1172 - acc: 0.9625 - val_loss: 0.1641 - val_acc: 0.9505\n",
            "Epoch 186/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1170 - acc: 0.9600 - val_loss: 0.1800 - val_acc: 0.9369\n",
            "Epoch 187/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1144 - acc: 0.9645 - val_loss: 0.2098 - val_acc: 0.9234\n",
            "Epoch 188/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1175 - acc: 0.9580 - val_loss: 0.1642 - val_acc: 0.9505\n",
            "Epoch 189/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1166 - acc: 0.9630 - val_loss: 0.1998 - val_acc: 0.9324\n",
            "Epoch 190/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1138 - acc: 0.9645 - val_loss: 0.1904 - val_acc: 0.9369\n",
            "Epoch 191/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1140 - acc: 0.9645 - val_loss: 0.1278 - val_acc: 0.9505\n",
            "Epoch 192/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1156 - acc: 0.9610 - val_loss: 0.1577 - val_acc: 0.9459\n",
            "Epoch 193/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.1159 - acc: 0.9625 - val_loss: 0.1798 - val_acc: 0.9369\n",
            "Epoch 194/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1146 - acc: 0.9610 - val_loss: 0.1896 - val_acc: 0.9369\n",
            "Epoch 195/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1131 - acc: 0.9625 - val_loss: 0.1603 - val_acc: 0.9459\n",
            "Epoch 196/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.1133 - acc: 0.9600 - val_loss: 0.1730 - val_acc: 0.9369\n",
            "Epoch 197/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1109 - acc: 0.9645 - val_loss: 0.1558 - val_acc: 0.9595\n",
            "Epoch 198/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1126 - acc: 0.9635 - val_loss: 0.1260 - val_acc: 0.9459\n",
            "Epoch 199/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1098 - acc: 0.9650 - val_loss: 0.1884 - val_acc: 0.9324\n",
            "Epoch 200/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1086 - acc: 0.9685 - val_loss: 0.1425 - val_acc: 0.9550\n",
            "Epoch 201/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1104 - acc: 0.9650 - val_loss: 0.2120 - val_acc: 0.9279\n",
            "Epoch 202/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1115 - acc: 0.9635 - val_loss: 0.1626 - val_acc: 0.9505\n",
            "Epoch 203/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1080 - acc: 0.9660 - val_loss: 0.1717 - val_acc: 0.9414\n",
            "Epoch 204/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1122 - acc: 0.9645 - val_loss: 0.1653 - val_acc: 0.9414\n",
            "Epoch 205/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1118 - acc: 0.9635 - val_loss: 0.1608 - val_acc: 0.9459\n",
            "Epoch 206/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.1079 - acc: 0.9640 - val_loss: 0.1572 - val_acc: 0.9414\n",
            "Epoch 207/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1088 - acc: 0.9640 - val_loss: 0.1320 - val_acc: 0.9550\n",
            "Epoch 208/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1076 - acc: 0.9675 - val_loss: 0.1633 - val_acc: 0.9459\n",
            "Epoch 209/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1106 - acc: 0.9625 - val_loss: 0.2075 - val_acc: 0.9234\n",
            "Epoch 210/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1110 - acc: 0.9610 - val_loss: 0.1715 - val_acc: 0.9369\n",
            "Epoch 211/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1068 - acc: 0.9695 - val_loss: 0.2203 - val_acc: 0.9189\n",
            "Epoch 212/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1062 - acc: 0.9655 - val_loss: 0.2119 - val_acc: 0.9279\n",
            "Epoch 213/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1103 - acc: 0.9640 - val_loss: 0.1789 - val_acc: 0.9414\n",
            "Epoch 214/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.1065 - acc: 0.9650 - val_loss: 0.1851 - val_acc: 0.9324\n",
            "Epoch 215/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1079 - acc: 0.9655 - val_loss: 0.2008 - val_acc: 0.9279\n",
            "Epoch 216/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.1047 - acc: 0.9660 - val_loss: 0.2234 - val_acc: 0.9234\n",
            "Epoch 217/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1051 - acc: 0.9660 - val_loss: 0.2154 - val_acc: 0.9189\n",
            "Epoch 218/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1059 - acc: 0.9670 - val_loss: 0.1410 - val_acc: 0.9414\n",
            "Epoch 219/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1055 - acc: 0.9660 - val_loss: 0.1443 - val_acc: 0.9505\n",
            "Epoch 220/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1062 - acc: 0.9650 - val_loss: 0.1594 - val_acc: 0.9369\n",
            "Epoch 221/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.1053 - acc: 0.9660 - val_loss: 0.1728 - val_acc: 0.9414\n",
            "Epoch 222/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1027 - acc: 0.9670 - val_loss: 0.1186 - val_acc: 0.9685\n",
            "Epoch 223/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1050 - acc: 0.9670 - val_loss: 0.1508 - val_acc: 0.9505\n",
            "Epoch 224/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1035 - acc: 0.9675 - val_loss: 0.1713 - val_acc: 0.9414\n",
            "Epoch 225/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.1038 - acc: 0.9625 - val_loss: 0.1739 - val_acc: 0.9369\n",
            "Epoch 226/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1068 - acc: 0.9635 - val_loss: 0.1819 - val_acc: 0.9369\n",
            "Epoch 227/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1036 - acc: 0.9625 - val_loss: 0.1870 - val_acc: 0.9369\n",
            "Epoch 228/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1016 - acc: 0.9670 - val_loss: 0.1400 - val_acc: 0.9595\n",
            "Epoch 229/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1054 - acc: 0.9650 - val_loss: 0.1497 - val_acc: 0.9459\n",
            "Epoch 230/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1017 - acc: 0.9675 - val_loss: 0.1760 - val_acc: 0.9414\n",
            "Epoch 231/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1030 - acc: 0.9660 - val_loss: 0.2275 - val_acc: 0.9099\n",
            "Epoch 232/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1035 - acc: 0.9650 - val_loss: 0.1958 - val_acc: 0.9324\n",
            "Epoch 233/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.1020 - acc: 0.9665 - val_loss: 0.1990 - val_acc: 0.9279\n",
            "Epoch 234/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.1024 - acc: 0.9665 - val_loss: 0.1755 - val_acc: 0.9369\n",
            "Epoch 235/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.1008 - acc: 0.9680 - val_loss: 0.1583 - val_acc: 0.9414\n",
            "Epoch 236/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0990 - acc: 0.9680 - val_loss: 0.1919 - val_acc: 0.9324\n",
            "Epoch 237/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1024 - acc: 0.9605 - val_loss: 0.1716 - val_acc: 0.9279\n",
            "Epoch 238/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.1025 - acc: 0.9660 - val_loss: 0.1406 - val_acc: 0.9595\n",
            "Epoch 239/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.1022 - acc: 0.9665 - val_loss: 0.1763 - val_acc: 0.9324\n",
            "Epoch 240/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0983 - acc: 0.9660 - val_loss: 0.2008 - val_acc: 0.9234\n",
            "Epoch 241/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1005 - acc: 0.9650 - val_loss: 0.1991 - val_acc: 0.9234\n",
            "Epoch 242/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1000 - acc: 0.9690 - val_loss: 0.1406 - val_acc: 0.9505\n",
            "Epoch 243/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0977 - acc: 0.9690 - val_loss: 0.1259 - val_acc: 0.9550\n",
            "Epoch 244/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0999 - acc: 0.9665 - val_loss: 0.1728 - val_acc: 0.9369\n",
            "Epoch 245/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0996 - acc: 0.9685 - val_loss: 0.1622 - val_acc: 0.9459\n",
            "Epoch 246/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.1006 - acc: 0.9660 - val_loss: 0.1393 - val_acc: 0.9640\n",
            "Epoch 247/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0993 - acc: 0.9675 - val_loss: 0.1815 - val_acc: 0.9369\n",
            "Epoch 248/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.1001 - acc: 0.9655 - val_loss: 0.1484 - val_acc: 0.9459\n",
            "Epoch 249/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0984 - acc: 0.9690 - val_loss: 0.1684 - val_acc: 0.9369\n",
            "Epoch 250/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0960 - acc: 0.9705 - val_loss: 0.1220 - val_acc: 0.9459\n",
            "Epoch 251/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0992 - acc: 0.9695 - val_loss: 0.1703 - val_acc: 0.9414\n",
            "Epoch 252/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0984 - acc: 0.9660 - val_loss: 0.1826 - val_acc: 0.9459\n",
            "Epoch 253/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0989 - acc: 0.9660 - val_loss: 0.2058 - val_acc: 0.9189\n",
            "Epoch 254/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0969 - acc: 0.9670 - val_loss: 0.1674 - val_acc: 0.9324\n",
            "Epoch 255/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0996 - acc: 0.9690 - val_loss: 0.1417 - val_acc: 0.9550\n",
            "Epoch 256/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0933 - acc: 0.9690 - val_loss: 0.2094 - val_acc: 0.9234\n",
            "Epoch 257/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0971 - acc: 0.9660 - val_loss: 0.2283 - val_acc: 0.9189\n",
            "Epoch 258/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0954 - acc: 0.9680 - val_loss: 0.1551 - val_acc: 0.9505\n",
            "Epoch 259/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0951 - acc: 0.9700 - val_loss: 0.1803 - val_acc: 0.9279\n",
            "Epoch 260/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.1008 - acc: 0.9670 - val_loss: 0.1796 - val_acc: 0.9459\n",
            "Epoch 261/1500\n",
            "1998/1998 [==============================] - 0s 131us/step - loss: 0.0987 - acc: 0.9630 - val_loss: 0.1610 - val_acc: 0.9505\n",
            "Epoch 262/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0953 - acc: 0.9690 - val_loss: 0.1761 - val_acc: 0.9369\n",
            "Epoch 263/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0935 - acc: 0.9695 - val_loss: 0.1635 - val_acc: 0.9414\n",
            "Epoch 264/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0937 - acc: 0.9700 - val_loss: 0.1639 - val_acc: 0.9459\n",
            "Epoch 265/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0945 - acc: 0.9680 - val_loss: 0.1643 - val_acc: 0.9369\n",
            "Epoch 266/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0936 - acc: 0.9675 - val_loss: 0.2164 - val_acc: 0.9234\n",
            "Epoch 267/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0955 - acc: 0.9665 - val_loss: 0.1428 - val_acc: 0.9459\n",
            "Epoch 268/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0947 - acc: 0.9660 - val_loss: 0.1952 - val_acc: 0.9279\n",
            "Epoch 269/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0936 - acc: 0.9685 - val_loss: 0.1435 - val_acc: 0.9459\n",
            "Epoch 270/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0921 - acc: 0.9700 - val_loss: 0.1401 - val_acc: 0.9595\n",
            "Epoch 271/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0926 - acc: 0.9705 - val_loss: 0.2188 - val_acc: 0.9279\n",
            "Epoch 272/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0970 - acc: 0.9685 - val_loss: 0.1731 - val_acc: 0.9324\n",
            "Epoch 273/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0936 - acc: 0.9670 - val_loss: 0.1711 - val_acc: 0.9324\n",
            "Epoch 274/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0923 - acc: 0.9695 - val_loss: 0.1404 - val_acc: 0.9595\n",
            "Epoch 275/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0945 - acc: 0.9705 - val_loss: 0.1941 - val_acc: 0.9279\n",
            "Epoch 276/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0900 - acc: 0.9690 - val_loss: 0.2521 - val_acc: 0.9144\n",
            "Epoch 277/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0935 - acc: 0.9685 - val_loss: 0.1961 - val_acc: 0.9369\n",
            "Epoch 278/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0941 - acc: 0.9675 - val_loss: 0.1250 - val_acc: 0.9595\n",
            "Epoch 279/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0970 - acc: 0.9665 - val_loss: 0.1302 - val_acc: 0.9640\n",
            "Epoch 280/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0919 - acc: 0.9695 - val_loss: 0.1501 - val_acc: 0.9505\n",
            "Epoch 281/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0939 - acc: 0.9690 - val_loss: 0.1610 - val_acc: 0.9369\n",
            "Epoch 282/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0911 - acc: 0.9680 - val_loss: 0.2222 - val_acc: 0.9279\n",
            "Epoch 283/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0934 - acc: 0.9695 - val_loss: 0.1738 - val_acc: 0.9279\n",
            "Epoch 284/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0886 - acc: 0.9725 - val_loss: 0.1476 - val_acc: 0.9369\n",
            "Epoch 285/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.0913 - acc: 0.9705 - val_loss: 0.1960 - val_acc: 0.9324\n",
            "Epoch 286/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0886 - acc: 0.9715 - val_loss: 0.1463 - val_acc: 0.9459\n",
            "Epoch 287/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.0898 - acc: 0.9745 - val_loss: 0.1429 - val_acc: 0.9550\n",
            "Epoch 288/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0878 - acc: 0.9740 - val_loss: 0.2394 - val_acc: 0.9234\n",
            "Epoch 289/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0993 - acc: 0.9665 - val_loss: 0.1408 - val_acc: 0.9550\n",
            "Epoch 290/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0936 - acc: 0.9705 - val_loss: 0.2169 - val_acc: 0.9279\n",
            "Epoch 291/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0890 - acc: 0.9710 - val_loss: 0.1654 - val_acc: 0.9369\n",
            "Epoch 292/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0866 - acc: 0.9720 - val_loss: 0.1266 - val_acc: 0.9505\n",
            "Epoch 293/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0868 - acc: 0.9710 - val_loss: 0.1712 - val_acc: 0.9369\n",
            "Epoch 294/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0898 - acc: 0.9695 - val_loss: 0.1647 - val_acc: 0.9369\n",
            "Epoch 295/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0925 - acc: 0.9695 - val_loss: 0.2113 - val_acc: 0.9234\n",
            "Epoch 296/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0906 - acc: 0.9685 - val_loss: 0.1902 - val_acc: 0.9369\n",
            "Epoch 297/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.0891 - acc: 0.9735 - val_loss: 0.1696 - val_acc: 0.9369\n",
            "Epoch 298/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0875 - acc: 0.9710 - val_loss: 0.2153 - val_acc: 0.9234\n",
            "Epoch 299/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.0854 - acc: 0.9730 - val_loss: 0.1796 - val_acc: 0.9369\n",
            "Epoch 300/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0912 - acc: 0.9690 - val_loss: 0.1720 - val_acc: 0.9459\n",
            "Epoch 301/1500\n",
            "1998/1998 [==============================] - 0s 134us/step - loss: 0.0878 - acc: 0.9710 - val_loss: 0.1762 - val_acc: 0.9369\n",
            "Epoch 302/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0911 - acc: 0.9690 - val_loss: 0.1538 - val_acc: 0.9505\n",
            "Epoch 303/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0880 - acc: 0.9735 - val_loss: 0.1994 - val_acc: 0.9324\n",
            "Epoch 304/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0869 - acc: 0.9710 - val_loss: 0.1530 - val_acc: 0.9459\n",
            "Epoch 305/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0838 - acc: 0.9745 - val_loss: 0.1739 - val_acc: 0.9369\n",
            "Epoch 306/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0875 - acc: 0.9705 - val_loss: 0.1604 - val_acc: 0.9459\n",
            "Epoch 307/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0866 - acc: 0.9760 - val_loss: 0.1758 - val_acc: 0.9414\n",
            "Epoch 308/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0851 - acc: 0.9735 - val_loss: 0.1501 - val_acc: 0.9459\n",
            "Epoch 309/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0889 - acc: 0.9705 - val_loss: 0.2010 - val_acc: 0.9369\n",
            "Epoch 310/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0867 - acc: 0.9730 - val_loss: 0.1695 - val_acc: 0.9369\n",
            "Epoch 311/1500\n",
            "1998/1998 [==============================] - 0s 131us/step - loss: 0.0851 - acc: 0.9735 - val_loss: 0.1940 - val_acc: 0.9279\n",
            "Epoch 312/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0903 - acc: 0.9700 - val_loss: 0.1779 - val_acc: 0.9459\n",
            "Epoch 313/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0844 - acc: 0.9750 - val_loss: 0.1810 - val_acc: 0.9459\n",
            "Epoch 314/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0873 - acc: 0.9735 - val_loss: 0.1384 - val_acc: 0.9505\n",
            "Epoch 315/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0866 - acc: 0.9745 - val_loss: 0.1460 - val_acc: 0.9505\n",
            "Epoch 316/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0892 - acc: 0.9695 - val_loss: 0.1336 - val_acc: 0.9595\n",
            "Epoch 317/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0872 - acc: 0.9705 - val_loss: 0.1600 - val_acc: 0.9505\n",
            "Epoch 318/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0858 - acc: 0.9740 - val_loss: 0.2018 - val_acc: 0.9279\n",
            "Epoch 319/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0845 - acc: 0.9730 - val_loss: 0.2086 - val_acc: 0.9324\n",
            "Epoch 320/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0877 - acc: 0.9705 - val_loss: 0.1737 - val_acc: 0.9414\n",
            "Epoch 321/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.0856 - acc: 0.9740 - val_loss: 0.2168 - val_acc: 0.9189\n",
            "Epoch 322/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0850 - acc: 0.9740 - val_loss: 0.2112 - val_acc: 0.9279\n",
            "Epoch 323/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0848 - acc: 0.9730 - val_loss: 0.1640 - val_acc: 0.9505\n",
            "Epoch 324/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0837 - acc: 0.9740 - val_loss: 0.2432 - val_acc: 0.9234\n",
            "Epoch 325/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0845 - acc: 0.9715 - val_loss: 0.2257 - val_acc: 0.9279\n",
            "Epoch 326/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0841 - acc: 0.9755 - val_loss: 0.1897 - val_acc: 0.9459\n",
            "Epoch 327/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0841 - acc: 0.9735 - val_loss: 0.1532 - val_acc: 0.9550\n",
            "Epoch 328/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0839 - acc: 0.9740 - val_loss: 0.2513 - val_acc: 0.9054\n",
            "Epoch 329/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0896 - acc: 0.9730 - val_loss: 0.1738 - val_acc: 0.9459\n",
            "Epoch 330/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0830 - acc: 0.9735 - val_loss: 0.2478 - val_acc: 0.9234\n",
            "Epoch 331/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0849 - acc: 0.9730 - val_loss: 0.2115 - val_acc: 0.9324\n",
            "Epoch 332/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0845 - acc: 0.9740 - val_loss: 0.1614 - val_acc: 0.9369\n",
            "Epoch 333/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0842 - acc: 0.9735 - val_loss: 0.1517 - val_acc: 0.9414\n",
            "Epoch 334/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0830 - acc: 0.9755 - val_loss: 0.1391 - val_acc: 0.9595\n",
            "Epoch 335/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0820 - acc: 0.9730 - val_loss: 0.1669 - val_acc: 0.9459\n",
            "Epoch 336/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0801 - acc: 0.9760 - val_loss: 0.2481 - val_acc: 0.9369\n",
            "Epoch 337/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0818 - acc: 0.9780 - val_loss: 0.1499 - val_acc: 0.9459\n",
            "Epoch 338/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0804 - acc: 0.9785 - val_loss: 0.2103 - val_acc: 0.9414\n",
            "Epoch 339/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0837 - acc: 0.9760 - val_loss: 0.1257 - val_acc: 0.9505\n",
            "Epoch 340/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0838 - acc: 0.9715 - val_loss: 0.1196 - val_acc: 0.9595\n",
            "Epoch 341/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0872 - acc: 0.9685 - val_loss: 0.1826 - val_acc: 0.9324\n",
            "Epoch 342/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0851 - acc: 0.9755 - val_loss: 0.1245 - val_acc: 0.9640\n",
            "Epoch 343/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0882 - acc: 0.9745 - val_loss: 0.1222 - val_acc: 0.9595\n",
            "Epoch 344/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0819 - acc: 0.9725 - val_loss: 0.1476 - val_acc: 0.9505\n",
            "Epoch 345/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0809 - acc: 0.9740 - val_loss: 0.1765 - val_acc: 0.9459\n",
            "Epoch 346/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0826 - acc: 0.9725 - val_loss: 0.2555 - val_acc: 0.9144\n",
            "Epoch 347/1500\n",
            "1998/1998 [==============================] - 0s 131us/step - loss: 0.0844 - acc: 0.9715 - val_loss: 0.1789 - val_acc: 0.9505\n",
            "Epoch 348/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0808 - acc: 0.9745 - val_loss: 0.1719 - val_acc: 0.9459\n",
            "Epoch 349/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0824 - acc: 0.9750 - val_loss: 0.1389 - val_acc: 0.9550\n",
            "Epoch 350/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0853 - acc: 0.9745 - val_loss: 0.1756 - val_acc: 0.9459\n",
            "Epoch 351/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0841 - acc: 0.9735 - val_loss: 0.2825 - val_acc: 0.9144\n",
            "Epoch 352/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0858 - acc: 0.9705 - val_loss: 0.1559 - val_acc: 0.9505\n",
            "Epoch 353/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0795 - acc: 0.9770 - val_loss: 0.1977 - val_acc: 0.9459\n",
            "Epoch 354/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0791 - acc: 0.9765 - val_loss: 0.1527 - val_acc: 0.9550\n",
            "Epoch 355/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0816 - acc: 0.9740 - val_loss: 0.1911 - val_acc: 0.9279\n",
            "Epoch 356/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0806 - acc: 0.9745 - val_loss: 0.1662 - val_acc: 0.9505\n",
            "Epoch 357/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0853 - acc: 0.9760 - val_loss: 0.1968 - val_acc: 0.9459\n",
            "Epoch 358/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0832 - acc: 0.9730 - val_loss: 0.2161 - val_acc: 0.9414\n",
            "Epoch 359/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0820 - acc: 0.9745 - val_loss: 0.2099 - val_acc: 0.9324\n",
            "Epoch 360/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0787 - acc: 0.9760 - val_loss: 0.1923 - val_acc: 0.9414\n",
            "Epoch 361/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0780 - acc: 0.9775 - val_loss: 0.1611 - val_acc: 0.9505\n",
            "Epoch 362/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0775 - acc: 0.9810 - val_loss: 0.1297 - val_acc: 0.9550\n",
            "Epoch 363/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0796 - acc: 0.9740 - val_loss: 0.1886 - val_acc: 0.9414\n",
            "Epoch 364/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0815 - acc: 0.9750 - val_loss: 0.2087 - val_acc: 0.9369\n",
            "Epoch 365/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0806 - acc: 0.9755 - val_loss: 0.2658 - val_acc: 0.9234\n",
            "Epoch 366/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0907 - acc: 0.9685 - val_loss: 0.2165 - val_acc: 0.9324\n",
            "Epoch 367/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0818 - acc: 0.9750 - val_loss: 0.2379 - val_acc: 0.9099\n",
            "Epoch 368/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0804 - acc: 0.9775 - val_loss: 0.1611 - val_acc: 0.9505\n",
            "Epoch 369/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0772 - acc: 0.9795 - val_loss: 0.1703 - val_acc: 0.9459\n",
            "Epoch 370/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0814 - acc: 0.9745 - val_loss: 0.1702 - val_acc: 0.9505\n",
            "Epoch 371/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0796 - acc: 0.9760 - val_loss: 0.1767 - val_acc: 0.9459\n",
            "Epoch 372/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0771 - acc: 0.9775 - val_loss: 0.1782 - val_acc: 0.9550\n",
            "Epoch 373/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0768 - acc: 0.9755 - val_loss: 0.1811 - val_acc: 0.9414\n",
            "Epoch 374/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0783 - acc: 0.9755 - val_loss: 0.2096 - val_acc: 0.9414\n",
            "Epoch 375/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0803 - acc: 0.9730 - val_loss: 0.2047 - val_acc: 0.9414\n",
            "Epoch 376/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0843 - acc: 0.9735 - val_loss: 0.1635 - val_acc: 0.9459\n",
            "Epoch 377/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0783 - acc: 0.9775 - val_loss: 0.1996 - val_acc: 0.9414\n",
            "Epoch 378/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0790 - acc: 0.9755 - val_loss: 0.2141 - val_acc: 0.9459\n",
            "Epoch 379/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0792 - acc: 0.9770 - val_loss: 0.2226 - val_acc: 0.9459\n",
            "Epoch 380/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0776 - acc: 0.9790 - val_loss: 0.2149 - val_acc: 0.9505\n",
            "Epoch 381/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0774 - acc: 0.9760 - val_loss: 0.1925 - val_acc: 0.9459\n",
            "Epoch 382/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0759 - acc: 0.9770 - val_loss: 0.2210 - val_acc: 0.9414\n",
            "Epoch 383/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0802 - acc: 0.9770 - val_loss: 0.1552 - val_acc: 0.9505\n",
            "Epoch 384/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0807 - acc: 0.9740 - val_loss: 0.1884 - val_acc: 0.9414\n",
            "Epoch 385/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0790 - acc: 0.9775 - val_loss: 0.1871 - val_acc: 0.9414\n",
            "Epoch 386/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0770 - acc: 0.9780 - val_loss: 0.2311 - val_acc: 0.9279\n",
            "Epoch 387/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0783 - acc: 0.9745 - val_loss: 0.1955 - val_acc: 0.9459\n",
            "Epoch 388/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0771 - acc: 0.9740 - val_loss: 0.2184 - val_acc: 0.9324\n",
            "Epoch 389/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0774 - acc: 0.9770 - val_loss: 0.2318 - val_acc: 0.9414\n",
            "Epoch 390/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0766 - acc: 0.9780 - val_loss: 0.2240 - val_acc: 0.9414\n",
            "Epoch 391/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0760 - acc: 0.9750 - val_loss: 0.1653 - val_acc: 0.9505\n",
            "Epoch 392/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0838 - acc: 0.9755 - val_loss: 0.2188 - val_acc: 0.9459\n",
            "Epoch 393/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0755 - acc: 0.9790 - val_loss: 0.1635 - val_acc: 0.9550\n",
            "Epoch 394/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0773 - acc: 0.9780 - val_loss: 0.2076 - val_acc: 0.9414\n",
            "Epoch 395/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0830 - acc: 0.9725 - val_loss: 0.2186 - val_acc: 0.9234\n",
            "Epoch 396/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0779 - acc: 0.9780 - val_loss: 0.2260 - val_acc: 0.9369\n",
            "Epoch 397/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0742 - acc: 0.9790 - val_loss: 0.2028 - val_acc: 0.9459\n",
            "Epoch 398/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0770 - acc: 0.9750 - val_loss: 0.1751 - val_acc: 0.9459\n",
            "Epoch 399/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0763 - acc: 0.9810 - val_loss: 0.2083 - val_acc: 0.9414\n",
            "Epoch 400/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0823 - acc: 0.9740 - val_loss: 0.1731 - val_acc: 0.9505\n",
            "Epoch 401/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0767 - acc: 0.9780 - val_loss: 0.2189 - val_acc: 0.9414\n",
            "Epoch 402/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0738 - acc: 0.9805 - val_loss: 0.2051 - val_acc: 0.9369\n",
            "Epoch 403/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0767 - acc: 0.9765 - val_loss: 0.2019 - val_acc: 0.9459\n",
            "Epoch 404/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0731 - acc: 0.9795 - val_loss: 0.1786 - val_acc: 0.9459\n",
            "Epoch 405/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0753 - acc: 0.9790 - val_loss: 0.1808 - val_acc: 0.9505\n",
            "Epoch 406/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0784 - acc: 0.9775 - val_loss: 0.2164 - val_acc: 0.9414\n",
            "Epoch 407/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0740 - acc: 0.9785 - val_loss: 0.1710 - val_acc: 0.9505\n",
            "Epoch 408/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0793 - acc: 0.9755 - val_loss: 0.2024 - val_acc: 0.9595\n",
            "Epoch 409/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0786 - acc: 0.9745 - val_loss: 0.1681 - val_acc: 0.9550\n",
            "Epoch 410/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0807 - acc: 0.9725 - val_loss: 0.1779 - val_acc: 0.9459\n",
            "Epoch 411/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0771 - acc: 0.9755 - val_loss: 0.1920 - val_acc: 0.9369\n",
            "Epoch 412/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0746 - acc: 0.9775 - val_loss: 0.2021 - val_acc: 0.9505\n",
            "Epoch 413/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0731 - acc: 0.9765 - val_loss: 0.2332 - val_acc: 0.9369\n",
            "Epoch 414/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0724 - acc: 0.9780 - val_loss: 0.2161 - val_acc: 0.9324\n",
            "Epoch 415/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0719 - acc: 0.9765 - val_loss: 0.2315 - val_acc: 0.9324\n",
            "Epoch 416/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0743 - acc: 0.9820 - val_loss: 0.1718 - val_acc: 0.9459\n",
            "Epoch 417/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0745 - acc: 0.9755 - val_loss: 0.2077 - val_acc: 0.9459\n",
            "Epoch 418/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0743 - acc: 0.9795 - val_loss: 0.1883 - val_acc: 0.9414\n",
            "Epoch 419/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0731 - acc: 0.9800 - val_loss: 0.2049 - val_acc: 0.9369\n",
            "Epoch 420/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0725 - acc: 0.9795 - val_loss: 0.2243 - val_acc: 0.9324\n",
            "Epoch 421/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0747 - acc: 0.9775 - val_loss: 0.2533 - val_acc: 0.9369\n",
            "Epoch 422/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0749 - acc: 0.9785 - val_loss: 0.1837 - val_acc: 0.9414\n",
            "Epoch 423/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0755 - acc: 0.9765 - val_loss: 0.1964 - val_acc: 0.9369\n",
            "Epoch 424/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0701 - acc: 0.9820 - val_loss: 0.1826 - val_acc: 0.9459\n",
            "Epoch 425/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0738 - acc: 0.9790 - val_loss: 0.2248 - val_acc: 0.9414\n",
            "Epoch 426/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0706 - acc: 0.9805 - val_loss: 0.1743 - val_acc: 0.9414\n",
            "Epoch 427/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0724 - acc: 0.9785 - val_loss: 0.1823 - val_acc: 0.9459\n",
            "Epoch 428/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0719 - acc: 0.9785 - val_loss: 0.2230 - val_acc: 0.9369\n",
            "Epoch 429/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0693 - acc: 0.9820 - val_loss: 0.1833 - val_acc: 0.9505\n",
            "Epoch 430/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0798 - acc: 0.9780 - val_loss: 0.3098 - val_acc: 0.9234\n",
            "Epoch 431/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0764 - acc: 0.9775 - val_loss: 0.2414 - val_acc: 0.9369\n",
            "Epoch 432/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0709 - acc: 0.9810 - val_loss: 0.2302 - val_acc: 0.9414\n",
            "Epoch 433/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0684 - acc: 0.9815 - val_loss: 0.2079 - val_acc: 0.9459\n",
            "Epoch 434/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0717 - acc: 0.9775 - val_loss: 0.2353 - val_acc: 0.9369\n",
            "Epoch 435/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0718 - acc: 0.9800 - val_loss: 0.2321 - val_acc: 0.9369\n",
            "Epoch 436/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0714 - acc: 0.9795 - val_loss: 0.1800 - val_acc: 0.9505\n",
            "Epoch 437/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0707 - acc: 0.9780 - val_loss: 0.1886 - val_acc: 0.9459\n",
            "Epoch 438/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0696 - acc: 0.9805 - val_loss: 0.2481 - val_acc: 0.9324\n",
            "Epoch 439/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0680 - acc: 0.9815 - val_loss: 0.1976 - val_acc: 0.9505\n",
            "Epoch 440/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0723 - acc: 0.9800 - val_loss: 0.2400 - val_acc: 0.9324\n",
            "Epoch 441/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0721 - acc: 0.9815 - val_loss: 0.2246 - val_acc: 0.9414\n",
            "Epoch 442/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0722 - acc: 0.9785 - val_loss: 0.1954 - val_acc: 0.9414\n",
            "Epoch 443/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0688 - acc: 0.9785 - val_loss: 0.2637 - val_acc: 0.9324\n",
            "Epoch 444/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0680 - acc: 0.9825 - val_loss: 0.1796 - val_acc: 0.9459\n",
            "Epoch 445/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0689 - acc: 0.9820 - val_loss: 0.2027 - val_acc: 0.9459\n",
            "Epoch 446/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0734 - acc: 0.9755 - val_loss: 0.2463 - val_acc: 0.9324\n",
            "Epoch 447/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0702 - acc: 0.9835 - val_loss: 0.2384 - val_acc: 0.9369\n",
            "Epoch 448/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0666 - acc: 0.9830 - val_loss: 0.2630 - val_acc: 0.9324\n",
            "Epoch 449/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0665 - acc: 0.9835 - val_loss: 0.1843 - val_acc: 0.9595\n",
            "Epoch 450/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0689 - acc: 0.9815 - val_loss: 0.2342 - val_acc: 0.9414\n",
            "Epoch 451/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0740 - acc: 0.9800 - val_loss: 0.2287 - val_acc: 0.9414\n",
            "Epoch 452/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0688 - acc: 0.9825 - val_loss: 0.2082 - val_acc: 0.9414\n",
            "Epoch 453/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0677 - acc: 0.9805 - val_loss: 0.2417 - val_acc: 0.9324\n",
            "Epoch 454/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0686 - acc: 0.9800 - val_loss: 0.2000 - val_acc: 0.9414\n",
            "Epoch 455/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0689 - acc: 0.9825 - val_loss: 0.2788 - val_acc: 0.9234\n",
            "Epoch 456/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0678 - acc: 0.9805 - val_loss: 0.2138 - val_acc: 0.9505\n",
            "Epoch 457/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0729 - acc: 0.9775 - val_loss: 0.2092 - val_acc: 0.9459\n",
            "Epoch 458/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0680 - acc: 0.9810 - val_loss: 0.1917 - val_acc: 0.9414\n",
            "Epoch 459/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0760 - acc: 0.9775 - val_loss: 0.2296 - val_acc: 0.9414\n",
            "Epoch 460/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0712 - acc: 0.9790 - val_loss: 0.2187 - val_acc: 0.9459\n",
            "Epoch 461/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0676 - acc: 0.9810 - val_loss: 0.2286 - val_acc: 0.9459\n",
            "Epoch 462/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0697 - acc: 0.9805 - val_loss: 0.1677 - val_acc: 0.9505\n",
            "Epoch 463/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0663 - acc: 0.9840 - val_loss: 0.2248 - val_acc: 0.9414\n",
            "Epoch 464/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0645 - acc: 0.9835 - val_loss: 0.2137 - val_acc: 0.9414\n",
            "Epoch 465/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0654 - acc: 0.9815 - val_loss: 0.1583 - val_acc: 0.9550\n",
            "Epoch 466/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0666 - acc: 0.9805 - val_loss: 0.2844 - val_acc: 0.9234\n",
            "Epoch 467/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0647 - acc: 0.9830 - val_loss: 0.2276 - val_acc: 0.9459\n",
            "Epoch 468/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0649 - acc: 0.9815 - val_loss: 0.2402 - val_acc: 0.9324\n",
            "Epoch 469/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0632 - acc: 0.9840 - val_loss: 0.2262 - val_acc: 0.9459\n",
            "Epoch 470/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0658 - acc: 0.9820 - val_loss: 0.2607 - val_acc: 0.9234\n",
            "Epoch 471/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0677 - acc: 0.9815 - val_loss: 0.2448 - val_acc: 0.9459\n",
            "Epoch 472/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0699 - acc: 0.9815 - val_loss: 0.2658 - val_acc: 0.9369\n",
            "Epoch 473/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0653 - acc: 0.9820 - val_loss: 0.2157 - val_acc: 0.9459\n",
            "Epoch 474/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0660 - acc: 0.9815 - val_loss: 0.2201 - val_acc: 0.9459\n",
            "Epoch 475/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0645 - acc: 0.9830 - val_loss: 0.1692 - val_acc: 0.9595\n",
            "Epoch 476/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0700 - acc: 0.9780 - val_loss: 0.2601 - val_acc: 0.9369\n",
            "Epoch 477/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0661 - acc: 0.9820 - val_loss: 0.2493 - val_acc: 0.9369\n",
            "Epoch 478/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0658 - acc: 0.9825 - val_loss: 0.2731 - val_acc: 0.9234\n",
            "Epoch 479/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0649 - acc: 0.9835 - val_loss: 0.2677 - val_acc: 0.9414\n",
            "Epoch 480/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0693 - acc: 0.9775 - val_loss: 0.2567 - val_acc: 0.9279\n",
            "Epoch 481/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0653 - acc: 0.9820 - val_loss: 0.1990 - val_acc: 0.9505\n",
            "Epoch 482/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0646 - acc: 0.9820 - val_loss: 0.2397 - val_acc: 0.9459\n",
            "Epoch 483/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0641 - acc: 0.9800 - val_loss: 0.2398 - val_acc: 0.9369\n",
            "Epoch 484/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0645 - acc: 0.9795 - val_loss: 0.2746 - val_acc: 0.9324\n",
            "Epoch 485/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0651 - acc: 0.9805 - val_loss: 0.2251 - val_acc: 0.9414\n",
            "Epoch 486/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0633 - acc: 0.9830 - val_loss: 0.2408 - val_acc: 0.9414\n",
            "Epoch 487/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0636 - acc: 0.9840 - val_loss: 0.2322 - val_acc: 0.9459\n",
            "Epoch 488/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0637 - acc: 0.9820 - val_loss: 0.2775 - val_acc: 0.9324\n",
            "Epoch 489/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0663 - acc: 0.9810 - val_loss: 0.2144 - val_acc: 0.9505\n",
            "Epoch 490/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0619 - acc: 0.9830 - val_loss: 0.2094 - val_acc: 0.9505\n",
            "Epoch 491/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0610 - acc: 0.9845 - val_loss: 0.2434 - val_acc: 0.9414\n",
            "Epoch 492/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0641 - acc: 0.9820 - val_loss: 0.2839 - val_acc: 0.9324\n",
            "Epoch 493/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0648 - acc: 0.9850 - val_loss: 0.2577 - val_acc: 0.9369\n",
            "Epoch 494/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0691 - acc: 0.9790 - val_loss: 0.2302 - val_acc: 0.9550\n",
            "Epoch 495/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0653 - acc: 0.9795 - val_loss: 0.2687 - val_acc: 0.9414\n",
            "Epoch 496/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0648 - acc: 0.9825 - val_loss: 0.1987 - val_acc: 0.9459\n",
            "Epoch 497/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0638 - acc: 0.9840 - val_loss: 0.2783 - val_acc: 0.9234\n",
            "Epoch 498/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0641 - acc: 0.9815 - val_loss: 0.2668 - val_acc: 0.9414\n",
            "Epoch 499/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0632 - acc: 0.9825 - val_loss: 0.2533 - val_acc: 0.9505\n",
            "Epoch 500/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0608 - acc: 0.9840 - val_loss: 0.2741 - val_acc: 0.9369\n",
            "Epoch 501/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0619 - acc: 0.9840 - val_loss: 0.2175 - val_acc: 0.9459\n",
            "Epoch 502/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0644 - acc: 0.9820 - val_loss: 0.2156 - val_acc: 0.9505\n",
            "Epoch 503/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0659 - acc: 0.9800 - val_loss: 0.2449 - val_acc: 0.9369\n",
            "Epoch 504/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0611 - acc: 0.9820 - val_loss: 0.2686 - val_acc: 0.9369\n",
            "Epoch 505/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0598 - acc: 0.9840 - val_loss: 0.2459 - val_acc: 0.9505\n",
            "Epoch 506/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0612 - acc: 0.9830 - val_loss: 0.2174 - val_acc: 0.9505\n",
            "Epoch 507/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0632 - acc: 0.9825 - val_loss: 0.2344 - val_acc: 0.9369\n",
            "Epoch 508/1500\n",
            "1998/1998 [==============================] - 0s 131us/step - loss: 0.0606 - acc: 0.9845 - val_loss: 0.1996 - val_acc: 0.9505\n",
            "Epoch 509/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0616 - acc: 0.9815 - val_loss: 0.2294 - val_acc: 0.9414\n",
            "Epoch 510/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0714 - acc: 0.9770 - val_loss: 0.1959 - val_acc: 0.9459\n",
            "Epoch 511/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0652 - acc: 0.9825 - val_loss: 0.1984 - val_acc: 0.9550\n",
            "Epoch 512/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0600 - acc: 0.9850 - val_loss: 0.2490 - val_acc: 0.9414\n",
            "Epoch 513/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0582 - acc: 0.9850 - val_loss: 0.2707 - val_acc: 0.9414\n",
            "Epoch 514/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0621 - acc: 0.9810 - val_loss: 0.2240 - val_acc: 0.9459\n",
            "Epoch 515/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0600 - acc: 0.9840 - val_loss: 0.2272 - val_acc: 0.9505\n",
            "Epoch 516/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0588 - acc: 0.9825 - val_loss: 0.2776 - val_acc: 0.9369\n",
            "Epoch 517/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0585 - acc: 0.9830 - val_loss: 0.2523 - val_acc: 0.9414\n",
            "Epoch 518/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0594 - acc: 0.9825 - val_loss: 0.2778 - val_acc: 0.9369\n",
            "Epoch 519/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0630 - acc: 0.9825 - val_loss: 0.2223 - val_acc: 0.9414\n",
            "Epoch 520/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0630 - acc: 0.9845 - val_loss: 0.2567 - val_acc: 0.9414\n",
            "Epoch 521/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0637 - acc: 0.9800 - val_loss: 0.2700 - val_acc: 0.9414\n",
            "Epoch 522/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0605 - acc: 0.9820 - val_loss: 0.2388 - val_acc: 0.9459\n",
            "Epoch 523/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0602 - acc: 0.9820 - val_loss: 0.2482 - val_acc: 0.9414\n",
            "Epoch 524/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0622 - acc: 0.9815 - val_loss: 0.2304 - val_acc: 0.9414\n",
            "Epoch 525/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0610 - acc: 0.9840 - val_loss: 0.2424 - val_acc: 0.9414\n",
            "Epoch 526/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0596 - acc: 0.9830 - val_loss: 0.2860 - val_acc: 0.9369\n",
            "Epoch 527/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0599 - acc: 0.9815 - val_loss: 0.2544 - val_acc: 0.9505\n",
            "Epoch 528/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0658 - acc: 0.9795 - val_loss: 0.2537 - val_acc: 0.9459\n",
            "Epoch 529/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0584 - acc: 0.9830 - val_loss: 0.2407 - val_acc: 0.9459\n",
            "Epoch 530/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0588 - acc: 0.9840 - val_loss: 0.2828 - val_acc: 0.9414\n",
            "Epoch 531/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0586 - acc: 0.9820 - val_loss: 0.2536 - val_acc: 0.9414\n",
            "Epoch 532/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0633 - acc: 0.9830 - val_loss: 0.2654 - val_acc: 0.9369\n",
            "Epoch 533/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0622 - acc: 0.9795 - val_loss: 0.2339 - val_acc: 0.9459\n",
            "Epoch 534/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0589 - acc: 0.9820 - val_loss: 0.2694 - val_acc: 0.9459\n",
            "Epoch 535/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0556 - acc: 0.9865 - val_loss: 0.2522 - val_acc: 0.9459\n",
            "Epoch 536/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0557 - acc: 0.9860 - val_loss: 0.2475 - val_acc: 0.9369\n",
            "Epoch 537/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0592 - acc: 0.9820 - val_loss: 0.2827 - val_acc: 0.9369\n",
            "Epoch 538/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0640 - acc: 0.9795 - val_loss: 0.2871 - val_acc: 0.9414\n",
            "Epoch 539/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0586 - acc: 0.9830 - val_loss: 0.2870 - val_acc: 0.9324\n",
            "Epoch 540/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0593 - acc: 0.9800 - val_loss: 0.2490 - val_acc: 0.9414\n",
            "Epoch 541/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0632 - acc: 0.9810 - val_loss: 0.2557 - val_acc: 0.9505\n",
            "Epoch 542/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0572 - acc: 0.9840 - val_loss: 0.2796 - val_acc: 0.9369\n",
            "Epoch 543/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0608 - acc: 0.9825 - val_loss: 0.2870 - val_acc: 0.9324\n",
            "Epoch 544/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0608 - acc: 0.9815 - val_loss: 0.2682 - val_acc: 0.9414\n",
            "Epoch 545/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0572 - acc: 0.9850 - val_loss: 0.2559 - val_acc: 0.9414\n",
            "Epoch 546/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0555 - acc: 0.9850 - val_loss: 0.3088 - val_acc: 0.9324\n",
            "Epoch 547/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0578 - acc: 0.9830 - val_loss: 0.2731 - val_acc: 0.9459\n",
            "Epoch 548/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0593 - acc: 0.9830 - val_loss: 0.2596 - val_acc: 0.9459\n",
            "Epoch 549/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.0626 - acc: 0.9790 - val_loss: 0.2354 - val_acc: 0.9459\n",
            "Epoch 550/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0587 - acc: 0.9825 - val_loss: 0.2729 - val_acc: 0.9414\n",
            "Epoch 551/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0562 - acc: 0.9845 - val_loss: 0.3008 - val_acc: 0.9369\n",
            "Epoch 552/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0576 - acc: 0.9850 - val_loss: 0.3006 - val_acc: 0.9414\n",
            "Epoch 553/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0550 - acc: 0.9865 - val_loss: 0.3033 - val_acc: 0.9369\n",
            "Epoch 554/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0578 - acc: 0.9830 - val_loss: 0.2593 - val_acc: 0.9459\n",
            "Epoch 555/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0570 - acc: 0.9825 - val_loss: 0.2976 - val_acc: 0.9369\n",
            "Epoch 556/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0572 - acc: 0.9830 - val_loss: 0.3124 - val_acc: 0.9369\n",
            "Epoch 557/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0547 - acc: 0.9855 - val_loss: 0.2523 - val_acc: 0.9459\n",
            "Epoch 558/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0563 - acc: 0.9835 - val_loss: 0.2531 - val_acc: 0.9459\n",
            "Epoch 559/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0668 - acc: 0.9805 - val_loss: 0.2853 - val_acc: 0.9414\n",
            "Epoch 560/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0617 - acc: 0.9820 - val_loss: 0.2312 - val_acc: 0.9505\n",
            "Epoch 561/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0580 - acc: 0.9815 - val_loss: 0.2675 - val_acc: 0.9369\n",
            "Epoch 562/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0541 - acc: 0.9870 - val_loss: 0.2592 - val_acc: 0.9459\n",
            "Epoch 563/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0560 - acc: 0.9850 - val_loss: 0.2294 - val_acc: 0.9459\n",
            "Epoch 564/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0568 - acc: 0.9830 - val_loss: 0.2597 - val_acc: 0.9414\n",
            "Epoch 565/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0533 - acc: 0.9855 - val_loss: 0.2581 - val_acc: 0.9459\n",
            "Epoch 566/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0530 - acc: 0.9865 - val_loss: 0.2328 - val_acc: 0.9459\n",
            "Epoch 567/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0595 - acc: 0.9815 - val_loss: 0.2204 - val_acc: 0.9550\n",
            "Epoch 568/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0647 - acc: 0.9805 - val_loss: 0.2754 - val_acc: 0.9369\n",
            "Epoch 569/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0687 - acc: 0.9795 - val_loss: 0.2632 - val_acc: 0.9550\n",
            "Epoch 570/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0633 - acc: 0.9780 - val_loss: 0.2446 - val_acc: 0.9505\n",
            "Epoch 571/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0535 - acc: 0.9855 - val_loss: 0.2673 - val_acc: 0.9414\n",
            "Epoch 572/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0520 - acc: 0.9865 - val_loss: 0.2508 - val_acc: 0.9505\n",
            "Epoch 573/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0528 - acc: 0.9850 - val_loss: 0.2669 - val_acc: 0.9414\n",
            "Epoch 574/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0543 - acc: 0.9835 - val_loss: 0.3412 - val_acc: 0.9279\n",
            "Epoch 575/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0527 - acc: 0.9850 - val_loss: 0.2241 - val_acc: 0.9550\n",
            "Epoch 576/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0527 - acc: 0.9850 - val_loss: 0.3239 - val_acc: 0.9099\n",
            "Epoch 577/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0531 - acc: 0.9855 - val_loss: 0.2671 - val_acc: 0.9414\n",
            "Epoch 578/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0527 - acc: 0.9845 - val_loss: 0.3119 - val_acc: 0.9414\n",
            "Epoch 579/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0539 - acc: 0.9850 - val_loss: 0.2648 - val_acc: 0.9459\n",
            "Epoch 580/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0671 - acc: 0.9835 - val_loss: 0.3490 - val_acc: 0.9324\n",
            "Epoch 581/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0677 - acc: 0.9795 - val_loss: 0.2761 - val_acc: 0.9369\n",
            "Epoch 582/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0571 - acc: 0.9830 - val_loss: 0.3096 - val_acc: 0.9324\n",
            "Epoch 583/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0533 - acc: 0.9850 - val_loss: 0.3024 - val_acc: 0.9324\n",
            "Epoch 584/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0544 - acc: 0.9850 - val_loss: 0.3179 - val_acc: 0.9279\n",
            "Epoch 585/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0552 - acc: 0.9820 - val_loss: 0.2917 - val_acc: 0.9414\n",
            "Epoch 586/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0602 - acc: 0.9810 - val_loss: 0.2809 - val_acc: 0.9459\n",
            "Epoch 587/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0602 - acc: 0.9805 - val_loss: 0.3865 - val_acc: 0.9189\n",
            "Epoch 588/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0584 - acc: 0.9835 - val_loss: 0.3122 - val_acc: 0.9414\n",
            "Epoch 589/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0571 - acc: 0.9850 - val_loss: 0.2577 - val_acc: 0.9459\n",
            "Epoch 590/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0538 - acc: 0.9840 - val_loss: 0.3036 - val_acc: 0.9459\n",
            "Epoch 591/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0538 - acc: 0.9830 - val_loss: 0.3244 - val_acc: 0.9369\n",
            "Epoch 592/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0531 - acc: 0.9850 - val_loss: 0.2847 - val_acc: 0.9414\n",
            "Epoch 593/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0540 - acc: 0.9840 - val_loss: 0.3180 - val_acc: 0.9414\n",
            "Epoch 594/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0568 - acc: 0.9815 - val_loss: 0.3798 - val_acc: 0.9279\n",
            "Epoch 595/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0604 - acc: 0.9795 - val_loss: 0.2413 - val_acc: 0.9505\n",
            "Epoch 596/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0544 - acc: 0.9860 - val_loss: 0.3269 - val_acc: 0.9369\n",
            "Epoch 597/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0568 - acc: 0.9840 - val_loss: 0.3456 - val_acc: 0.9324\n",
            "Epoch 598/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0512 - acc: 0.9845 - val_loss: 0.2790 - val_acc: 0.9369\n",
            "Epoch 599/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0509 - acc: 0.9870 - val_loss: 0.2740 - val_acc: 0.9414\n",
            "Epoch 600/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0498 - acc: 0.9860 - val_loss: 0.2719 - val_acc: 0.9414\n",
            "Epoch 601/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0510 - acc: 0.9855 - val_loss: 0.2786 - val_acc: 0.9414\n",
            "Epoch 602/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0678 - acc: 0.9795 - val_loss: 0.3579 - val_acc: 0.9189\n",
            "Epoch 603/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0609 - acc: 0.9775 - val_loss: 0.3221 - val_acc: 0.9459\n",
            "Epoch 604/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0567 - acc: 0.9825 - val_loss: 0.2803 - val_acc: 0.9505\n",
            "Epoch 605/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0550 - acc: 0.9850 - val_loss: 0.3319 - val_acc: 0.9369\n",
            "Epoch 606/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0513 - acc: 0.9860 - val_loss: 0.3141 - val_acc: 0.9414\n",
            "Epoch 607/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0525 - acc: 0.9860 - val_loss: 0.3129 - val_acc: 0.9414\n",
            "Epoch 608/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0514 - acc: 0.9860 - val_loss: 0.2713 - val_acc: 0.9505\n",
            "Epoch 609/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0504 - acc: 0.9880 - val_loss: 0.2849 - val_acc: 0.9505\n",
            "Epoch 610/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0503 - acc: 0.9865 - val_loss: 0.3100 - val_acc: 0.9414\n",
            "Epoch 611/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0547 - acc: 0.9850 - val_loss: 0.2865 - val_acc: 0.9459\n",
            "Epoch 612/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0599 - acc: 0.9805 - val_loss: 0.3520 - val_acc: 0.9234\n",
            "Epoch 613/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0558 - acc: 0.9845 - val_loss: 0.3167 - val_acc: 0.9324\n",
            "Epoch 614/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0524 - acc: 0.9855 - val_loss: 0.3549 - val_acc: 0.9324\n",
            "Epoch 615/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0509 - acc: 0.9865 - val_loss: 0.3419 - val_acc: 0.9369\n",
            "Epoch 616/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0501 - acc: 0.9875 - val_loss: 0.3381 - val_acc: 0.9369\n",
            "Epoch 617/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0568 - acc: 0.9850 - val_loss: 0.3641 - val_acc: 0.9324\n",
            "Epoch 618/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0608 - acc: 0.9805 - val_loss: 0.3345 - val_acc: 0.9369\n",
            "Epoch 619/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0572 - acc: 0.9815 - val_loss: 0.2551 - val_acc: 0.9459\n",
            "Epoch 620/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0560 - acc: 0.9840 - val_loss: 0.3079 - val_acc: 0.9414\n",
            "Epoch 621/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0505 - acc: 0.9865 - val_loss: 0.3339 - val_acc: 0.9324\n",
            "Epoch 622/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0496 - acc: 0.9850 - val_loss: 0.2859 - val_acc: 0.9414\n",
            "Epoch 623/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0489 - acc: 0.9870 - val_loss: 0.3152 - val_acc: 0.9414\n",
            "Epoch 624/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0497 - acc: 0.9870 - val_loss: 0.3393 - val_acc: 0.9414\n",
            "Epoch 625/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0494 - acc: 0.9855 - val_loss: 0.3379 - val_acc: 0.9324\n",
            "Epoch 626/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0574 - acc: 0.9815 - val_loss: 0.2591 - val_acc: 0.9550\n",
            "Epoch 627/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0544 - acc: 0.9840 - val_loss: 0.3535 - val_acc: 0.9324\n",
            "Epoch 628/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0527 - acc: 0.9850 - val_loss: 0.3328 - val_acc: 0.9369\n",
            "Epoch 629/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0522 - acc: 0.9855 - val_loss: 0.3286 - val_acc: 0.9459\n",
            "Epoch 630/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0496 - acc: 0.9835 - val_loss: 0.3635 - val_acc: 0.9279\n",
            "Epoch 631/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0535 - acc: 0.9850 - val_loss: 0.2664 - val_acc: 0.9459\n",
            "Epoch 632/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0527 - acc: 0.9855 - val_loss: 0.3058 - val_acc: 0.9279\n",
            "Epoch 633/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0509 - acc: 0.9830 - val_loss: 0.3435 - val_acc: 0.9144\n",
            "Epoch 634/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0539 - acc: 0.9835 - val_loss: 0.2939 - val_acc: 0.9369\n",
            "Epoch 635/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0706 - acc: 0.9785 - val_loss: 0.3109 - val_acc: 0.9279\n",
            "Epoch 636/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0598 - acc: 0.9830 - val_loss: 0.3062 - val_acc: 0.9324\n",
            "Epoch 637/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0537 - acc: 0.9840 - val_loss: 0.3552 - val_acc: 0.9369\n",
            "Epoch 638/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0484 - acc: 0.9865 - val_loss: 0.3233 - val_acc: 0.9369\n",
            "Epoch 639/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0483 - acc: 0.9870 - val_loss: 0.3397 - val_acc: 0.9369\n",
            "Epoch 640/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0508 - acc: 0.9860 - val_loss: 0.3768 - val_acc: 0.9234\n",
            "Epoch 641/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0504 - acc: 0.9845 - val_loss: 0.2810 - val_acc: 0.9369\n",
            "Epoch 642/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0482 - acc: 0.9875 - val_loss: 0.2754 - val_acc: 0.9550\n",
            "Epoch 643/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0505 - acc: 0.9845 - val_loss: 0.3069 - val_acc: 0.9459\n",
            "Epoch 644/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0540 - acc: 0.9845 - val_loss: 0.2998 - val_acc: 0.9324\n",
            "Epoch 645/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0504 - acc: 0.9850 - val_loss: 0.3493 - val_acc: 0.9279\n",
            "Epoch 646/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0502 - acc: 0.9855 - val_loss: 0.3836 - val_acc: 0.9279\n",
            "Epoch 647/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0510 - acc: 0.9835 - val_loss: 0.3418 - val_acc: 0.9414\n",
            "Epoch 648/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0490 - acc: 0.9860 - val_loss: 0.3521 - val_acc: 0.9369\n",
            "Epoch 649/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0492 - acc: 0.9890 - val_loss: 0.3923 - val_acc: 0.9324\n",
            "Epoch 650/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0495 - acc: 0.9860 - val_loss: 0.3281 - val_acc: 0.9505\n",
            "Epoch 651/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0541 - acc: 0.9830 - val_loss: 0.3827 - val_acc: 0.9324\n",
            "Epoch 652/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0507 - acc: 0.9860 - val_loss: 0.3171 - val_acc: 0.9505\n",
            "Epoch 653/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0482 - acc: 0.9865 - val_loss: 0.3948 - val_acc: 0.9189\n",
            "Epoch 654/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0704 - acc: 0.9750 - val_loss: 0.4996 - val_acc: 0.8964\n",
            "Epoch 655/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0598 - acc: 0.9780 - val_loss: 0.3670 - val_acc: 0.9369\n",
            "Epoch 656/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0545 - acc: 0.9850 - val_loss: 0.3326 - val_acc: 0.9414\n",
            "Epoch 657/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0507 - acc: 0.9870 - val_loss: 0.3485 - val_acc: 0.9234\n",
            "Epoch 658/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0476 - acc: 0.9865 - val_loss: 0.3394 - val_acc: 0.9369\n",
            "Epoch 659/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0472 - acc: 0.9875 - val_loss: 0.3752 - val_acc: 0.9324\n",
            "Epoch 660/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0479 - acc: 0.9870 - val_loss: 0.3364 - val_acc: 0.9369\n",
            "Epoch 661/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0456 - acc: 0.9880 - val_loss: 0.3383 - val_acc: 0.9279\n",
            "Epoch 662/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0476 - acc: 0.9860 - val_loss: 0.3587 - val_acc: 0.9279\n",
            "Epoch 663/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0501 - acc: 0.9845 - val_loss: 0.3680 - val_acc: 0.9324\n",
            "Epoch 664/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.0659 - acc: 0.9795 - val_loss: 0.3058 - val_acc: 0.9369\n",
            "Epoch 665/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0503 - acc: 0.9850 - val_loss: 0.3373 - val_acc: 0.9369\n",
            "Epoch 666/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0475 - acc: 0.9875 - val_loss: 0.3374 - val_acc: 0.9459\n",
            "Epoch 667/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0475 - acc: 0.9865 - val_loss: 0.3086 - val_acc: 0.9414\n",
            "Epoch 668/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0473 - acc: 0.9870 - val_loss: 0.3323 - val_acc: 0.9369\n",
            "Epoch 669/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0454 - acc: 0.9870 - val_loss: 0.3138 - val_acc: 0.9505\n",
            "Epoch 670/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0478 - acc: 0.9870 - val_loss: 0.3502 - val_acc: 0.9459\n",
            "Epoch 671/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0599 - acc: 0.9825 - val_loss: 0.3251 - val_acc: 0.9369\n",
            "Epoch 672/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0586 - acc: 0.9820 - val_loss: 0.3641 - val_acc: 0.9414\n",
            "Epoch 673/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0615 - acc: 0.9780 - val_loss: 0.3925 - val_acc: 0.9189\n",
            "Epoch 674/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0591 - acc: 0.9815 - val_loss: 0.3613 - val_acc: 0.9279\n",
            "Epoch 675/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0461 - acc: 0.9865 - val_loss: 0.2853 - val_acc: 0.9414\n",
            "Epoch 676/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0461 - acc: 0.9870 - val_loss: 0.3017 - val_acc: 0.9459\n",
            "Epoch 677/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0456 - acc: 0.9875 - val_loss: 0.3038 - val_acc: 0.9459\n",
            "Epoch 678/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0482 - acc: 0.9865 - val_loss: 0.3778 - val_acc: 0.9324\n",
            "Epoch 679/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0464 - acc: 0.9870 - val_loss: 0.3266 - val_acc: 0.9324\n",
            "Epoch 680/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0481 - acc: 0.9865 - val_loss: 0.3610 - val_acc: 0.9279\n",
            "Epoch 681/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0475 - acc: 0.9840 - val_loss: 0.3588 - val_acc: 0.9324\n",
            "Epoch 682/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0461 - acc: 0.9865 - val_loss: 0.3368 - val_acc: 0.9414\n",
            "Epoch 683/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0594 - acc: 0.9830 - val_loss: 0.4231 - val_acc: 0.9279\n",
            "Epoch 684/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0566 - acc: 0.9805 - val_loss: 0.3316 - val_acc: 0.9550\n",
            "Epoch 685/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0682 - acc: 0.9760 - val_loss: 0.3246 - val_acc: 0.9324\n",
            "Epoch 686/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0520 - acc: 0.9845 - val_loss: 0.3406 - val_acc: 0.9324\n",
            "Epoch 687/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0458 - acc: 0.9865 - val_loss: 0.3502 - val_acc: 0.9324\n",
            "Epoch 688/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0456 - acc: 0.9880 - val_loss: 0.3749 - val_acc: 0.9324\n",
            "Epoch 689/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0454 - acc: 0.9870 - val_loss: 0.3217 - val_acc: 0.9414\n",
            "Epoch 690/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0447 - acc: 0.9870 - val_loss: 0.3045 - val_acc: 0.9414\n",
            "Epoch 691/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0456 - acc: 0.9860 - val_loss: 0.3603 - val_acc: 0.9414\n",
            "Epoch 692/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0448 - acc: 0.9870 - val_loss: 0.3035 - val_acc: 0.9459\n",
            "Epoch 693/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0457 - acc: 0.9870 - val_loss: 0.3414 - val_acc: 0.9414\n",
            "Epoch 694/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0457 - acc: 0.9875 - val_loss: 0.3216 - val_acc: 0.9369\n",
            "Epoch 695/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0641 - acc: 0.9790 - val_loss: 0.3103 - val_acc: 0.9459\n",
            "Epoch 696/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0530 - acc: 0.9845 - val_loss: 0.3700 - val_acc: 0.9279\n",
            "Epoch 697/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0493 - acc: 0.9850 - val_loss: 0.3358 - val_acc: 0.9459\n",
            "Epoch 698/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0482 - acc: 0.9855 - val_loss: 0.3135 - val_acc: 0.9459\n",
            "Epoch 699/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0483 - acc: 0.9850 - val_loss: 0.3890 - val_acc: 0.9234\n",
            "Epoch 700/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0489 - acc: 0.9845 - val_loss: 0.3532 - val_acc: 0.9414\n",
            "Epoch 701/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0473 - acc: 0.9845 - val_loss: 0.3524 - val_acc: 0.9414\n",
            "Epoch 702/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0502 - acc: 0.9840 - val_loss: 0.3401 - val_acc: 0.9414\n",
            "Epoch 703/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0551 - acc: 0.9835 - val_loss: 0.3081 - val_acc: 0.9550\n",
            "Epoch 704/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0524 - acc: 0.9845 - val_loss: 0.3509 - val_acc: 0.9324\n",
            "Epoch 705/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0469 - acc: 0.9850 - val_loss: 0.3272 - val_acc: 0.9414\n",
            "Epoch 706/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0468 - acc: 0.9870 - val_loss: 0.3192 - val_acc: 0.9414\n",
            "Epoch 707/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0462 - acc: 0.9885 - val_loss: 0.3444 - val_acc: 0.9414\n",
            "Epoch 708/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0484 - acc: 0.9865 - val_loss: 0.3050 - val_acc: 0.9414\n",
            "Epoch 709/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0562 - acc: 0.9830 - val_loss: 0.2862 - val_acc: 0.9459\n",
            "Epoch 710/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0484 - acc: 0.9855 - val_loss: 0.3403 - val_acc: 0.9459\n",
            "Epoch 711/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0481 - acc: 0.9860 - val_loss: 0.3445 - val_acc: 0.9369\n",
            "Epoch 712/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0469 - acc: 0.9870 - val_loss: 0.3496 - val_acc: 0.9369\n",
            "Epoch 713/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0468 - acc: 0.9860 - val_loss: 0.4120 - val_acc: 0.9189\n",
            "Epoch 714/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0570 - acc: 0.9850 - val_loss: 0.3233 - val_acc: 0.9414\n",
            "Epoch 715/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0591 - acc: 0.9820 - val_loss: 0.3353 - val_acc: 0.9505\n",
            "Epoch 716/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0490 - acc: 0.9840 - val_loss: 0.3733 - val_acc: 0.9369\n",
            "Epoch 717/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0459 - acc: 0.9860 - val_loss: 0.3774 - val_acc: 0.9414\n",
            "Epoch 718/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0528 - acc: 0.9840 - val_loss: 0.3714 - val_acc: 0.9324\n",
            "Epoch 719/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0452 - acc: 0.9865 - val_loss: 0.3423 - val_acc: 0.9505\n",
            "Epoch 720/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0466 - acc: 0.9875 - val_loss: 0.4247 - val_acc: 0.9324\n",
            "Epoch 721/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0487 - acc: 0.9860 - val_loss: 0.3749 - val_acc: 0.9369\n",
            "Epoch 722/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0525 - acc: 0.9840 - val_loss: 0.3404 - val_acc: 0.9369\n",
            "Epoch 723/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0514 - acc: 0.9845 - val_loss: 0.3555 - val_acc: 0.9414\n",
            "Epoch 724/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0454 - acc: 0.9865 - val_loss: 0.4157 - val_acc: 0.9279\n",
            "Epoch 725/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0433 - acc: 0.9885 - val_loss: 0.3234 - val_acc: 0.9505\n",
            "Epoch 726/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0431 - acc: 0.9890 - val_loss: 0.4003 - val_acc: 0.9414\n",
            "Epoch 727/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0471 - acc: 0.9880 - val_loss: 0.3563 - val_acc: 0.9324\n",
            "Epoch 728/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0507 - acc: 0.9850 - val_loss: 0.4573 - val_acc: 0.9099\n",
            "Epoch 729/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0493 - acc: 0.9840 - val_loss: 0.3376 - val_acc: 0.9369\n",
            "Epoch 730/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0487 - acc: 0.9860 - val_loss: 0.3481 - val_acc: 0.9414\n",
            "Epoch 731/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0458 - acc: 0.9880 - val_loss: 0.3773 - val_acc: 0.9369\n",
            "Epoch 732/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0454 - acc: 0.9870 - val_loss: 0.3603 - val_acc: 0.9414\n",
            "Epoch 733/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0439 - acc: 0.9865 - val_loss: 0.3898 - val_acc: 0.9324\n",
            "Epoch 734/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0431 - acc: 0.9880 - val_loss: 0.3563 - val_acc: 0.9505\n",
            "Epoch 735/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0468 - acc: 0.9850 - val_loss: 0.3744 - val_acc: 0.9324\n",
            "Epoch 736/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0456 - acc: 0.9865 - val_loss: 0.4096 - val_acc: 0.9279\n",
            "Epoch 737/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0500 - acc: 0.9840 - val_loss: 0.4805 - val_acc: 0.9234\n",
            "Epoch 738/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0771 - acc: 0.9730 - val_loss: 0.3019 - val_acc: 0.9595\n",
            "Epoch 739/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0533 - acc: 0.9830 - val_loss: 0.3565 - val_acc: 0.9369\n",
            "Epoch 740/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0504 - acc: 0.9840 - val_loss: 0.3446 - val_acc: 0.9505\n",
            "Epoch 741/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0438 - acc: 0.9880 - val_loss: 0.3332 - val_acc: 0.9459\n",
            "Epoch 742/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0418 - acc: 0.9895 - val_loss: 0.3756 - val_acc: 0.9324\n",
            "Epoch 743/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0419 - acc: 0.9875 - val_loss: 0.3974 - val_acc: 0.9234\n",
            "Epoch 744/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0466 - acc: 0.9870 - val_loss: 0.3310 - val_acc: 0.9459\n",
            "Epoch 745/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0467 - acc: 0.9870 - val_loss: 0.3782 - val_acc: 0.9459\n",
            "Epoch 746/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0424 - acc: 0.9900 - val_loss: 0.4137 - val_acc: 0.9279\n",
            "Epoch 747/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0477 - acc: 0.9870 - val_loss: 0.4111 - val_acc: 0.9324\n",
            "Epoch 748/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0472 - acc: 0.9870 - val_loss: 0.3254 - val_acc: 0.9369\n",
            "Epoch 749/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0562 - acc: 0.9810 - val_loss: 0.3347 - val_acc: 0.9414\n",
            "Epoch 750/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0566 - acc: 0.9820 - val_loss: 0.4060 - val_acc: 0.9369\n",
            "Epoch 751/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0472 - acc: 0.9860 - val_loss: 0.3574 - val_acc: 0.9459\n",
            "Epoch 752/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0456 - acc: 0.9865 - val_loss: 0.3366 - val_acc: 0.9505\n",
            "Epoch 753/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0446 - acc: 0.9870 - val_loss: 0.3404 - val_acc: 0.9459\n",
            "Epoch 754/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0438 - acc: 0.9865 - val_loss: 0.3571 - val_acc: 0.9459\n",
            "Epoch 755/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0415 - acc: 0.9875 - val_loss: 0.3728 - val_acc: 0.9414\n",
            "Epoch 756/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0416 - acc: 0.9890 - val_loss: 0.3542 - val_acc: 0.9459\n",
            "Epoch 757/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0420 - acc: 0.9890 - val_loss: 0.4321 - val_acc: 0.9189\n",
            "Epoch 758/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0440 - acc: 0.9870 - val_loss: 0.3618 - val_acc: 0.9369\n",
            "Epoch 759/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0523 - acc: 0.9840 - val_loss: 0.3697 - val_acc: 0.9369\n",
            "Epoch 760/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0635 - acc: 0.9795 - val_loss: 0.4019 - val_acc: 0.9414\n",
            "Epoch 761/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0537 - acc: 0.9845 - val_loss: 0.4020 - val_acc: 0.9369\n",
            "Epoch 762/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0450 - acc: 0.9860 - val_loss: 0.3534 - val_acc: 0.9369\n",
            "Epoch 763/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0425 - acc: 0.9875 - val_loss: 0.3818 - val_acc: 0.9369\n",
            "Epoch 764/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0434 - acc: 0.9875 - val_loss: 0.4026 - val_acc: 0.9324\n",
            "Epoch 765/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0426 - acc: 0.9875 - val_loss: 0.4339 - val_acc: 0.9324\n",
            "Epoch 766/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0443 - acc: 0.9865 - val_loss: 0.4257 - val_acc: 0.9324\n",
            "Epoch 767/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0457 - acc: 0.9875 - val_loss: 0.4144 - val_acc: 0.9414\n",
            "Epoch 768/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0426 - acc: 0.9875 - val_loss: 0.4064 - val_acc: 0.9414\n",
            "Epoch 769/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0418 - acc: 0.9870 - val_loss: 0.4175 - val_acc: 0.9414\n",
            "Epoch 770/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0484 - acc: 0.9865 - val_loss: 0.4108 - val_acc: 0.9414\n",
            "Epoch 771/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0618 - acc: 0.9790 - val_loss: 0.4453 - val_acc: 0.9279\n",
            "Epoch 772/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0495 - acc: 0.9835 - val_loss: 0.4208 - val_acc: 0.9324\n",
            "Epoch 773/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0445 - acc: 0.9870 - val_loss: 0.4583 - val_acc: 0.9279\n",
            "Epoch 774/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0432 - acc: 0.9875 - val_loss: 0.4617 - val_acc: 0.9324\n",
            "Epoch 775/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0409 - acc: 0.9880 - val_loss: 0.3786 - val_acc: 0.9414\n",
            "Epoch 776/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0414 - acc: 0.9885 - val_loss: 0.3995 - val_acc: 0.9459\n",
            "Epoch 777/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0411 - acc: 0.9870 - val_loss: 0.3936 - val_acc: 0.9414\n",
            "Epoch 778/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0424 - acc: 0.9875 - val_loss: 0.3944 - val_acc: 0.9459\n",
            "Epoch 779/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0445 - acc: 0.9870 - val_loss: 0.3535 - val_acc: 0.9550\n",
            "Epoch 780/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0644 - acc: 0.9815 - val_loss: 0.3820 - val_acc: 0.9459\n",
            "Epoch 781/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0496 - acc: 0.9850 - val_loss: 0.4126 - val_acc: 0.9369\n",
            "Epoch 782/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0472 - acc: 0.9865 - val_loss: 0.4744 - val_acc: 0.9324\n",
            "Epoch 783/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0486 - acc: 0.9875 - val_loss: 0.3588 - val_acc: 0.9505\n",
            "Epoch 784/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0447 - acc: 0.9850 - val_loss: 0.3576 - val_acc: 0.9459\n",
            "Epoch 785/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0420 - acc: 0.9895 - val_loss: 0.3536 - val_acc: 0.9324\n",
            "Epoch 786/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0422 - acc: 0.9885 - val_loss: 0.3868 - val_acc: 0.9369\n",
            "Epoch 787/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.0429 - acc: 0.9875 - val_loss: 0.3993 - val_acc: 0.9414\n",
            "Epoch 788/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0432 - acc: 0.9880 - val_loss: 0.3703 - val_acc: 0.9414\n",
            "Epoch 789/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0431 - acc: 0.9875 - val_loss: 0.3943 - val_acc: 0.9369\n",
            "Epoch 790/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0444 - acc: 0.9865 - val_loss: 0.4232 - val_acc: 0.9369\n",
            "Epoch 791/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0522 - acc: 0.9855 - val_loss: 0.3253 - val_acc: 0.9279\n",
            "Epoch 792/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0473 - acc: 0.9840 - val_loss: 0.4021 - val_acc: 0.9234\n",
            "Epoch 793/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0430 - acc: 0.9890 - val_loss: 0.4153 - val_acc: 0.9324\n",
            "Epoch 794/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0492 - acc: 0.9870 - val_loss: 0.3567 - val_acc: 0.9414\n",
            "Epoch 795/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0463 - acc: 0.9870 - val_loss: 0.4232 - val_acc: 0.9324\n",
            "Epoch 796/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0450 - acc: 0.9865 - val_loss: 0.3530 - val_acc: 0.9414\n",
            "Epoch 797/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0417 - acc: 0.9875 - val_loss: 0.4113 - val_acc: 0.9414\n",
            "Epoch 798/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0417 - acc: 0.9890 - val_loss: 0.3550 - val_acc: 0.9459\n",
            "Epoch 799/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0432 - acc: 0.9875 - val_loss: 0.3780 - val_acc: 0.9459\n",
            "Epoch 800/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0421 - acc: 0.9880 - val_loss: 0.4202 - val_acc: 0.9279\n",
            "Epoch 801/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0432 - acc: 0.9875 - val_loss: 0.3921 - val_acc: 0.9459\n",
            "Epoch 802/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0395 - acc: 0.9895 - val_loss: 0.4323 - val_acc: 0.9279\n",
            "Epoch 803/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0424 - acc: 0.9875 - val_loss: 0.3992 - val_acc: 0.9369\n",
            "Epoch 804/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0510 - acc: 0.9840 - val_loss: 0.4509 - val_acc: 0.9234\n",
            "Epoch 805/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0506 - acc: 0.9840 - val_loss: 0.4035 - val_acc: 0.9324\n",
            "Epoch 806/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0558 - acc: 0.9810 - val_loss: 0.4049 - val_acc: 0.9505\n",
            "Epoch 807/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0601 - acc: 0.9805 - val_loss: 0.3737 - val_acc: 0.9459\n",
            "Epoch 808/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0518 - acc: 0.9840 - val_loss: 0.4503 - val_acc: 0.9279\n",
            "Epoch 809/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0448 - acc: 0.9870 - val_loss: 0.3921 - val_acc: 0.9459\n",
            "Epoch 810/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0421 - acc: 0.9870 - val_loss: 0.3772 - val_acc: 0.9505\n",
            "Epoch 811/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0419 - acc: 0.9885 - val_loss: 0.4556 - val_acc: 0.9324\n",
            "Epoch 812/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0400 - acc: 0.9875 - val_loss: 0.4179 - val_acc: 0.9369\n",
            "Epoch 813/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0403 - acc: 0.9880 - val_loss: 0.5079 - val_acc: 0.9144\n",
            "Epoch 814/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0416 - acc: 0.9870 - val_loss: 0.3924 - val_acc: 0.9414\n",
            "Epoch 815/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0419 - acc: 0.9890 - val_loss: 0.4161 - val_acc: 0.9324\n",
            "Epoch 816/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0531 - acc: 0.9825 - val_loss: 0.4294 - val_acc: 0.9459\n",
            "Epoch 817/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0554 - acc: 0.9775 - val_loss: 0.4022 - val_acc: 0.9324\n",
            "Epoch 818/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0481 - acc: 0.9850 - val_loss: 0.4747 - val_acc: 0.9234\n",
            "Epoch 819/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0467 - acc: 0.9865 - val_loss: 0.4665 - val_acc: 0.9144\n",
            "Epoch 820/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0408 - acc: 0.9870 - val_loss: 0.4575 - val_acc: 0.9234\n",
            "Epoch 821/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0410 - acc: 0.9885 - val_loss: 0.4447 - val_acc: 0.9189\n",
            "Epoch 822/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0396 - acc: 0.9875 - val_loss: 0.4224 - val_acc: 0.9324\n",
            "Epoch 823/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0397 - acc: 0.9880 - val_loss: 0.4369 - val_acc: 0.9189\n",
            "Epoch 824/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0394 - acc: 0.9900 - val_loss: 0.4197 - val_acc: 0.9414\n",
            "Epoch 825/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0406 - acc: 0.9880 - val_loss: 0.4151 - val_acc: 0.9234\n",
            "Epoch 826/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0449 - acc: 0.9865 - val_loss: 0.3890 - val_acc: 0.9369\n",
            "Epoch 827/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0471 - acc: 0.9830 - val_loss: 0.4078 - val_acc: 0.9414\n",
            "Epoch 828/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0420 - acc: 0.9870 - val_loss: 0.3763 - val_acc: 0.9505\n",
            "Epoch 829/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0392 - acc: 0.9890 - val_loss: 0.4248 - val_acc: 0.9324\n",
            "Epoch 830/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0393 - acc: 0.9895 - val_loss: 0.4259 - val_acc: 0.9369\n",
            "Epoch 831/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0390 - acc: 0.9880 - val_loss: 0.4221 - val_acc: 0.9369\n",
            "Epoch 832/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0512 - acc: 0.9840 - val_loss: 0.3733 - val_acc: 0.9505\n",
            "Epoch 833/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0420 - acc: 0.9895 - val_loss: 0.4699 - val_acc: 0.9324\n",
            "Epoch 834/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0682 - acc: 0.9805 - val_loss: 0.4159 - val_acc: 0.9279\n",
            "Epoch 835/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0478 - acc: 0.9835 - val_loss: 0.4255 - val_acc: 0.9279\n",
            "Epoch 836/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0421 - acc: 0.9875 - val_loss: 0.3939 - val_acc: 0.9369\n",
            "Epoch 837/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0386 - acc: 0.9895 - val_loss: 0.4310 - val_acc: 0.9279\n",
            "Epoch 838/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0388 - acc: 0.9880 - val_loss: 0.3972 - val_acc: 0.9414\n",
            "Epoch 839/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0375 - acc: 0.9905 - val_loss: 0.4676 - val_acc: 0.9279\n",
            "Epoch 840/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0384 - acc: 0.9885 - val_loss: 0.4408 - val_acc: 0.9189\n",
            "Epoch 841/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0399 - acc: 0.9890 - val_loss: 0.4633 - val_acc: 0.9279\n",
            "Epoch 842/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0563 - acc: 0.9840 - val_loss: 0.4689 - val_acc: 0.9234\n",
            "Epoch 843/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0639 - acc: 0.9800 - val_loss: 0.4579 - val_acc: 0.9144\n",
            "Epoch 844/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0451 - acc: 0.9855 - val_loss: 0.4409 - val_acc: 0.9144\n",
            "Epoch 845/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0400 - acc: 0.9895 - val_loss: 0.4207 - val_acc: 0.9234\n",
            "Epoch 846/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0376 - acc: 0.9910 - val_loss: 0.4222 - val_acc: 0.9189\n",
            "Epoch 847/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0389 - acc: 0.9870 - val_loss: 0.4054 - val_acc: 0.9279\n",
            "Epoch 848/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0432 - acc: 0.9880 - val_loss: 0.4308 - val_acc: 0.9189\n",
            "Epoch 849/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0402 - acc: 0.9890 - val_loss: 0.3961 - val_acc: 0.9279\n",
            "Epoch 850/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0396 - acc: 0.9885 - val_loss: 0.3970 - val_acc: 0.9234\n",
            "Epoch 851/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0378 - acc: 0.9900 - val_loss: 0.4005 - val_acc: 0.9279\n",
            "Epoch 852/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0380 - acc: 0.9890 - val_loss: 0.4067 - val_acc: 0.9234\n",
            "Epoch 853/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0393 - acc: 0.9900 - val_loss: 0.4086 - val_acc: 0.9189\n",
            "Epoch 854/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0386 - acc: 0.9885 - val_loss: 0.3716 - val_acc: 0.9369\n",
            "Epoch 855/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0393 - acc: 0.9885 - val_loss: 0.3834 - val_acc: 0.9459\n",
            "Epoch 856/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0537 - acc: 0.9845 - val_loss: 0.4004 - val_acc: 0.9279\n",
            "Epoch 857/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0490 - acc: 0.9840 - val_loss: 0.4626 - val_acc: 0.9189\n",
            "Epoch 858/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0482 - acc: 0.9825 - val_loss: 0.3845 - val_acc: 0.9369\n",
            "Epoch 859/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0427 - acc: 0.9875 - val_loss: 0.5106 - val_acc: 0.9099\n",
            "Epoch 860/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0402 - acc: 0.9860 - val_loss: 0.4585 - val_acc: 0.9189\n",
            "Epoch 861/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0383 - acc: 0.9890 - val_loss: 0.4231 - val_acc: 0.9234\n",
            "Epoch 862/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0373 - acc: 0.9875 - val_loss: 0.3726 - val_acc: 0.9505\n",
            "Epoch 863/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0386 - acc: 0.9885 - val_loss: 0.4646 - val_acc: 0.9189\n",
            "Epoch 864/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0395 - acc: 0.9880 - val_loss: 0.4935 - val_acc: 0.9324\n",
            "Epoch 865/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0399 - acc: 0.9870 - val_loss: 0.4621 - val_acc: 0.9279\n",
            "Epoch 866/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0387 - acc: 0.9895 - val_loss: 0.4554 - val_acc: 0.9279\n",
            "Epoch 867/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0390 - acc: 0.9890 - val_loss: 0.3924 - val_acc: 0.9279\n",
            "Epoch 868/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0495 - acc: 0.9840 - val_loss: 0.5652 - val_acc: 0.9189\n",
            "Epoch 869/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0718 - acc: 0.9770 - val_loss: 0.3624 - val_acc: 0.9505\n",
            "Epoch 870/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0551 - acc: 0.9840 - val_loss: 0.4242 - val_acc: 0.9279\n",
            "Epoch 871/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0453 - acc: 0.9860 - val_loss: 0.4386 - val_acc: 0.9189\n",
            "Epoch 872/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0394 - acc: 0.9895 - val_loss: 0.3598 - val_acc: 0.9459\n",
            "Epoch 873/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0387 - acc: 0.9895 - val_loss: 0.4477 - val_acc: 0.9189\n",
            "Epoch 874/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0379 - acc: 0.9895 - val_loss: 0.4224 - val_acc: 0.9414\n",
            "Epoch 875/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0375 - acc: 0.9885 - val_loss: 0.4074 - val_acc: 0.9234\n",
            "Epoch 876/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0366 - acc: 0.9900 - val_loss: 0.3883 - val_acc: 0.9459\n",
            "Epoch 877/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0376 - acc: 0.9885 - val_loss: 0.3847 - val_acc: 0.9414\n",
            "Epoch 878/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0393 - acc: 0.9865 - val_loss: 0.4242 - val_acc: 0.9369\n",
            "Epoch 879/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0507 - acc: 0.9825 - val_loss: 0.5531 - val_acc: 0.8919\n",
            "Epoch 880/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0617 - acc: 0.9780 - val_loss: 0.4116 - val_acc: 0.9234\n",
            "Epoch 881/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0461 - acc: 0.9850 - val_loss: 0.4169 - val_acc: 0.9279\n",
            "Epoch 882/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0386 - acc: 0.9885 - val_loss: 0.4077 - val_acc: 0.9279\n",
            "Epoch 883/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0375 - acc: 0.9885 - val_loss: 0.4214 - val_acc: 0.9279\n",
            "Epoch 884/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0362 - acc: 0.9900 - val_loss: 0.4139 - val_acc: 0.9324\n",
            "Epoch 885/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0362 - acc: 0.9890 - val_loss: 0.4919 - val_acc: 0.9189\n",
            "Epoch 886/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0365 - acc: 0.9880 - val_loss: 0.3868 - val_acc: 0.9324\n",
            "Epoch 887/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0370 - acc: 0.9890 - val_loss: 0.4633 - val_acc: 0.9324\n",
            "Epoch 888/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0364 - acc: 0.9895 - val_loss: 0.4653 - val_acc: 0.9279\n",
            "Epoch 889/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0365 - acc: 0.9905 - val_loss: 0.4382 - val_acc: 0.9369\n",
            "Epoch 890/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0372 - acc: 0.9890 - val_loss: 0.4196 - val_acc: 0.9234\n",
            "Epoch 891/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0370 - acc: 0.9910 - val_loss: 0.4280 - val_acc: 0.9279\n",
            "Epoch 892/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0618 - acc: 0.9825 - val_loss: 0.3175 - val_acc: 0.9459\n",
            "Epoch 893/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0686 - acc: 0.9760 - val_loss: 0.4684 - val_acc: 0.9234\n",
            "Epoch 894/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0456 - acc: 0.9855 - val_loss: 0.5298 - val_acc: 0.9234\n",
            "Epoch 895/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0396 - acc: 0.9880 - val_loss: 0.4348 - val_acc: 0.9414\n",
            "Epoch 896/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0371 - acc: 0.9890 - val_loss: 0.4293 - val_acc: 0.9234\n",
            "Epoch 897/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0360 - acc: 0.9885 - val_loss: 0.4265 - val_acc: 0.9369\n",
            "Epoch 898/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0362 - acc: 0.9900 - val_loss: 0.5100 - val_acc: 0.9009\n",
            "Epoch 899/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0364 - acc: 0.9880 - val_loss: 0.4618 - val_acc: 0.9234\n",
            "Epoch 900/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0379 - acc: 0.9895 - val_loss: 0.4308 - val_acc: 0.9234\n",
            "Epoch 901/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0417 - acc: 0.9860 - val_loss: 0.3682 - val_acc: 0.9550\n",
            "Epoch 902/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0411 - acc: 0.9890 - val_loss: 0.4918 - val_acc: 0.9279\n",
            "Epoch 903/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0372 - acc: 0.9890 - val_loss: 0.4260 - val_acc: 0.9414\n",
            "Epoch 904/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0356 - acc: 0.9900 - val_loss: 0.4565 - val_acc: 0.9324\n",
            "Epoch 905/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0362 - acc: 0.9900 - val_loss: 0.4203 - val_acc: 0.9369\n",
            "Epoch 906/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0394 - acc: 0.9890 - val_loss: 0.4411 - val_acc: 0.9324\n",
            "Epoch 907/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0422 - acc: 0.9875 - val_loss: 0.4296 - val_acc: 0.9279\n",
            "Epoch 908/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0605 - acc: 0.9805 - val_loss: 0.3892 - val_acc: 0.9459\n",
            "Epoch 909/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0494 - acc: 0.9840 - val_loss: 0.5183 - val_acc: 0.9279\n",
            "Epoch 910/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0453 - acc: 0.9860 - val_loss: 0.4048 - val_acc: 0.9369\n",
            "Epoch 911/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0379 - acc: 0.9890 - val_loss: 0.4149 - val_acc: 0.9459\n",
            "Epoch 912/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0364 - acc: 0.9900 - val_loss: 0.4606 - val_acc: 0.9279\n",
            "Epoch 913/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0381 - acc: 0.9895 - val_loss: 0.3995 - val_acc: 0.9459\n",
            "Epoch 914/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0358 - acc: 0.9900 - val_loss: 0.4541 - val_acc: 0.9279\n",
            "Epoch 915/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0359 - acc: 0.9890 - val_loss: 0.4638 - val_acc: 0.9279\n",
            "Epoch 916/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0359 - acc: 0.9915 - val_loss: 0.4685 - val_acc: 0.9189\n",
            "Epoch 917/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0356 - acc: 0.9905 - val_loss: 0.4453 - val_acc: 0.9414\n",
            "Epoch 918/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0366 - acc: 0.9890 - val_loss: 0.4335 - val_acc: 0.9189\n",
            "Epoch 919/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0368 - acc: 0.9895 - val_loss: 0.4491 - val_acc: 0.9414\n",
            "Epoch 920/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0392 - acc: 0.9880 - val_loss: 0.3950 - val_acc: 0.9369\n",
            "Epoch 921/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0612 - acc: 0.9795 - val_loss: 0.3338 - val_acc: 0.9550\n",
            "Epoch 922/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0494 - acc: 0.9840 - val_loss: 0.4229 - val_acc: 0.9144\n",
            "Epoch 923/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0579 - acc: 0.9770 - val_loss: 0.3310 - val_acc: 0.9459\n",
            "Epoch 924/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0469 - acc: 0.9840 - val_loss: 0.4110 - val_acc: 0.9144\n",
            "Epoch 925/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0419 - acc: 0.9855 - val_loss: 0.3638 - val_acc: 0.9414\n",
            "Epoch 926/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0373 - acc: 0.9890 - val_loss: 0.3547 - val_acc: 0.9505\n",
            "Epoch 927/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0355 - acc: 0.9900 - val_loss: 0.3797 - val_acc: 0.9459\n",
            "Epoch 928/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0358 - acc: 0.9885 - val_loss: 0.3858 - val_acc: 0.9279\n",
            "Epoch 929/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0347 - acc: 0.9915 - val_loss: 0.4102 - val_acc: 0.9189\n",
            "Epoch 930/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0344 - acc: 0.9890 - val_loss: 0.3768 - val_acc: 0.9324\n",
            "Epoch 931/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0346 - acc: 0.9895 - val_loss: 0.4340 - val_acc: 0.9189\n",
            "Epoch 932/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0344 - acc: 0.9905 - val_loss: 0.4289 - val_acc: 0.9189\n",
            "Epoch 933/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0466 - acc: 0.9855 - val_loss: 0.5447 - val_acc: 0.9324\n",
            "Epoch 934/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0575 - acc: 0.9805 - val_loss: 0.4469 - val_acc: 0.9099\n",
            "Epoch 935/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0392 - acc: 0.9880 - val_loss: 0.4734 - val_acc: 0.9189\n",
            "Epoch 936/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0361 - acc: 0.9900 - val_loss: 0.4365 - val_acc: 0.9369\n",
            "Epoch 937/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0349 - acc: 0.9895 - val_loss: 0.4188 - val_acc: 0.9414\n",
            "Epoch 938/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0341 - acc: 0.9895 - val_loss: 0.4281 - val_acc: 0.9189\n",
            "Epoch 939/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0339 - acc: 0.9910 - val_loss: 0.3891 - val_acc: 0.9459\n",
            "Epoch 940/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0383 - acc: 0.9890 - val_loss: 0.4712 - val_acc: 0.9279\n",
            "Epoch 941/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0467 - acc: 0.9860 - val_loss: 0.4698 - val_acc: 0.9189\n",
            "Epoch 942/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0537 - acc: 0.9800 - val_loss: 0.3680 - val_acc: 0.9550\n",
            "Epoch 943/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0478 - acc: 0.9830 - val_loss: 0.4033 - val_acc: 0.9414\n",
            "Epoch 944/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0441 - acc: 0.9860 - val_loss: 0.4278 - val_acc: 0.9189\n",
            "Epoch 945/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0360 - acc: 0.9895 - val_loss: 0.4227 - val_acc: 0.9369\n",
            "Epoch 946/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0349 - acc: 0.9900 - val_loss: 0.4440 - val_acc: 0.9324\n",
            "Epoch 947/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0341 - acc: 0.9905 - val_loss: 0.4288 - val_acc: 0.9324\n",
            "Epoch 948/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0348 - acc: 0.9890 - val_loss: 0.4178 - val_acc: 0.9279\n",
            "Epoch 949/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0331 - acc: 0.9910 - val_loss: 0.4439 - val_acc: 0.9459\n",
            "Epoch 950/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0350 - acc: 0.9895 - val_loss: 0.5042 - val_acc: 0.9054\n",
            "Epoch 951/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0371 - acc: 0.9890 - val_loss: 0.4490 - val_acc: 0.9324\n",
            "Epoch 952/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0341 - acc: 0.9910 - val_loss: 0.5015 - val_acc: 0.8874\n",
            "Epoch 953/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0418 - acc: 0.9890 - val_loss: 0.6063 - val_acc: 0.8919\n",
            "Epoch 954/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0594 - acc: 0.9795 - val_loss: 0.4869 - val_acc: 0.9279\n",
            "Epoch 955/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0484 - acc: 0.9840 - val_loss: 0.4609 - val_acc: 0.9099\n",
            "Epoch 956/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0413 - acc: 0.9865 - val_loss: 0.3741 - val_acc: 0.9414\n",
            "Epoch 957/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0359 - acc: 0.9895 - val_loss: 0.4636 - val_acc: 0.9009\n",
            "Epoch 958/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0343 - acc: 0.9900 - val_loss: 0.4704 - val_acc: 0.9189\n",
            "Epoch 959/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0329 - acc: 0.9905 - val_loss: 0.4204 - val_acc: 0.9414\n",
            "Epoch 960/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0354 - acc: 0.9900 - val_loss: 0.4688 - val_acc: 0.9189\n",
            "Epoch 961/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0344 - acc: 0.9900 - val_loss: 0.4390 - val_acc: 0.9189\n",
            "Epoch 962/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0357 - acc: 0.9900 - val_loss: 0.4421 - val_acc: 0.9414\n",
            "Epoch 963/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0354 - acc: 0.9895 - val_loss: 0.4521 - val_acc: 0.9279\n",
            "Epoch 964/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0347 - acc: 0.9895 - val_loss: 0.4812 - val_acc: 0.9234\n",
            "Epoch 965/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0370 - acc: 0.9875 - val_loss: 0.4408 - val_acc: 0.9459\n",
            "Epoch 966/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0368 - acc: 0.9900 - val_loss: 0.4952 - val_acc: 0.9369\n",
            "Epoch 967/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0499 - acc: 0.9860 - val_loss: 0.4540 - val_acc: 0.9324\n",
            "Epoch 968/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0590 - acc: 0.9800 - val_loss: 0.4032 - val_acc: 0.9414\n",
            "Epoch 969/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0462 - acc: 0.9855 - val_loss: 0.4858 - val_acc: 0.9189\n",
            "Epoch 970/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0371 - acc: 0.9905 - val_loss: 0.4394 - val_acc: 0.9369\n",
            "Epoch 971/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0339 - acc: 0.9910 - val_loss: 0.4353 - val_acc: 0.9324\n",
            "Epoch 972/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0334 - acc: 0.9910 - val_loss: 0.4496 - val_acc: 0.9144\n",
            "Epoch 973/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0333 - acc: 0.9905 - val_loss: 0.4634 - val_acc: 0.9189\n",
            "Epoch 974/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0340 - acc: 0.9900 - val_loss: 0.4540 - val_acc: 0.9369\n",
            "Epoch 975/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0363 - acc: 0.9910 - val_loss: 0.4591 - val_acc: 0.9234\n",
            "Epoch 976/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0351 - acc: 0.9895 - val_loss: 0.4786 - val_acc: 0.9234\n",
            "Epoch 977/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0344 - acc: 0.9900 - val_loss: 0.4768 - val_acc: 0.9324\n",
            "Epoch 978/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0335 - acc: 0.9910 - val_loss: 0.4677 - val_acc: 0.9189\n",
            "Epoch 979/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0340 - acc: 0.9915 - val_loss: 0.4827 - val_acc: 0.9099\n",
            "Epoch 980/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0342 - acc: 0.9900 - val_loss: 0.4497 - val_acc: 0.9459\n",
            "Epoch 981/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0560 - acc: 0.9805 - val_loss: 0.5004 - val_acc: 0.9189\n",
            "Epoch 982/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0453 - acc: 0.9870 - val_loss: 0.4925 - val_acc: 0.9369\n",
            "Epoch 983/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0379 - acc: 0.9885 - val_loss: 0.4905 - val_acc: 0.9099\n",
            "Epoch 984/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0347 - acc: 0.9895 - val_loss: 0.4555 - val_acc: 0.9234\n",
            "Epoch 985/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0334 - acc: 0.9910 - val_loss: 0.4853 - val_acc: 0.9144\n",
            "Epoch 986/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0329 - acc: 0.9910 - val_loss: 0.4791 - val_acc: 0.9279\n",
            "Epoch 987/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0324 - acc: 0.9910 - val_loss: 0.4014 - val_acc: 0.9505\n",
            "Epoch 988/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0340 - acc: 0.9910 - val_loss: 0.4542 - val_acc: 0.9414\n",
            "Epoch 989/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0520 - acc: 0.9835 - val_loss: 0.4847 - val_acc: 0.9144\n",
            "Epoch 990/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0525 - acc: 0.9810 - val_loss: 0.4023 - val_acc: 0.9459\n",
            "Epoch 991/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0431 - acc: 0.9855 - val_loss: 0.4319 - val_acc: 0.9369\n",
            "Epoch 992/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0341 - acc: 0.9900 - val_loss: 0.4504 - val_acc: 0.9234\n",
            "Epoch 993/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0332 - acc: 0.9910 - val_loss: 0.4515 - val_acc: 0.9369\n",
            "Epoch 994/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.0327 - acc: 0.9915 - val_loss: 0.4148 - val_acc: 0.9459\n",
            "Epoch 995/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0331 - acc: 0.9910 - val_loss: 0.4959 - val_acc: 0.9054\n",
            "Epoch 996/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0322 - acc: 0.9910 - val_loss: 0.4509 - val_acc: 0.9414\n",
            "Epoch 997/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0323 - acc: 0.9895 - val_loss: 0.4570 - val_acc: 0.9414\n",
            "Epoch 998/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0330 - acc: 0.9900 - val_loss: 0.4524 - val_acc: 0.9369\n",
            "Epoch 999/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0414 - acc: 0.9870 - val_loss: 0.5093 - val_acc: 0.9324\n",
            "Epoch 1000/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0529 - acc: 0.9830 - val_loss: 0.4800 - val_acc: 0.9234\n",
            "Epoch 1001/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0415 - acc: 0.9875 - val_loss: 0.5314 - val_acc: 0.9099\n",
            "Epoch 1002/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0367 - acc: 0.9895 - val_loss: 0.4892 - val_acc: 0.9144\n",
            "Epoch 1003/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0342 - acc: 0.9905 - val_loss: 0.4780 - val_acc: 0.9324\n",
            "Epoch 1004/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0326 - acc: 0.9900 - val_loss: 0.4921 - val_acc: 0.9324\n",
            "Epoch 1005/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0327 - acc: 0.9900 - val_loss: 0.4839 - val_acc: 0.9369\n",
            "Epoch 1006/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0323 - acc: 0.9910 - val_loss: 0.4724 - val_acc: 0.9369\n",
            "Epoch 1007/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0328 - acc: 0.9900 - val_loss: 0.4644 - val_acc: 0.9369\n",
            "Epoch 1008/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0328 - acc: 0.9900 - val_loss: 0.5214 - val_acc: 0.9279\n",
            "Epoch 1009/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0334 - acc: 0.9890 - val_loss: 0.4557 - val_acc: 0.9279\n",
            "Epoch 1010/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0348 - acc: 0.9890 - val_loss: 0.4199 - val_acc: 0.9324\n",
            "Epoch 1011/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0362 - acc: 0.9890 - val_loss: 0.4667 - val_acc: 0.9054\n",
            "Epoch 1012/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0512 - acc: 0.9835 - val_loss: 0.6442 - val_acc: 0.8874\n",
            "Epoch 1013/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0550 - acc: 0.9830 - val_loss: 0.5249 - val_acc: 0.8964\n",
            "Epoch 1014/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0425 - acc: 0.9845 - val_loss: 0.3821 - val_acc: 0.9144\n",
            "Epoch 1015/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0391 - acc: 0.9890 - val_loss: 0.4028 - val_acc: 0.9414\n",
            "Epoch 1016/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0343 - acc: 0.9885 - val_loss: 0.4494 - val_acc: 0.9414\n",
            "Epoch 1017/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0344 - acc: 0.9890 - val_loss: 0.4366 - val_acc: 0.9099\n",
            "Epoch 1018/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0326 - acc: 0.9905 - val_loss: 0.4427 - val_acc: 0.9414\n",
            "Epoch 1019/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0332 - acc: 0.9895 - val_loss: 0.4332 - val_acc: 0.9369\n",
            "Epoch 1020/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0326 - acc: 0.9900 - val_loss: 0.4328 - val_acc: 0.9369\n",
            "Epoch 1021/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0318 - acc: 0.9910 - val_loss: 0.4345 - val_acc: 0.9324\n",
            "Epoch 1022/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0335 - acc: 0.9905 - val_loss: 0.4905 - val_acc: 0.9009\n",
            "Epoch 1023/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0323 - acc: 0.9910 - val_loss: 0.4300 - val_acc: 0.9369\n",
            "Epoch 1024/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0356 - acc: 0.9890 - val_loss: 0.4558 - val_acc: 0.8964\n",
            "Epoch 1025/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0463 - acc: 0.9855 - val_loss: 0.4509 - val_acc: 0.9144\n",
            "Epoch 1026/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0680 - acc: 0.9795 - val_loss: 0.4557 - val_acc: 0.9234\n",
            "Epoch 1027/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0419 - acc: 0.9880 - val_loss: 0.4312 - val_acc: 0.9414\n",
            "Epoch 1028/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0409 - acc: 0.9885 - val_loss: 0.4605 - val_acc: 0.9144\n",
            "Epoch 1029/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0376 - acc: 0.9905 - val_loss: 0.4847 - val_acc: 0.9324\n",
            "Epoch 1030/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0337 - acc: 0.9890 - val_loss: 0.5031 - val_acc: 0.9234\n",
            "Epoch 1031/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0326 - acc: 0.9905 - val_loss: 0.4830 - val_acc: 0.9279\n",
            "Epoch 1032/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0323 - acc: 0.9905 - val_loss: 0.5086 - val_acc: 0.9279\n",
            "Epoch 1033/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0335 - acc: 0.9895 - val_loss: 0.4646 - val_acc: 0.9234\n",
            "Epoch 1034/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0319 - acc: 0.9930 - val_loss: 0.4904 - val_acc: 0.9144\n",
            "Epoch 1035/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0317 - acc: 0.9910 - val_loss: 0.5006 - val_acc: 0.9099\n",
            "Epoch 1036/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0316 - acc: 0.9915 - val_loss: 0.4720 - val_acc: 0.9324\n",
            "Epoch 1037/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0316 - acc: 0.9910 - val_loss: 0.4614 - val_acc: 0.9324\n",
            "Epoch 1038/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0350 - acc: 0.9910 - val_loss: 0.4388 - val_acc: 0.9414\n",
            "Epoch 1039/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0591 - acc: 0.9830 - val_loss: 0.5369 - val_acc: 0.8964\n",
            "Epoch 1040/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0596 - acc: 0.9795 - val_loss: 0.5634 - val_acc: 0.9009\n",
            "Epoch 1041/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0390 - acc: 0.9865 - val_loss: 0.5230 - val_acc: 0.9144\n",
            "Epoch 1042/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0325 - acc: 0.9900 - val_loss: 0.5160 - val_acc: 0.9279\n",
            "Epoch 1043/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0321 - acc: 0.9900 - val_loss: 0.5532 - val_acc: 0.8964\n",
            "Epoch 1044/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0316 - acc: 0.9905 - val_loss: 0.5027 - val_acc: 0.9189\n",
            "Epoch 1045/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0321 - acc: 0.9915 - val_loss: 0.4724 - val_acc: 0.9414\n",
            "Epoch 1046/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0327 - acc: 0.9900 - val_loss: 0.4156 - val_acc: 0.9505\n",
            "Epoch 1047/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0315 - acc: 0.9910 - val_loss: 0.5713 - val_acc: 0.8964\n",
            "Epoch 1048/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0315 - acc: 0.9920 - val_loss: 0.4985 - val_acc: 0.9324\n",
            "Epoch 1049/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0340 - acc: 0.9895 - val_loss: 0.5614 - val_acc: 0.9324\n",
            "Epoch 1050/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0449 - acc: 0.9845 - val_loss: 0.4842 - val_acc: 0.9009\n",
            "Epoch 1051/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0389 - acc: 0.9870 - val_loss: 0.4722 - val_acc: 0.9279\n",
            "Epoch 1052/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0351 - acc: 0.9890 - val_loss: 0.5321 - val_acc: 0.9054\n",
            "Epoch 1053/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0347 - acc: 0.9900 - val_loss: 0.4945 - val_acc: 0.8919\n",
            "Epoch 1054/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0375 - acc: 0.9890 - val_loss: 0.4790 - val_acc: 0.9279\n",
            "Epoch 1055/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0326 - acc: 0.9890 - val_loss: 0.4471 - val_acc: 0.9369\n",
            "Epoch 1056/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0337 - acc: 0.9890 - val_loss: 0.5018 - val_acc: 0.9324\n",
            "Epoch 1057/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0478 - acc: 0.9860 - val_loss: 0.6159 - val_acc: 0.9054\n",
            "Epoch 1058/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0346 - acc: 0.9900 - val_loss: 0.5173 - val_acc: 0.9324\n",
            "Epoch 1059/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0356 - acc: 0.9905 - val_loss: 0.5650 - val_acc: 0.9009\n",
            "Epoch 1060/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0443 - acc: 0.9850 - val_loss: 0.4575 - val_acc: 0.9324\n",
            "Epoch 1061/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0349 - acc: 0.9890 - val_loss: 0.4679 - val_acc: 0.9234\n",
            "Epoch 1062/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0310 - acc: 0.9915 - val_loss: 0.4750 - val_acc: 0.9459\n",
            "Epoch 1063/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0321 - acc: 0.9900 - val_loss: 0.4726 - val_acc: 0.9324\n",
            "Epoch 1064/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0303 - acc: 0.9915 - val_loss: 0.4698 - val_acc: 0.9279\n",
            "Epoch 1065/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0313 - acc: 0.9890 - val_loss: 0.4700 - val_acc: 0.9324\n",
            "Epoch 1066/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0323 - acc: 0.9915 - val_loss: 0.4645 - val_acc: 0.9234\n",
            "Epoch 1067/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0320 - acc: 0.9905 - val_loss: 0.5548 - val_acc: 0.9009\n",
            "Epoch 1068/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0377 - acc: 0.9885 - val_loss: 0.6361 - val_acc: 0.8874\n",
            "Epoch 1069/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0717 - acc: 0.9800 - val_loss: 0.4372 - val_acc: 0.9550\n",
            "Epoch 1070/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0424 - acc: 0.9865 - val_loss: 0.4282 - val_acc: 0.9369\n",
            "Epoch 1071/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0357 - acc: 0.9895 - val_loss: 0.5379 - val_acc: 0.9099\n",
            "Epoch 1072/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0333 - acc: 0.9890 - val_loss: 0.4914 - val_acc: 0.9099\n",
            "Epoch 1073/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0329 - acc: 0.9900 - val_loss: 0.4535 - val_acc: 0.9324\n",
            "Epoch 1074/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0311 - acc: 0.9915 - val_loss: 0.4964 - val_acc: 0.9234\n",
            "Epoch 1075/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0311 - acc: 0.9905 - val_loss: 0.4876 - val_acc: 0.9279\n",
            "Epoch 1076/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0295 - acc: 0.9910 - val_loss: 0.5469 - val_acc: 0.8874\n",
            "Epoch 1077/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0314 - acc: 0.9915 - val_loss: 0.5078 - val_acc: 0.9279\n",
            "Epoch 1078/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0306 - acc: 0.9920 - val_loss: 0.4816 - val_acc: 0.9234\n",
            "Epoch 1079/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0296 - acc: 0.9920 - val_loss: 0.5634 - val_acc: 0.9009\n",
            "Epoch 1080/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0388 - acc: 0.9900 - val_loss: 0.4892 - val_acc: 0.9369\n",
            "Epoch 1081/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0609 - acc: 0.9780 - val_loss: 0.5761 - val_acc: 0.8829\n",
            "Epoch 1082/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0416 - acc: 0.9865 - val_loss: 0.5577 - val_acc: 0.8919\n",
            "Epoch 1083/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0343 - acc: 0.9905 - val_loss: 0.4627 - val_acc: 0.9414\n",
            "Epoch 1084/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0314 - acc: 0.9905 - val_loss: 0.4747 - val_acc: 0.9279\n",
            "Epoch 1085/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0301 - acc: 0.9920 - val_loss: 0.4781 - val_acc: 0.9279\n",
            "Epoch 1086/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0308 - acc: 0.9915 - val_loss: 0.5143 - val_acc: 0.9099\n",
            "Epoch 1087/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0298 - acc: 0.9920 - val_loss: 0.5204 - val_acc: 0.9279\n",
            "Epoch 1088/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0348 - acc: 0.9880 - val_loss: 0.4715 - val_acc: 0.9234\n",
            "Epoch 1089/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0525 - acc: 0.9830 - val_loss: 0.5683 - val_acc: 0.9054\n",
            "Epoch 1090/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0398 - acc: 0.9865 - val_loss: 0.5618 - val_acc: 0.9189\n",
            "Epoch 1091/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0341 - acc: 0.9915 - val_loss: 0.4485 - val_acc: 0.9279\n",
            "Epoch 1092/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0317 - acc: 0.9895 - val_loss: 0.5006 - val_acc: 0.9234\n",
            "Epoch 1093/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0299 - acc: 0.9910 - val_loss: 0.4472 - val_acc: 0.9414\n",
            "Epoch 1094/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0292 - acc: 0.9930 - val_loss: 0.4928 - val_acc: 0.9054\n",
            "Epoch 1095/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0296 - acc: 0.9915 - val_loss: 0.4754 - val_acc: 0.9099\n",
            "Epoch 1096/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0314 - acc: 0.9900 - val_loss: 0.4794 - val_acc: 0.9279\n",
            "Epoch 1097/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0427 - acc: 0.9875 - val_loss: 0.4384 - val_acc: 0.9459\n",
            "Epoch 1098/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0380 - acc: 0.9885 - val_loss: 0.5289 - val_acc: 0.9324\n",
            "Epoch 1099/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0327 - acc: 0.9905 - val_loss: 0.4583 - val_acc: 0.9369\n",
            "Epoch 1100/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0305 - acc: 0.9905 - val_loss: 0.4990 - val_acc: 0.9189\n",
            "Epoch 1101/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0299 - acc: 0.9915 - val_loss: 0.4801 - val_acc: 0.9189\n",
            "Epoch 1102/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0300 - acc: 0.9905 - val_loss: 0.5060 - val_acc: 0.9234\n",
            "Epoch 1103/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0367 - acc: 0.9900 - val_loss: 0.5096 - val_acc: 0.9279\n",
            "Epoch 1104/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0406 - acc: 0.9885 - val_loss: 0.4374 - val_acc: 0.9369\n",
            "Epoch 1105/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0411 - acc: 0.9895 - val_loss: 0.4694 - val_acc: 0.9144\n",
            "Epoch 1106/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0495 - acc: 0.9830 - val_loss: 0.5151 - val_acc: 0.9279\n",
            "Epoch 1107/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0499 - acc: 0.9835 - val_loss: 0.5252 - val_acc: 0.9279\n",
            "Epoch 1108/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0348 - acc: 0.9900 - val_loss: 0.4046 - val_acc: 0.9369\n",
            "Epoch 1109/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0307 - acc: 0.9920 - val_loss: 0.4493 - val_acc: 0.9234\n",
            "Epoch 1110/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0308 - acc: 0.9915 - val_loss: 0.4859 - val_acc: 0.9279\n",
            "Epoch 1111/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0296 - acc: 0.9920 - val_loss: 0.4641 - val_acc: 0.9189\n",
            "Epoch 1112/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0290 - acc: 0.9925 - val_loss: 0.4915 - val_acc: 0.9054\n",
            "Epoch 1113/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0300 - acc: 0.9915 - val_loss: 0.4677 - val_acc: 0.9279\n",
            "Epoch 1114/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0401 - acc: 0.9880 - val_loss: 0.5586 - val_acc: 0.9234\n",
            "Epoch 1115/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0533 - acc: 0.9835 - val_loss: 0.4958 - val_acc: 0.9144\n",
            "Epoch 1116/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0321 - acc: 0.9920 - val_loss: 0.4258 - val_acc: 0.9324\n",
            "Epoch 1117/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0334 - acc: 0.9905 - val_loss: 0.4715 - val_acc: 0.9234\n",
            "Epoch 1118/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0307 - acc: 0.9910 - val_loss: 0.4626 - val_acc: 0.9369\n",
            "Epoch 1119/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0296 - acc: 0.9930 - val_loss: 0.5255 - val_acc: 0.9099\n",
            "Epoch 1120/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0298 - acc: 0.9930 - val_loss: 0.4866 - val_acc: 0.9144\n",
            "Epoch 1121/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0294 - acc: 0.9930 - val_loss: 0.4755 - val_acc: 0.9324\n",
            "Epoch 1122/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0295 - acc: 0.9930 - val_loss: 0.4462 - val_acc: 0.9414\n",
            "Epoch 1123/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0315 - acc: 0.9910 - val_loss: 0.5036 - val_acc: 0.9279\n",
            "Epoch 1124/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0315 - acc: 0.9900 - val_loss: 0.4801 - val_acc: 0.9324\n",
            "Epoch 1125/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0459 - acc: 0.9850 - val_loss: 0.5204 - val_acc: 0.9144\n",
            "Epoch 1126/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0525 - acc: 0.9820 - val_loss: 0.6100 - val_acc: 0.8829\n",
            "Epoch 1127/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0470 - acc: 0.9875 - val_loss: 0.4837 - val_acc: 0.9324\n",
            "Epoch 1128/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0344 - acc: 0.9900 - val_loss: 0.4889 - val_acc: 0.9324\n",
            "Epoch 1129/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0302 - acc: 0.9920 - val_loss: 0.4977 - val_acc: 0.9324\n",
            "Epoch 1130/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0306 - acc: 0.9905 - val_loss: 0.4326 - val_acc: 0.9459\n",
            "Epoch 1131/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0305 - acc: 0.9925 - val_loss: 0.5126 - val_acc: 0.9234\n",
            "Epoch 1132/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0289 - acc: 0.9910 - val_loss: 0.4753 - val_acc: 0.9189\n",
            "Epoch 1133/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0291 - acc: 0.9925 - val_loss: 0.5422 - val_acc: 0.9189\n",
            "Epoch 1134/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0484 - acc: 0.9845 - val_loss: 0.5384 - val_acc: 0.9099\n",
            "Epoch 1135/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0315 - acc: 0.9905 - val_loss: 0.4739 - val_acc: 0.9099\n",
            "Epoch 1136/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0300 - acc: 0.9925 - val_loss: 0.4520 - val_acc: 0.9414\n",
            "Epoch 1137/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0302 - acc: 0.9930 - val_loss: 0.5111 - val_acc: 0.9144\n",
            "Epoch 1138/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0342 - acc: 0.9895 - val_loss: 0.5150 - val_acc: 0.9234\n",
            "Epoch 1139/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0348 - acc: 0.9875 - val_loss: 0.5986 - val_acc: 0.9099\n",
            "Epoch 1140/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0309 - acc: 0.9915 - val_loss: 0.5063 - val_acc: 0.9189\n",
            "Epoch 1141/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0289 - acc: 0.9910 - val_loss: 0.4954 - val_acc: 0.9279\n",
            "Epoch 1142/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0296 - acc: 0.9910 - val_loss: 0.5346 - val_acc: 0.9234\n",
            "Epoch 1143/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0317 - acc: 0.9910 - val_loss: 0.5198 - val_acc: 0.9279\n",
            "Epoch 1144/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0438 - acc: 0.9835 - val_loss: 0.5186 - val_acc: 0.9279\n",
            "Epoch 1145/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0399 - acc: 0.9865 - val_loss: 0.5029 - val_acc: 0.9324\n",
            "Epoch 1146/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0311 - acc: 0.9910 - val_loss: 0.4821 - val_acc: 0.9324\n",
            "Epoch 1147/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0299 - acc: 0.9925 - val_loss: 0.5648 - val_acc: 0.8919\n",
            "Epoch 1148/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0306 - acc: 0.9925 - val_loss: 0.5143 - val_acc: 0.9099\n",
            "Epoch 1149/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0298 - acc: 0.9915 - val_loss: 0.5144 - val_acc: 0.9279\n",
            "Epoch 1150/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0290 - acc: 0.9910 - val_loss: 0.5287 - val_acc: 0.9009\n",
            "Epoch 1151/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0297 - acc: 0.9910 - val_loss: 0.5158 - val_acc: 0.9189\n",
            "Epoch 1152/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0319 - acc: 0.9905 - val_loss: 0.4473 - val_acc: 0.9324\n",
            "Epoch 1153/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0301 - acc: 0.9920 - val_loss: 0.5897 - val_acc: 0.9009\n",
            "Epoch 1154/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0330 - acc: 0.9925 - val_loss: 0.4482 - val_acc: 0.9234\n",
            "Epoch 1155/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0550 - acc: 0.9820 - val_loss: 0.5588 - val_acc: 0.8964\n",
            "Epoch 1156/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0538 - acc: 0.9840 - val_loss: 0.5271 - val_acc: 0.9234\n",
            "Epoch 1157/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0584 - acc: 0.9815 - val_loss: 0.4732 - val_acc: 0.9189\n",
            "Epoch 1158/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0349 - acc: 0.9900 - val_loss: 0.4806 - val_acc: 0.9234\n",
            "Epoch 1159/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0322 - acc: 0.9915 - val_loss: 0.4770 - val_acc: 0.9234\n",
            "Epoch 1160/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0299 - acc: 0.9930 - val_loss: 0.4935 - val_acc: 0.9279\n",
            "Epoch 1161/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0288 - acc: 0.9915 - val_loss: 0.5193 - val_acc: 0.9009\n",
            "Epoch 1162/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0286 - acc: 0.9920 - val_loss: 0.4881 - val_acc: 0.9324\n",
            "Epoch 1163/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0299 - acc: 0.9910 - val_loss: 0.4986 - val_acc: 0.9279\n",
            "Epoch 1164/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0283 - acc: 0.9925 - val_loss: 0.4903 - val_acc: 0.9279\n",
            "Epoch 1165/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0285 - acc: 0.9925 - val_loss: 0.5031 - val_acc: 0.9099\n",
            "Epoch 1166/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0293 - acc: 0.9910 - val_loss: 0.5558 - val_acc: 0.9009\n",
            "Epoch 1167/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0309 - acc: 0.9900 - val_loss: 0.4882 - val_acc: 0.9144\n",
            "Epoch 1168/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0326 - acc: 0.9915 - val_loss: 0.5324 - val_acc: 0.8919\n",
            "Epoch 1169/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0309 - acc: 0.9915 - val_loss: 0.4846 - val_acc: 0.9279\n",
            "Epoch 1170/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0310 - acc: 0.9910 - val_loss: 0.5122 - val_acc: 0.9279\n",
            "Epoch 1171/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0470 - acc: 0.9840 - val_loss: 0.5489 - val_acc: 0.9189\n",
            "Epoch 1172/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0645 - acc: 0.9810 - val_loss: 0.5311 - val_acc: 0.9144\n",
            "Epoch 1173/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0362 - acc: 0.9875 - val_loss: 0.4592 - val_acc: 0.9234\n",
            "Epoch 1174/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0317 - acc: 0.9900 - val_loss: 0.4972 - val_acc: 0.9144\n",
            "Epoch 1175/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0297 - acc: 0.9905 - val_loss: 0.5491 - val_acc: 0.9099\n",
            "Epoch 1176/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0293 - acc: 0.9920 - val_loss: 0.4974 - val_acc: 0.9279\n",
            "Epoch 1177/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0283 - acc: 0.9920 - val_loss: 0.4744 - val_acc: 0.9234\n",
            "Epoch 1178/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0281 - acc: 0.9905 - val_loss: 0.5578 - val_acc: 0.8919\n",
            "Epoch 1179/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0286 - acc: 0.9920 - val_loss: 0.4848 - val_acc: 0.9279\n",
            "Epoch 1180/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0286 - acc: 0.9910 - val_loss: 0.5057 - val_acc: 0.9279\n",
            "Epoch 1181/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0292 - acc: 0.9935 - val_loss: 0.4800 - val_acc: 0.9279\n",
            "Epoch 1182/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0305 - acc: 0.9905 - val_loss: 0.5752 - val_acc: 0.9099\n",
            "Epoch 1183/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0309 - acc: 0.9910 - val_loss: 0.5637 - val_acc: 0.9009\n",
            "Epoch 1184/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0296 - acc: 0.9910 - val_loss: 0.5314 - val_acc: 0.9189\n",
            "Epoch 1185/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0290 - acc: 0.9915 - val_loss: 0.5139 - val_acc: 0.9189\n",
            "Epoch 1186/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0314 - acc: 0.9905 - val_loss: 0.4590 - val_acc: 0.9279\n",
            "Epoch 1187/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0503 - acc: 0.9840 - val_loss: 0.5518 - val_acc: 0.9144\n",
            "Epoch 1188/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0696 - acc: 0.9805 - val_loss: 0.5445 - val_acc: 0.9234\n",
            "Epoch 1189/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0465 - acc: 0.9865 - val_loss: 0.4346 - val_acc: 0.9369\n",
            "Epoch 1190/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0298 - acc: 0.9920 - val_loss: 0.4973 - val_acc: 0.9279\n",
            "Epoch 1191/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0283 - acc: 0.9930 - val_loss: 0.5341 - val_acc: 0.9054\n",
            "Epoch 1192/1500\n",
            "1998/1998 [==============================] - 0s 118us/step - loss: 0.0275 - acc: 0.9920 - val_loss: 0.5285 - val_acc: 0.9054\n",
            "Epoch 1193/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0271 - acc: 0.9935 - val_loss: 0.5310 - val_acc: 0.9099\n",
            "Epoch 1194/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0271 - acc: 0.9920 - val_loss: 0.4896 - val_acc: 0.9279\n",
            "Epoch 1195/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0278 - acc: 0.9930 - val_loss: 0.4946 - val_acc: 0.9234\n",
            "Epoch 1196/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0287 - acc: 0.9915 - val_loss: 0.4916 - val_acc: 0.9324\n",
            "Epoch 1197/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0289 - acc: 0.9920 - val_loss: 0.5066 - val_acc: 0.9189\n",
            "Epoch 1198/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0268 - acc: 0.9930 - val_loss: 0.4520 - val_acc: 0.9279\n",
            "Epoch 1199/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0340 - acc: 0.9920 - val_loss: 0.5716 - val_acc: 0.9234\n",
            "Epoch 1200/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0421 - acc: 0.9895 - val_loss: 0.4763 - val_acc: 0.9279\n",
            "Epoch 1201/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0545 - acc: 0.9845 - val_loss: 0.6554 - val_acc: 0.8874\n",
            "Epoch 1202/1500\n",
            "1998/1998 [==============================] - 0s 118us/step - loss: 0.0395 - acc: 0.9880 - val_loss: 0.5235 - val_acc: 0.9054\n",
            "Epoch 1203/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0308 - acc: 0.9905 - val_loss: 0.5541 - val_acc: 0.9054\n",
            "Epoch 1204/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0300 - acc: 0.9920 - val_loss: 0.5663 - val_acc: 0.9144\n",
            "Epoch 1205/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0291 - acc: 0.9920 - val_loss: 0.4806 - val_acc: 0.9369\n",
            "Epoch 1206/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0291 - acc: 0.9900 - val_loss: 0.4174 - val_acc: 0.9505\n",
            "Epoch 1207/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0300 - acc: 0.9905 - val_loss: 0.4998 - val_acc: 0.9324\n",
            "Epoch 1208/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0271 - acc: 0.9925 - val_loss: 0.5119 - val_acc: 0.9234\n",
            "Epoch 1209/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0274 - acc: 0.9930 - val_loss: 0.5318 - val_acc: 0.9234\n",
            "Epoch 1210/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0277 - acc: 0.9935 - val_loss: 0.5150 - val_acc: 0.9234\n",
            "Epoch 1211/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0272 - acc: 0.9930 - val_loss: 0.5340 - val_acc: 0.9189\n",
            "Epoch 1212/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0390 - acc: 0.9885 - val_loss: 0.4550 - val_acc: 0.9099\n",
            "Epoch 1213/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0512 - acc: 0.9840 - val_loss: 0.6304 - val_acc: 0.9144\n",
            "Epoch 1214/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0417 - acc: 0.9870 - val_loss: 0.6344 - val_acc: 0.9144\n",
            "Epoch 1215/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0328 - acc: 0.9915 - val_loss: 0.5363 - val_acc: 0.9144\n",
            "Epoch 1216/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0312 - acc: 0.9910 - val_loss: 0.5300 - val_acc: 0.9234\n",
            "Epoch 1217/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0292 - acc: 0.9915 - val_loss: 0.5957 - val_acc: 0.8874\n",
            "Epoch 1218/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0280 - acc: 0.9920 - val_loss: 0.5288 - val_acc: 0.9234\n",
            "Epoch 1219/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0287 - acc: 0.9920 - val_loss: 0.5368 - val_acc: 0.9189\n",
            "Epoch 1220/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0287 - acc: 0.9915 - val_loss: 0.5808 - val_acc: 0.9099\n",
            "Epoch 1221/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0270 - acc: 0.9910 - val_loss: 0.5297 - val_acc: 0.9189\n",
            "Epoch 1222/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0275 - acc: 0.9915 - val_loss: 0.5416 - val_acc: 0.9189\n",
            "Epoch 1223/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0266 - acc: 0.9925 - val_loss: 0.5844 - val_acc: 0.9189\n",
            "Epoch 1224/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0270 - acc: 0.9925 - val_loss: 0.5911 - val_acc: 0.9144\n",
            "Epoch 1225/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0545 - acc: 0.9820 - val_loss: 0.4813 - val_acc: 0.9324\n",
            "Epoch 1226/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0439 - acc: 0.9880 - val_loss: 0.5677 - val_acc: 0.9144\n",
            "Epoch 1227/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0289 - acc: 0.9920 - val_loss: 0.5766 - val_acc: 0.8964\n",
            "Epoch 1228/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0273 - acc: 0.9930 - val_loss: 0.5633 - val_acc: 0.9099\n",
            "Epoch 1229/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0262 - acc: 0.9920 - val_loss: 0.5267 - val_acc: 0.9279\n",
            "Epoch 1230/1500\n",
            "1998/1998 [==============================] - 0s 131us/step - loss: 0.0270 - acc: 0.9920 - val_loss: 0.4926 - val_acc: 0.9324\n",
            "Epoch 1231/1500\n",
            "1998/1998 [==============================] - 0s 131us/step - loss: 0.0268 - acc: 0.9940 - val_loss: 0.5250 - val_acc: 0.9054\n",
            "Epoch 1232/1500\n",
            "1998/1998 [==============================] - 0s 132us/step - loss: 0.0287 - acc: 0.9910 - val_loss: 0.5324 - val_acc: 0.9279\n",
            "Epoch 1233/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0288 - acc: 0.9920 - val_loss: 0.4932 - val_acc: 0.9324\n",
            "Epoch 1234/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0273 - acc: 0.9930 - val_loss: 0.5242 - val_acc: 0.9279\n",
            "Epoch 1235/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0282 - acc: 0.9920 - val_loss: 0.5554 - val_acc: 0.8919\n",
            "Epoch 1236/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0271 - acc: 0.9910 - val_loss: 0.5396 - val_acc: 0.9189\n",
            "Epoch 1237/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.0290 - acc: 0.9920 - val_loss: 0.5781 - val_acc: 0.9099\n",
            "Epoch 1238/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0416 - acc: 0.9865 - val_loss: 0.5811 - val_acc: 0.9234\n",
            "Epoch 1239/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0500 - acc: 0.9825 - val_loss: 0.5968 - val_acc: 0.9189\n",
            "Epoch 1240/1500\n",
            "1998/1998 [==============================] - 0s 133us/step - loss: 0.0403 - acc: 0.9890 - val_loss: 0.5501 - val_acc: 0.9189\n",
            "Epoch 1241/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0335 - acc: 0.9890 - val_loss: 0.4804 - val_acc: 0.9324\n",
            "Epoch 1242/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0293 - acc: 0.9920 - val_loss: 0.5156 - val_acc: 0.9279\n",
            "Epoch 1243/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0284 - acc: 0.9925 - val_loss: 0.5221 - val_acc: 0.9234\n",
            "Epoch 1244/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0269 - acc: 0.9925 - val_loss: 0.6067 - val_acc: 0.9189\n",
            "Epoch 1245/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0278 - acc: 0.9930 - val_loss: 0.5272 - val_acc: 0.9279\n",
            "Epoch 1246/1500\n",
            "1998/1998 [==============================] - 0s 130us/step - loss: 0.0264 - acc: 0.9950 - val_loss: 0.5972 - val_acc: 0.9144\n",
            "Epoch 1247/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0257 - acc: 0.9935 - val_loss: 0.5845 - val_acc: 0.9054\n",
            "Epoch 1248/1500\n",
            "1998/1998 [==============================] - 0s 132us/step - loss: 0.0270 - acc: 0.9920 - val_loss: 0.5705 - val_acc: 0.9189\n",
            "Epoch 1249/1500\n",
            "1998/1998 [==============================] - 0s 131us/step - loss: 0.0277 - acc: 0.9930 - val_loss: 0.5828 - val_acc: 0.8964\n",
            "Epoch 1250/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0372 - acc: 0.9900 - val_loss: 0.5999 - val_acc: 0.8964\n",
            "Epoch 1251/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0370 - acc: 0.9870 - val_loss: 0.5926 - val_acc: 0.8919\n",
            "Epoch 1252/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0432 - acc: 0.9855 - val_loss: 0.5706 - val_acc: 0.9144\n",
            "Epoch 1253/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0413 - acc: 0.9850 - val_loss: 0.5586 - val_acc: 0.8874\n",
            "Epoch 1254/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0337 - acc: 0.9905 - val_loss: 0.4321 - val_acc: 0.9459\n",
            "Epoch 1255/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0414 - acc: 0.9875 - val_loss: 0.5291 - val_acc: 0.9189\n",
            "Epoch 1256/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0301 - acc: 0.9915 - val_loss: 0.6157 - val_acc: 0.9099\n",
            "Epoch 1257/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0267 - acc: 0.9930 - val_loss: 0.5350 - val_acc: 0.9144\n",
            "Epoch 1258/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0268 - acc: 0.9930 - val_loss: 0.5681 - val_acc: 0.9099\n",
            "Epoch 1259/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0258 - acc: 0.9930 - val_loss: 0.5802 - val_acc: 0.8964\n",
            "Epoch 1260/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0262 - acc: 0.9925 - val_loss: 0.5542 - val_acc: 0.9189\n",
            "Epoch 1261/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0267 - acc: 0.9935 - val_loss: 0.5730 - val_acc: 0.8919\n",
            "Epoch 1262/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0270 - acc: 0.9925 - val_loss: 0.5403 - val_acc: 0.9189\n",
            "Epoch 1263/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0253 - acc: 0.9940 - val_loss: 0.6105 - val_acc: 0.8784\n",
            "Epoch 1264/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0264 - acc: 0.9925 - val_loss: 0.6043 - val_acc: 0.8784\n",
            "Epoch 1265/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0372 - acc: 0.9885 - val_loss: 0.5178 - val_acc: 0.9009\n",
            "Epoch 1266/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0416 - acc: 0.9865 - val_loss: 0.5836 - val_acc: 0.8919\n",
            "Epoch 1267/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0382 - acc: 0.9875 - val_loss: 0.5419 - val_acc: 0.9144\n",
            "Epoch 1268/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0313 - acc: 0.9910 - val_loss: 0.5189 - val_acc: 0.9279\n",
            "Epoch 1269/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0278 - acc: 0.9920 - val_loss: 0.5514 - val_acc: 0.9234\n",
            "Epoch 1270/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0271 - acc: 0.9925 - val_loss: 0.5353 - val_acc: 0.9144\n",
            "Epoch 1271/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0264 - acc: 0.9940 - val_loss: 0.5788 - val_acc: 0.8874\n",
            "Epoch 1272/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0258 - acc: 0.9920 - val_loss: 0.5671 - val_acc: 0.8874\n",
            "Epoch 1273/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0261 - acc: 0.9925 - val_loss: 0.5208 - val_acc: 0.9144\n",
            "Epoch 1274/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0327 - acc: 0.9885 - val_loss: 0.6140 - val_acc: 0.9054\n",
            "Epoch 1275/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0755 - acc: 0.9745 - val_loss: 0.4640 - val_acc: 0.9459\n",
            "Epoch 1276/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0420 - acc: 0.9870 - val_loss: 0.6184 - val_acc: 0.9099\n",
            "Epoch 1277/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0307 - acc: 0.9920 - val_loss: 0.5968 - val_acc: 0.9144\n",
            "Epoch 1278/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0280 - acc: 0.9920 - val_loss: 0.5604 - val_acc: 0.8964\n",
            "Epoch 1279/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0268 - acc: 0.9925 - val_loss: 0.5618 - val_acc: 0.9189\n",
            "Epoch 1280/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0264 - acc: 0.9935 - val_loss: 0.5525 - val_acc: 0.9144\n",
            "Epoch 1281/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0266 - acc: 0.9935 - val_loss: 0.6121 - val_acc: 0.8874\n",
            "Epoch 1282/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0259 - acc: 0.9930 - val_loss: 0.5783 - val_acc: 0.9054\n",
            "Epoch 1283/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0255 - acc: 0.9935 - val_loss: 0.5354 - val_acc: 0.9234\n",
            "Epoch 1284/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0260 - acc: 0.9925 - val_loss: 0.5951 - val_acc: 0.9009\n",
            "Epoch 1285/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0272 - acc: 0.9925 - val_loss: 0.6328 - val_acc: 0.9144\n",
            "Epoch 1286/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0283 - acc: 0.9920 - val_loss: 0.5589 - val_acc: 0.9009\n",
            "Epoch 1287/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0340 - acc: 0.9890 - val_loss: 0.4670 - val_acc: 0.9369\n",
            "Epoch 1288/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0330 - acc: 0.9900 - val_loss: 0.5609 - val_acc: 0.8964\n",
            "Epoch 1289/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0703 - acc: 0.9815 - val_loss: 0.3831 - val_acc: 0.9234\n",
            "Epoch 1290/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0500 - acc: 0.9850 - val_loss: 0.4941 - val_acc: 0.9279\n",
            "Epoch 1291/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0284 - acc: 0.9925 - val_loss: 0.4983 - val_acc: 0.9279\n",
            "Epoch 1292/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0285 - acc: 0.9915 - val_loss: 0.5502 - val_acc: 0.9189\n",
            "Epoch 1293/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0263 - acc: 0.9935 - val_loss: 0.5715 - val_acc: 0.8964\n",
            "Epoch 1294/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0259 - acc: 0.9930 - val_loss: 0.5723 - val_acc: 0.9099\n",
            "Epoch 1295/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0254 - acc: 0.9930 - val_loss: 0.5616 - val_acc: 0.9279\n",
            "Epoch 1296/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0255 - acc: 0.9925 - val_loss: 0.5440 - val_acc: 0.9189\n",
            "Epoch 1297/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0277 - acc: 0.9915 - val_loss: 0.5913 - val_acc: 0.8874\n",
            "Epoch 1298/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0285 - acc: 0.9915 - val_loss: 0.5995 - val_acc: 0.9009\n",
            "Epoch 1299/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0299 - acc: 0.9915 - val_loss: 0.5667 - val_acc: 0.9189\n",
            "Epoch 1300/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0375 - acc: 0.9880 - val_loss: 0.4835 - val_acc: 0.9009\n",
            "Epoch 1301/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0316 - acc: 0.9890 - val_loss: 0.6737 - val_acc: 0.8919\n",
            "Epoch 1302/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0445 - acc: 0.9840 - val_loss: 0.5082 - val_acc: 0.9279\n",
            "Epoch 1303/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0324 - acc: 0.9895 - val_loss: 0.6572 - val_acc: 0.9144\n",
            "Epoch 1304/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0299 - acc: 0.9945 - val_loss: 0.5771 - val_acc: 0.9054\n",
            "Epoch 1305/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0257 - acc: 0.9925 - val_loss: 0.5335 - val_acc: 0.9234\n",
            "Epoch 1306/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0254 - acc: 0.9930 - val_loss: 0.5994 - val_acc: 0.9054\n",
            "Epoch 1307/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0253 - acc: 0.9930 - val_loss: 0.5932 - val_acc: 0.8874\n",
            "Epoch 1308/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0253 - acc: 0.9930 - val_loss: 0.5687 - val_acc: 0.9054\n",
            "Epoch 1309/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0258 - acc: 0.9925 - val_loss: 0.5530 - val_acc: 0.9144\n",
            "Epoch 1310/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0342 - acc: 0.9905 - val_loss: 0.4861 - val_acc: 0.9324\n",
            "Epoch 1311/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0382 - acc: 0.9885 - val_loss: 0.5812 - val_acc: 0.8874\n",
            "Epoch 1312/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0340 - acc: 0.9885 - val_loss: 0.5132 - val_acc: 0.8964\n",
            "Epoch 1313/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0296 - acc: 0.9900 - val_loss: 0.5179 - val_acc: 0.9234\n",
            "Epoch 1314/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0288 - acc: 0.9910 - val_loss: 0.5464 - val_acc: 0.9234\n",
            "Epoch 1315/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0259 - acc: 0.9940 - val_loss: 0.5113 - val_acc: 0.9189\n",
            "Epoch 1316/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0265 - acc: 0.9925 - val_loss: 0.5916 - val_acc: 0.9009\n",
            "Epoch 1317/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0255 - acc: 0.9930 - val_loss: 0.5833 - val_acc: 0.8784\n",
            "Epoch 1318/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0252 - acc: 0.9935 - val_loss: 0.5763 - val_acc: 0.9144\n",
            "Epoch 1319/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0252 - acc: 0.9930 - val_loss: 0.6043 - val_acc: 0.8829\n",
            "Epoch 1320/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0250 - acc: 0.9930 - val_loss: 0.4765 - val_acc: 0.9324\n",
            "Epoch 1321/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0588 - acc: 0.9805 - val_loss: 0.6705 - val_acc: 0.9054\n",
            "Epoch 1322/1500\n",
            "1998/1998 [==============================] - 0s 131us/step - loss: 0.0524 - acc: 0.9820 - val_loss: 0.4553 - val_acc: 0.9144\n",
            "Epoch 1323/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0305 - acc: 0.9915 - val_loss: 0.5103 - val_acc: 0.9189\n",
            "Epoch 1324/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0294 - acc: 0.9915 - val_loss: 0.5295 - val_acc: 0.9189\n",
            "Epoch 1325/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0271 - acc: 0.9925 - val_loss: 0.5211 - val_acc: 0.9099\n",
            "Epoch 1326/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0248 - acc: 0.9935 - val_loss: 0.5673 - val_acc: 0.8919\n",
            "Epoch 1327/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0245 - acc: 0.9935 - val_loss: 0.5393 - val_acc: 0.9234\n",
            "Epoch 1328/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0244 - acc: 0.9930 - val_loss: 0.5298 - val_acc: 0.9189\n",
            "Epoch 1329/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0251 - acc: 0.9940 - val_loss: 0.5961 - val_acc: 0.9144\n",
            "Epoch 1330/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0265 - acc: 0.9930 - val_loss: 0.5844 - val_acc: 0.9099\n",
            "Epoch 1331/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0260 - acc: 0.9925 - val_loss: 0.6453 - val_acc: 0.9189\n",
            "Epoch 1332/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0263 - acc: 0.9920 - val_loss: 0.6052 - val_acc: 0.8829\n",
            "Epoch 1333/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0256 - acc: 0.9925 - val_loss: 0.6939 - val_acc: 0.8739\n",
            "Epoch 1334/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0279 - acc: 0.9945 - val_loss: 0.6325 - val_acc: 0.9144\n",
            "Epoch 1335/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0269 - acc: 0.9930 - val_loss: 0.5857 - val_acc: 0.8919\n",
            "Epoch 1336/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0660 - acc: 0.9810 - val_loss: 0.5897 - val_acc: 0.9279\n",
            "Epoch 1337/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0399 - acc: 0.9870 - val_loss: 0.6783 - val_acc: 0.8964\n",
            "Epoch 1338/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0279 - acc: 0.9925 - val_loss: 0.5183 - val_acc: 0.9144\n",
            "Epoch 1339/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0279 - acc: 0.9930 - val_loss: 0.6226 - val_acc: 0.9009\n",
            "Epoch 1340/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0243 - acc: 0.9930 - val_loss: 0.6554 - val_acc: 0.8739\n",
            "Epoch 1341/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0245 - acc: 0.9925 - val_loss: 0.6050 - val_acc: 0.9144\n",
            "Epoch 1342/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0252 - acc: 0.9930 - val_loss: 0.6144 - val_acc: 0.8784\n",
            "Epoch 1343/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0246 - acc: 0.9940 - val_loss: 0.6118 - val_acc: 0.8919\n",
            "Epoch 1344/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0247 - acc: 0.9925 - val_loss: 0.5429 - val_acc: 0.9099\n",
            "Epoch 1345/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0246 - acc: 0.9920 - val_loss: 0.5523 - val_acc: 0.9144\n",
            "Epoch 1346/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0254 - acc: 0.9925 - val_loss: 0.5925 - val_acc: 0.8919\n",
            "Epoch 1347/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0258 - acc: 0.9925 - val_loss: 0.6912 - val_acc: 0.8874\n",
            "Epoch 1348/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0380 - acc: 0.9865 - val_loss: 0.5081 - val_acc: 0.9324\n",
            "Epoch 1349/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0510 - acc: 0.9830 - val_loss: 0.8179 - val_acc: 0.8604\n",
            "Epoch 1350/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0485 - acc: 0.9835 - val_loss: 0.5500 - val_acc: 0.9144\n",
            "Epoch 1351/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0325 - acc: 0.9905 - val_loss: 0.6036 - val_acc: 0.9054\n",
            "Epoch 1352/1500\n",
            "1998/1998 [==============================] - 0s 128us/step - loss: 0.0321 - acc: 0.9890 - val_loss: 0.5377 - val_acc: 0.9009\n",
            "Epoch 1353/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0244 - acc: 0.9940 - val_loss: 0.5891 - val_acc: 0.8919\n",
            "Epoch 1354/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0242 - acc: 0.9935 - val_loss: 0.6223 - val_acc: 0.8829\n",
            "Epoch 1355/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0238 - acc: 0.9945 - val_loss: 0.5964 - val_acc: 0.8874\n",
            "Epoch 1356/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0241 - acc: 0.9930 - val_loss: 0.6558 - val_acc: 0.8829\n",
            "Epoch 1357/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0235 - acc: 0.9945 - val_loss: 0.6116 - val_acc: 0.8874\n",
            "Epoch 1358/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0249 - acc: 0.9925 - val_loss: 0.6507 - val_acc: 0.8829\n",
            "Epoch 1359/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0291 - acc: 0.9915 - val_loss: 0.7181 - val_acc: 0.8829\n",
            "Epoch 1360/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0347 - acc: 0.9895 - val_loss: 0.5589 - val_acc: 0.9279\n",
            "Epoch 1361/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0503 - acc: 0.9825 - val_loss: 0.7169 - val_acc: 0.9009\n",
            "Epoch 1362/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0355 - acc: 0.9885 - val_loss: 0.5423 - val_acc: 0.9189\n",
            "Epoch 1363/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0445 - acc: 0.9855 - val_loss: 0.5528 - val_acc: 0.9234\n",
            "Epoch 1364/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0420 - acc: 0.9870 - val_loss: 0.6791 - val_acc: 0.9054\n",
            "Epoch 1365/1500\n",
            "1998/1998 [==============================] - 0s 118us/step - loss: 0.0272 - acc: 0.9905 - val_loss: 0.5390 - val_acc: 0.9279\n",
            "Epoch 1366/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0240 - acc: 0.9940 - val_loss: 0.6121 - val_acc: 0.8874\n",
            "Epoch 1367/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0243 - acc: 0.9925 - val_loss: 0.5958 - val_acc: 0.9144\n",
            "Epoch 1368/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0240 - acc: 0.9940 - val_loss: 0.5684 - val_acc: 0.9144\n",
            "Epoch 1369/1500\n",
            "1998/1998 [==============================] - 0s 127us/step - loss: 0.0243 - acc: 0.9935 - val_loss: 0.6184 - val_acc: 0.8784\n",
            "Epoch 1370/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0242 - acc: 0.9935 - val_loss: 0.6339 - val_acc: 0.8829\n",
            "Epoch 1371/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0249 - acc: 0.9935 - val_loss: 0.6816 - val_acc: 0.8874\n",
            "Epoch 1372/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0296 - acc: 0.9915 - val_loss: 0.5472 - val_acc: 0.9054\n",
            "Epoch 1373/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0284 - acc: 0.9920 - val_loss: 0.6532 - val_acc: 0.8874\n",
            "Epoch 1374/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0270 - acc: 0.9930 - val_loss: 0.6172 - val_acc: 0.8829\n",
            "Epoch 1375/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0257 - acc: 0.9935 - val_loss: 0.6926 - val_acc: 0.8964\n",
            "Epoch 1376/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0272 - acc: 0.9910 - val_loss: 0.6085 - val_acc: 0.9189\n",
            "Epoch 1377/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0415 - acc: 0.9850 - val_loss: 0.5930 - val_acc: 0.8874\n",
            "Epoch 1378/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0468 - acc: 0.9840 - val_loss: 0.6300 - val_acc: 0.8964\n",
            "Epoch 1379/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0308 - acc: 0.9920 - val_loss: 0.5354 - val_acc: 0.9144\n",
            "Epoch 1380/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0278 - acc: 0.9920 - val_loss: 0.5491 - val_acc: 0.9009\n",
            "Epoch 1381/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0260 - acc: 0.9920 - val_loss: 0.5858 - val_acc: 0.8919\n",
            "Epoch 1382/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0240 - acc: 0.9930 - val_loss: 0.6091 - val_acc: 0.9054\n",
            "Epoch 1383/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0309 - acc: 0.9925 - val_loss: 0.5929 - val_acc: 0.8919\n",
            "Epoch 1384/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0260 - acc: 0.9915 - val_loss: 0.5808 - val_acc: 0.8874\n",
            "Epoch 1385/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0239 - acc: 0.9925 - val_loss: 0.5561 - val_acc: 0.9099\n",
            "Epoch 1386/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0248 - acc: 0.9935 - val_loss: 0.6649 - val_acc: 0.8964\n",
            "Epoch 1387/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0243 - acc: 0.9935 - val_loss: 0.5818 - val_acc: 0.8829\n",
            "Epoch 1388/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0243 - acc: 0.9925 - val_loss: 0.5821 - val_acc: 0.9054\n",
            "Epoch 1389/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0259 - acc: 0.9930 - val_loss: 0.6158 - val_acc: 0.9009\n",
            "Epoch 1390/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0243 - acc: 0.9935 - val_loss: 0.6646 - val_acc: 0.8829\n",
            "Epoch 1391/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0246 - acc: 0.9925 - val_loss: 0.6032 - val_acc: 0.9099\n",
            "Epoch 1392/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0245 - acc: 0.9930 - val_loss: 0.6237 - val_acc: 0.8964\n",
            "Epoch 1393/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0416 - acc: 0.9860 - val_loss: 0.5776 - val_acc: 0.8874\n",
            "Epoch 1394/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0536 - acc: 0.9845 - val_loss: 0.6773 - val_acc: 0.8784\n",
            "Epoch 1395/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0343 - acc: 0.9890 - val_loss: 0.5990 - val_acc: 0.8964\n",
            "Epoch 1396/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0294 - acc: 0.9925 - val_loss: 0.5609 - val_acc: 0.8829\n",
            "Epoch 1397/1500\n",
            "1998/1998 [==============================] - 0s 125us/step - loss: 0.0259 - acc: 0.9935 - val_loss: 0.6284 - val_acc: 0.8784\n",
            "Epoch 1398/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0241 - acc: 0.9925 - val_loss: 0.6184 - val_acc: 0.8784\n",
            "Epoch 1399/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0233 - acc: 0.9935 - val_loss: 0.6043 - val_acc: 0.8829\n",
            "Epoch 1400/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0227 - acc: 0.9935 - val_loss: 0.5893 - val_acc: 0.9054\n",
            "Epoch 1401/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0240 - acc: 0.9930 - val_loss: 0.6653 - val_acc: 0.8964\n",
            "Epoch 1402/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0235 - acc: 0.9935 - val_loss: 0.6109 - val_acc: 0.8964\n",
            "Epoch 1403/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0237 - acc: 0.9940 - val_loss: 0.6181 - val_acc: 0.8829\n",
            "Epoch 1404/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0251 - acc: 0.9940 - val_loss: 0.6512 - val_acc: 0.8874\n",
            "Epoch 1405/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0328 - acc: 0.9895 - val_loss: 0.6647 - val_acc: 0.8964\n",
            "Epoch 1406/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0402 - acc: 0.9885 - val_loss: 0.6444 - val_acc: 0.8874\n",
            "Epoch 1407/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0566 - acc: 0.9820 - val_loss: 0.6478 - val_acc: 0.8874\n",
            "Epoch 1408/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0370 - acc: 0.9885 - val_loss: 0.5819 - val_acc: 0.9099\n",
            "Epoch 1409/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0297 - acc: 0.9915 - val_loss: 0.5980 - val_acc: 0.9099\n",
            "Epoch 1410/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0260 - acc: 0.9925 - val_loss: 0.6119 - val_acc: 0.8829\n",
            "Epoch 1411/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0236 - acc: 0.9945 - val_loss: 0.5985 - val_acc: 0.9144\n",
            "Epoch 1412/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0237 - acc: 0.9935 - val_loss: 0.5893 - val_acc: 0.9189\n",
            "Epoch 1413/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0235 - acc: 0.9945 - val_loss: 0.6091 - val_acc: 0.9054\n",
            "Epoch 1414/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0242 - acc: 0.9925 - val_loss: 0.6356 - val_acc: 0.9009\n",
            "Epoch 1415/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0235 - acc: 0.9945 - val_loss: 0.6728 - val_acc: 0.9144\n",
            "Epoch 1416/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0237 - acc: 0.9935 - val_loss: 0.6418 - val_acc: 0.8964\n",
            "Epoch 1417/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0224 - acc: 0.9945 - val_loss: 0.6157 - val_acc: 0.8919\n",
            "Epoch 1418/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0243 - acc: 0.9920 - val_loss: 0.6434 - val_acc: 0.9009\n",
            "Epoch 1419/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0384 - acc: 0.9860 - val_loss: 0.6384 - val_acc: 0.8784\n",
            "Epoch 1420/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0410 - acc: 0.9870 - val_loss: 0.6405 - val_acc: 0.9054\n",
            "Epoch 1421/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0264 - acc: 0.9930 - val_loss: 0.6373 - val_acc: 0.8829\n",
            "Epoch 1422/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0233 - acc: 0.9935 - val_loss: 0.5841 - val_acc: 0.8874\n",
            "Epoch 1423/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0229 - acc: 0.9940 - val_loss: 0.6581 - val_acc: 0.8829\n",
            "Epoch 1424/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0232 - acc: 0.9930 - val_loss: 0.6494 - val_acc: 0.8964\n",
            "Epoch 1425/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0226 - acc: 0.9945 - val_loss: 0.6090 - val_acc: 0.8874\n",
            "Epoch 1426/1500\n",
            "1998/1998 [==============================] - 0s 118us/step - loss: 0.0229 - acc: 0.9925 - val_loss: 0.6877 - val_acc: 0.8784\n",
            "Epoch 1427/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0235 - acc: 0.9930 - val_loss: 0.6858 - val_acc: 0.8919\n",
            "Epoch 1428/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0298 - acc: 0.9915 - val_loss: 0.6218 - val_acc: 0.9234\n",
            "Epoch 1429/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0475 - acc: 0.9840 - val_loss: 0.5229 - val_acc: 0.9054\n",
            "Epoch 1430/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0493 - acc: 0.9820 - val_loss: 0.5946 - val_acc: 0.8964\n",
            "Epoch 1431/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0286 - acc: 0.9915 - val_loss: 0.5847 - val_acc: 0.9189\n",
            "Epoch 1432/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0261 - acc: 0.9910 - val_loss: 0.5618 - val_acc: 0.9324\n",
            "Epoch 1433/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0264 - acc: 0.9920 - val_loss: 0.6198 - val_acc: 0.9009\n",
            "Epoch 1434/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0232 - acc: 0.9930 - val_loss: 0.6570 - val_acc: 0.8784\n",
            "Epoch 1435/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0236 - acc: 0.9940 - val_loss: 0.6179 - val_acc: 0.8964\n",
            "Epoch 1436/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0224 - acc: 0.9935 - val_loss: 0.6275 - val_acc: 0.9054\n",
            "Epoch 1437/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0225 - acc: 0.9945 - val_loss: 0.7269 - val_acc: 0.8784\n",
            "Epoch 1438/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0232 - acc: 0.9920 - val_loss: 0.6674 - val_acc: 0.8874\n",
            "Epoch 1439/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0226 - acc: 0.9935 - val_loss: 0.6639 - val_acc: 0.9054\n",
            "Epoch 1440/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0232 - acc: 0.9955 - val_loss: 0.6357 - val_acc: 0.8874\n",
            "Epoch 1441/1500\n",
            "1998/1998 [==============================] - 0s 124us/step - loss: 0.0272 - acc: 0.9915 - val_loss: 0.6743 - val_acc: 0.8784\n",
            "Epoch 1442/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0276 - acc: 0.9930 - val_loss: 0.7219 - val_acc: 0.8919\n",
            "Epoch 1443/1500\n",
            "1998/1998 [==============================] - 0s 126us/step - loss: 0.0296 - acc: 0.9910 - val_loss: 0.6393 - val_acc: 0.9234\n",
            "Epoch 1444/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0582 - acc: 0.9840 - val_loss: 0.6880 - val_acc: 0.8964\n",
            "Epoch 1445/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0430 - acc: 0.9860 - val_loss: 0.6694 - val_acc: 0.8784\n",
            "Epoch 1446/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0302 - acc: 0.9910 - val_loss: 0.5368 - val_acc: 0.9234\n",
            "Epoch 1447/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0261 - acc: 0.9925 - val_loss: 0.6031 - val_acc: 0.8919\n",
            "Epoch 1448/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0239 - acc: 0.9930 - val_loss: 0.6402 - val_acc: 0.8919\n",
            "Epoch 1449/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0243 - acc: 0.9930 - val_loss: 0.6456 - val_acc: 0.8874\n",
            "Epoch 1450/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0233 - acc: 0.9945 - val_loss: 0.6655 - val_acc: 0.8964\n",
            "Epoch 1451/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0231 - acc: 0.9930 - val_loss: 0.6787 - val_acc: 0.8919\n",
            "Epoch 1452/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0235 - acc: 0.9930 - val_loss: 0.6143 - val_acc: 0.9009\n",
            "Epoch 1453/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0236 - acc: 0.9940 - val_loss: 0.6593 - val_acc: 0.9009\n",
            "Epoch 1454/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0226 - acc: 0.9935 - val_loss: 0.6198 - val_acc: 0.9099\n",
            "Epoch 1455/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0243 - acc: 0.9930 - val_loss: 0.6490 - val_acc: 0.9009\n",
            "Epoch 1456/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0239 - acc: 0.9935 - val_loss: 0.6398 - val_acc: 0.8964\n",
            "Epoch 1457/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0268 - acc: 0.9925 - val_loss: 0.6039 - val_acc: 0.9144\n",
            "Epoch 1458/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0309 - acc: 0.9895 - val_loss: 0.6234 - val_acc: 0.8829\n",
            "Epoch 1459/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0532 - acc: 0.9835 - val_loss: 0.6176 - val_acc: 0.9189\n",
            "Epoch 1460/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0488 - acc: 0.9855 - val_loss: 0.6533 - val_acc: 0.8919\n",
            "Epoch 1461/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0269 - acc: 0.9910 - val_loss: 0.7271 - val_acc: 0.8919\n",
            "Epoch 1462/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0237 - acc: 0.9945 - val_loss: 0.6757 - val_acc: 0.8874\n",
            "Epoch 1463/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0233 - acc: 0.9940 - val_loss: 0.6924 - val_acc: 0.8919\n",
            "Epoch 1464/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0226 - acc: 0.9940 - val_loss: 0.6940 - val_acc: 0.9009\n",
            "Epoch 1465/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0236 - acc: 0.9935 - val_loss: 0.6581 - val_acc: 0.9099\n",
            "Epoch 1466/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0224 - acc: 0.9940 - val_loss: 0.6862 - val_acc: 0.8919\n",
            "Epoch 1467/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0228 - acc: 0.9940 - val_loss: 0.7263 - val_acc: 0.8874\n",
            "Epoch 1468/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0217 - acc: 0.9945 - val_loss: 0.6295 - val_acc: 0.8964\n",
            "Epoch 1469/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0228 - acc: 0.9945 - val_loss: 0.7092 - val_acc: 0.8829\n",
            "Epoch 1470/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0313 - acc: 0.9925 - val_loss: 0.5673 - val_acc: 0.9144\n",
            "Epoch 1471/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0477 - acc: 0.9850 - val_loss: 0.6335 - val_acc: 0.8919\n",
            "Epoch 1472/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0360 - acc: 0.9895 - val_loss: 0.5997 - val_acc: 0.9099\n",
            "Epoch 1473/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0333 - acc: 0.9900 - val_loss: 0.6040 - val_acc: 0.8964\n",
            "Epoch 1474/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0237 - acc: 0.9920 - val_loss: 0.6478 - val_acc: 0.8919\n",
            "Epoch 1475/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0222 - acc: 0.9950 - val_loss: 0.6035 - val_acc: 0.9099\n",
            "Epoch 1476/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0228 - acc: 0.9935 - val_loss: 0.6943 - val_acc: 0.8874\n",
            "Epoch 1477/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0231 - acc: 0.9925 - val_loss: 0.6744 - val_acc: 0.8874\n",
            "Epoch 1478/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0224 - acc: 0.9935 - val_loss: 0.6725 - val_acc: 0.8829\n",
            "Epoch 1479/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0222 - acc: 0.9945 - val_loss: 0.6524 - val_acc: 0.8874\n",
            "Epoch 1480/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0225 - acc: 0.9935 - val_loss: 0.6322 - val_acc: 0.9009\n",
            "Epoch 1481/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0236 - acc: 0.9930 - val_loss: 0.6826 - val_acc: 0.8874\n",
            "Epoch 1482/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0301 - acc: 0.9895 - val_loss: 0.6077 - val_acc: 0.9144\n",
            "Epoch 1483/1500\n",
            "1998/1998 [==============================] - 0s 119us/step - loss: 0.0541 - acc: 0.9785 - val_loss: 0.7084 - val_acc: 0.9099\n",
            "Epoch 1484/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0478 - acc: 0.9840 - val_loss: 0.6345 - val_acc: 0.8874\n",
            "Epoch 1485/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0275 - acc: 0.9915 - val_loss: 0.6338 - val_acc: 0.8964\n",
            "Epoch 1486/1500\n",
            "1998/1998 [==============================] - 0s 129us/step - loss: 0.0267 - acc: 0.9910 - val_loss: 0.6225 - val_acc: 0.9099\n",
            "Epoch 1487/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0225 - acc: 0.9945 - val_loss: 0.6044 - val_acc: 0.9189\n",
            "Epoch 1488/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0223 - acc: 0.9930 - val_loss: 0.6392 - val_acc: 0.8919\n",
            "Epoch 1489/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0220 - acc: 0.9935 - val_loss: 0.6605 - val_acc: 0.8919\n",
            "Epoch 1490/1500\n",
            "1998/1998 [==============================] - 0s 123us/step - loss: 0.0226 - acc: 0.9945 - val_loss: 0.6686 - val_acc: 0.8919\n",
            "Epoch 1491/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0216 - acc: 0.9945 - val_loss: 0.6587 - val_acc: 0.8874\n",
            "Epoch 1492/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0212 - acc: 0.9935 - val_loss: 0.6281 - val_acc: 0.8829\n",
            "Epoch 1493/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0221 - acc: 0.9935 - val_loss: 0.7171 - val_acc: 0.8829\n",
            "Epoch 1494/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0219 - acc: 0.9955 - val_loss: 0.6778 - val_acc: 0.8919\n",
            "Epoch 1495/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0220 - acc: 0.9935 - val_loss: 0.6479 - val_acc: 0.8964\n",
            "Epoch 1496/1500\n",
            "1998/1998 [==============================] - 0s 120us/step - loss: 0.0219 - acc: 0.9945 - val_loss: 0.6690 - val_acc: 0.8874\n",
            "Epoch 1497/1500\n",
            "1998/1998 [==============================] - 0s 121us/step - loss: 0.0377 - acc: 0.9870 - val_loss: 0.6062 - val_acc: 0.8964\n",
            "Epoch 1498/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0797 - acc: 0.9760 - val_loss: 0.6254 - val_acc: 0.8964\n",
            "Epoch 1499/1500\n",
            "1998/1998 [==============================] - 0s 122us/step - loss: 0.0411 - acc: 0.9820 - val_loss: 0.6787 - val_acc: 0.8784\n",
            "Epoch 1500/1500\n",
            "1998/1998 [==============================] - 0s 118us/step - loss: 0.0277 - acc: 0.9910 - val_loss: 0.6323 - val_acc: 0.8829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f999eaf9780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "metadata": {
        "id": "MfoOEcuud6_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train_pred_neural_net = neural_net.predict_classes(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LYrGn1ekeGuQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e4d4242a-8163-4867-f1ea-6c6a141e85ed"
      },
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_train,y_train_pred_neural_net)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1101,    9],\n",
              "       [   0,  213]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "metadata": {
        "id": "lC8a_8EVecbw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_pred_neural_net = neural_net.predict_classes(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qKtKrTuKecnu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "d2ce6197-8d84-445d-ed48-1ff3a8a8f8d5"
      },
      "cell_type": "code",
      "source": [
        "confusion_matrix = confusion_matrix(y_test,y_test_pred_neural_net)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-33c5f669ae1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_pred_neural_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "I-tdvyK_A9Fr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "06b62809-6de7-4cbf-b95d-0e00e57e9e46"
      },
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix\n",
        "print (tn)\n",
        "print (fp)\n",
        "print (fn)\n",
        "print (tp)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-d4c92cb1c344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "pSF6ODd9ADsM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KkoXdR9c_4Dz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Predicted label')\n",
        "    plt.xlabel('True label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u15LW2mLAF6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "a11d58fe-59e9-4d81-ac6b-e4a3301e0ab5"
      },
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(confusion_matrix,[0,1])"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[103  20]\n",
            " [ 15   9]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAGOCAYAAADo92ZEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f999e45ec50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}